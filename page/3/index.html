<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="Solar">
<meta property="og:url" content="https://zhangchenchen.github.io/page/3/index.html">
<meta property="og:site_name" content="Solar">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Solar">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangchenchen.github.io/page/3/"/>





  <title> Solar </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-92407570-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Solar</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/10/29/kubernetes-monitoring-guide-1/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/10/29/kubernetes-monitoring-guide-1/" itemprop="url">
                  kubernetes-- kubernetes 监控指南（一）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-10-29T18:03:10+08:00">
                2017-10-29
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/10/29/kubernetes-monitoring-guide-1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/29/kubernetes-monitoring-guide-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于k8s的监控，官方文档有一个系列<a href="http://blog.kubernetes.io/2017/05/kubernetes-monitoring-guide.html" target="_blank" rel="noopener">Kubernetes: a monitoring guide</a>,涵盖了大部分内容，如果觉得费时可以直接参阅<a href="https://blog.ruhm.me/post/kubernetes-monitoring/" target="_blank" rel="noopener">这篇文章</a>,本文主要是偏重Heapster &amp; InfluxDB &amp; Grafana 方案的实践。</p>
<h2 id="Heapster-amp-InfluxDB-amp-Grafana"><a href="#Heapster-amp-InfluxDB-amp-Grafana" class="headerlink" title="Heapster &amp; InfluxDB &amp; Grafana"></a>Heapster &amp; InfluxDB &amp; Grafana</h2><p>Heapster &amp; InfluxDB &amp; Grafana的组合是kubernetes监控的官方推荐组合，官方dashboard的概览中就用到了这三个工具来展示监控数据。这三个组合中，</p>
<ul>
<li>Heapster 作为metric数据的聚合和处理</li>
<li>InfluxDB是一个时间序列数据库，用来存储Heapster传过来的metric数据</li>
<li>Grafana是一个dashboard的可视化展现工具。</li>
</ul>
<h3 id="Heapster-介绍"><a href="#Heapster-介绍" class="headerlink" title="Heapster 介绍"></a>Heapster 介绍</h3><p>Heapster会收集集群中的node，namespace，pod等级别的metric信息，对这些数据聚合之后存储到指定的后端存储系统中。Heapster是通过访问node上的kubelet的API来获取metric数据，而kubelet中聚合了cAdvisor这个工具采集当前节点的所有容器的性能数据。目前Heapster支持的后端数据库包括memory、InfluxDB、BigQuery、 Google Cloud Monitoring 和 Google Cloud Logging等。<br>Heapster收集到的metric数据可以通过restAPI访问，主要是CPU和内存数据，包括集群级别，node级别，namespace级别，pod级别，容器级别的metric数据。</p>
<h3 id="Grafanna-介绍"><a href="#Grafanna-介绍" class="headerlink" title="Grafanna 介绍"></a>Grafanna 介绍</h3><p>Grafanna是一个比较知名的dashboard可视化工具,可以将时序数据通过检索展现成图标或曲线等形式，通过插件机制支持多种后端数据源，比如InfluxDB, Graphite, Elasticsearch, Prometheus等。</p>
<h3 id="InfluxDB-介绍"><a href="#InfluxDB-介绍" class="headerlink" title="InfluxDB 介绍"></a>InfluxDB 介绍</h3><p>InfluxDB是基于LevelDB ，为了优化写请求比较多的情况而实现的一种时间序列数据库，每一条数据都带有时间戳属性，主要用于实时数据采集，时间跟踪记录等。</p>
<p>如下图，便是kubernetes监控的总体架构图：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-10-31-monitoring-architecture.png" alt="k8s-monitor-arc"></p>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><ul>
<li>首先要有一个k8s集群，然后clone heapster的yaml文件。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/kubernetes/heapster.git</span><br></pre></td></tr></table></figure>
<ul>
<li>根据yaml文件创建对应的服务，yaml文件有3个，分别对应Heapster &amp; InfluxDB &amp; Grafana，查看yaml文件，其实就是创建了一系列serviceAccount，DeployMent，Service。注：如果是生产环境，注意修改InfluxDB的存储由暂时性volume改为持久volume（将emptyDir: {}换成挂载磁盘或其他存储方式）。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ./heapster/deploy/kube-config/influxdb </span><br><span class="line">kubectl create -f  ./*</span><br></pre></td></tr></table></figure>
<ul>
<li>创建完成后，查看对应的pod是否起来了，如果没有起来在排错。对于kubeadm部署的集群，默认是采用tls双向认证，且采用RBAC的授权机制，所以还需要创建rolebinding.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f  ./heapster/deploy/kube-config/rbac/heapster-rbac.yaml</span><br></pre></td></tr></table></figure>
<h2 id="配置参数解析"><a href="#配置参数解析" class="headerlink" title="配置参数解析"></a>配置参数解析</h2><p>看下heapster的yaml文件：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">heapster</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">heapster</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        task:</span> <span class="string">monitoring</span></span><br><span class="line"><span class="attr">        k8s-app:</span> <span class="string">heapster</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">heapster</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">heapster</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">gcr.io/google_containers/heapster-amd64:v1.4.0</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        command:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">/heapster</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--source=kubernetes:https://kubernetes.default</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    task:</span> <span class="string">monitoring</span></span><br><span class="line">    <span class="comment"># For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span></span><br><span class="line">    <span class="comment"># If you are NOT using this as an addon, you should comment out this line.</span></span><br><span class="line">    <span class="string">kubernetes.io/cluster-service:</span> <span class="string">'true'</span></span><br><span class="line">    <span class="string">kubernetes.io/name:</span> <span class="string">Heapster</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">heapster</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">8082</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">heapster</span></span><br></pre></td></tr></table></figure>
<p>可以看到创建了一个serviceAccount，用于与api-server通信，创建了一个Deployment，拉取相关镜像，可以看到执行heapster命令时，有两个参数，一个source指定metric的来源，<a href="https://kubernetes.default就是默认的k8s集群访问地址，可以通过kubectl" target="_blank" rel="noopener">https://kubernetes.default就是默认的k8s集群访问地址，可以通过kubectl</a> get service获得对应IP。sink指定要存储的数据地址，这里就是influxDB的地址。最后创建对应的service。<br>其他服务类似。</p>
<h2 id="服务调用与实践"><a href="#服务调用与实践" class="headerlink" title="服务调用与实践"></a>服务调用与实践</h2><p>这三个服务都有对应的restAPI可以调用，执行kubectl cluster-info 可以获取对应的地址。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-10-31-cluster-info.jpg" alt="cluster-info"></p>
<p>可以看出都是走的api-server的端口，应该是api-server调的这三个服务。</p>
<p>如果直接在浏览器里输入路径，会出现认证不通过错误：“User “system:anonymous” cannot proxy services in the namespace “kube-system”.”。这时因为rbac的权限认证导致，可以在浏览器中导入admin证书，以admin的角色进行访问。也可以通过kubectl proxy，在本地访问，也可以设置api-server的insecure-port，通过不安全端口进行访问，因为不安全端口不会进行认证和授权。</p>
<h3 id="通过-insecure-port-访问-heapster"><a href="#通过-insecure-port-访问-heapster" class="headerlink" title="通过 insecure-port 访问 heapster"></a>通过 insecure-port 访问 heapster</h3><ul>
<li>修改api-server 的yaml文件，添加insecure-port,insecure-bind-address</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    component:</span> <span class="string">kube-apiserver</span></span><br><span class="line"><span class="attr">    tier:</span> <span class="string">control-plane</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-apiserver</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - command:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">kube-apiserver</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--tls-cert-file=/etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key</span></span><br><span class="line">    <span class="string">..................</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--insecure-port=6444</span> <span class="comment"># 添加不安全端口</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">--insecure-bind-address=0.0.0.0</span> <span class="comment"># 绑定不安全端口地址</span></span><br><span class="line">    <span class="string">..................</span></span><br></pre></td></tr></table></figure>
<ul>
<li>由不安全端口构造url,如下示例(关于如何构造url参考<a href="https://jimmysong.io/posts/using-heapster-to-get-object-metrics/" target="_blank" rel="noopener">使用Heapster获取kubernetes集群对象的metric数据</a>)：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">curl http://172.16.21.250:6444/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/namespaces/kube-system/metrics/memory/usage</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"metrics"</span>: [  </span><br><span class="line">   &#123;</span><br><span class="line">    <span class="string">"timestamp"</span>: <span class="string">"2017-11-01T15:18:00Z"</span>,</span><br><span class="line">    <span class="string">"value"</span>: 1124192256</span><br><span class="line">   &#125;,</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="string">"timestamp"</span>: <span class="string">"2017-11-01T15:19:00Z"</span>,</span><br><span class="line">    <span class="string">"value"</span>: 1120612352</span><br><span class="line">   &#125;,</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="string">"timestamp"</span>: <span class="string">"2017-11-01T15:20:00Z"</span>,</span><br><span class="line">    <span class="string">"value"</span>: 1123553280</span><br><span class="line">   &#125;,</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="string">"timestamp"</span>: <span class="string">"2017-11-01T15:21:00Z"</span>,</span><br><span class="line">    <span class="string">"value"</span>: 1121275904</span><br><span class="line">   &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"latestTimestamp"</span>: <span class="string">"2017-11-01T15:21:00Z"</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="访问-Grafana-界面"><a href="#访问-Grafana-界面" class="headerlink" title="访问 Grafana 界面"></a>访问 Grafana 界面</h3><p>Grafana也提供rest api，url 的构建同上，可以参考<a href="http://docs.grafana.org/http_api/" target="_blank" rel="noopener">HTTP API Reference</a>。除此之外，grafana还有一个UI可以访问，在Grafana的yaml文件中添加type：NodePort,然后就可以通过ip:NodePort访问该UI。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/grafana-20171101154016.jpg" alt="grafana-ui"></p>
<p>通过该UI界面，设置查询SQL，就可以定制自己需要的图表，参考<a href="https://blog.kublr.com/how-to-utilize-the-heapster-influxdb-grafana-stack-in-kubernetes-for-monitoring-pods-4a553f4d36c9" target="_blank" rel="noopener">How to Utilize the “Heapster + InfluxDB + Grafana” Stack in Kubernetes for Monitoring Pods</a></p>
<p>以上就是Heapster &amp; InfluxDB &amp; Grafana 实现k8s监控的内容，这套方案的优点就是部署简单，与k8s结合的很好，缺点也比较明显，heapster是专为k8s设计的，没有通用性，且没有alert机制（当然可以通过设置grafana，但是需要自己做镜像）。除了这一套方案外 ，还有一种广泛使用的方案：<a href="https://zhangchenchen.github.io/2017/11/09/kubernetes-monitoring-guide-2/">prometheus + Grafana</a>，这套方案相对比较通用，除了k8s还可以对接其他系统（比如运行在k8s里面的数据库集群等），且有报警模块alertmanager，缺点就是配置相对比较麻烦，不过为了以后的方便，还是建议采用这种方案。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://blog.codecp.org/2016/07/07/Kubernets%E7%9B%91%E6%8E%A7%20Heapster+InfluxDB+Grafana/" target="_blank" rel="noopener">Kubernets监控 Heapster+InfluxDB+Grafana </a></p>
<p><a href="http://blog.kubernetes.io/2017/05/kubernetes-monitoring-guide.html" target="_blank" rel="noopener">Kubernetes: a monitoring guide</a></p>
<p><a href="https://sysdig.com/blog/monitoring-kubernetes-with-sysdig-cloud/" target="_blank" rel="noopener">Monitoring Kubernetes</a></p>
<p><a href="https://blog.kublr.com/how-to-utilize-the-heapster-influxdb-grafana-stack-in-kubernetes-for-monitoring-pods-4a553f4d36c9" target="_blank" rel="noopener">How to Utilize the “Heapster + InfluxDB + Grafana” Stack in Kubernetes for Monitoring Pods</a></p>
<p><a href="https://jimmysong.io/posts/using-heapster-to-get-object-metrics/" target="_blank" rel="noopener">使用Heapster获取kubernetes集群对象的metric数据</a></p>
<p><a href="https://blog.ruhm.me/post/kubernetes-monitoring/" target="_blank" rel="noopener">UBERNETES 集群监控方案研究</a></p>
<p><a href="https://jishu.io/kubernetes/kubernetes-monitoring-with-prometheus/" target="_blank" rel="noopener">使用Prometheus完成Kubernetes集群监控</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/10/26/kubernetes-qos/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/10/26/kubernetes-qos/" itemprop="url">
                  kubernetes-- kubernetes scheduler设计以及resource QOS
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-10-26T15:03:10+08:00">
                2017-10-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/10/26/kubernetes-qos/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/26/kubernetes-qos/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="kubernetes-scheduler-设计"><a href="#kubernetes-scheduler-设计" class="headerlink" title="kubernetes scheduler 设计"></a>kubernetes scheduler 设计</h2><p>kubernetes最主要的功能是容器编排，而容器编排中第一个想到的便是新建一个容器时，需要将该容器调度到哪一个节点上，其实跟openstack中的nova-scheduler很像。<br>看下我们设计sheduler时需要满足的需求：</p>
<ul>
<li>从用户角度，需要满足指定节点，与指定pod亲和/反亲和（例如同为cpu密集型的服务不要放在同一节点），服务分散</li>
<li>从集群的角度，需要满足资源平衡，利用率高等</li>
</ul>
<p>看下kubernetes中大致的调度过程，图片来自<a href="http://oj6ydypm2.bkt.clouddn.com/Kubernetes%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86_%E4%B8%81%E6%B5%B7%E6%B4%8B.pdf" target="_blank" rel="noopener">Kubernetes中的资源管理 </a>：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20171026k8s-scheduler.jpg" alt="k8s-scheduler"></p>
<p>再看下调度器的具体调度算法，大致分了两类，predicate和prioritizer：</p>
<ul>
<li>predicate:一系列过滤函数将不满足的node排除</li>
<li>prioritizer: 对通过的节点按照一系列的算法进行优先级排序，最后选择优先级最高的。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20171026-kuber-scheduler-argo.jpg" alt="kube-scheduler-algori"></p>
<p>简单介绍下常用的predicate 的几个算法：</p>
<ul>
<li>PodFitsResources：节点上剩余的资源是否大于 pod 请求的资源</li>
<li>PodFitsHost：如果 pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配</li>
<li>PodFitsHostPorts：节点上已经使用的 port 是否和 pod 申请的 port 冲突</li>
<li>PodSelectorMatches：过滤掉和 pod 指定的 label 不匹配的节点</li>
<li>NoDiskConflict：已经 mount 的 volume 和 pod 指定的 volume 不冲突，除非它们都是只读</li>
</ul>
<p>prioritizer的常用算法：</p>
<ul>
<li>LeastRequestedPriority：将Pod部署到剩余CPU和内存最多的Node上，这里的request就是kubernetes的QOS中定义的request。</li>
<li>BalancedResourceAllocation：节点上 CPU 和 Memory 使用率越接近，权重越高。这个应该和上面的一起使用，不应该单独使用</li>
<li>ImageLocalityPriority：倾向于已经有要使用镜像的节点，镜像总大小值越大，权重越高</li>
<li>NodeLabelPriority ：检查Node是否存在Pod需要的Label，有则10分，没有则0分</li>
<li>Spreading：同一个Service下的Pod尽可能的分散在集群中。Node上运行的通Service<br>下的Pod数目越少，分数越高。</li>
<li>Anti-Affinity：与Spreading类似，但是分散在拥有特定Label的所有Node中，包括Node Affinity/Anti-Affinity和Pod Affinity/Anti-Affinity。</li>
</ul>
<p>此外，1.6之后还有Taints and Tolerations等scheduler策略，当然，也可以定制，然后创建pod时指定schedulername,参考<a href="http://blog.kubernetes.io/2017/03/advanced-scheduling-in-kubernetes.html" target="_blank" rel="noopener">Advanced Scheduling in Kubernetes</a>。</p>
<h2 id="kubernetes-resource-Quality-of-Service-QOS"><a href="#kubernetes-resource-Quality-of-Service-QOS" class="headerlink" title="kubernetes resource Quality of Service(QOS)"></a>kubernetes resource Quality of Service(QOS)</h2><p>kubernetes中的QOS实现是通过request和limit两个概念,主要包括两个具体的指标：CPU和内存。request可以理解为pod的下限，即最少分配给该pod的资源，limit则相应的是分配资源的上限。<br>对于每一个资源，container可以指定具体的资源需求（requests）和限制（limits），requests申请范围是0到node节点的最大配置，而limits申请范围是requests到无限，即0 &lt;= requests &lt;=Node Allocatable, requests &lt;= limits &lt;= Infinity。<br>对于CPU，如果pod中服务使用CPU超过设置的limits，pod不会被kill掉但会被限制。如果没有设置limits，pod可以使用全部空闲的cpu资源。<br>对于内存，当一个pod使用内存超过了设置的limits，pod中container的进程会被kernel因OOM kill掉。当container因为OOM被kill掉时，系统倾向于在其原所在的机器上重启该container或本机或其他重新创建一个pod。<br>在Kubernetes中，pod的QoS级别分为以下三种：Guaranteed, Burstable与 Best-Effort。</p>
<h3 id="Guaranteed"><a href="#Guaranteed" class="headerlink" title="Guaranteed"></a>Guaranteed</h3><p>满足条件：pod中所有容器都必须统一设置limits，并且设置参数都一致，如果有一个容器要设置requests，那么所有容器都要设置，并设置参数同limits一致。如果一个容器只指明limit而未设定request，则request的值等于limit值。</p>
<p>Guaranteed举例1：容器只指明了limits而未指明requests。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">name: foo</span><br><span class="line">resources:</span><br><span class="line">  limits:</span><br><span class="line">    cpu: 10m</span><br><span class="line">    memory: 1Gi</span><br><span class="line">name: bar</span><br><span class="line">resources:</span><br><span class="line">  limits:</span><br><span class="line">    cpu: 100m</span><br><span class="line">    memory: 100Mi</span><br></pre></td></tr></table></figure>
<p>Guaranteed举例2：requests与limit均指定且值相等。<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">name: foo</span><br><span class="line">resources:</span><br><span class="line">  limits:</span><br><span class="line">    cpu: 10m</span><br><span class="line">    memory: 1Gi</span><br><span class="line">  requests:</span><br><span class="line">    cpu: 10m</span><br><span class="line">    memory: 1Gi</span><br><span class="line"></span><br><span class="line">name: bar</span><br><span class="line">resources:</span><br><span class="line">  limits:</span><br><span class="line">    cpu: 100m</span><br><span class="line">    memory: 100Mi</span><br><span class="line">  requests:</span><br><span class="line">    cpu: 100m</span><br><span class="line">    memory: 100Mi</span><br></pre></td></tr></table></figure></p>
<h3 id="Best-Effort"><a href="#Best-Effort" class="headerlink" title="Best-Effort"></a>Best-Effort</h3><p>满足条件：Pod中所有容器的所有Resource的request和limit都没有赋值。</p>
<p>示例：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">    name: foo</span><br><span class="line">        resources:</span><br><span class="line">    name: bar</span><br><span class="line">        resources:</span><br></pre></td></tr></table></figure></p>
<h3 id="Burstable"><a href="#Burstable" class="headerlink" title="Burstable"></a>Burstable</h3><p>满足条件：pod中只要有一个容器的requests和limits的设置不相同，满足“0&lt;request&lt;limit&lt;∞” 。</p>
<p>示例,Container bar没有指定resources：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">    name: foo</span><br><span class="line">        resources:</span><br><span class="line">            limits:</span><br><span class="line">                cpu: 10m</span><br><span class="line">                memory: 1Gi</span><br><span class="line">            requests:</span><br><span class="line">                cpu: 10m</span><br><span class="line">                memory: 1Gi</span><br><span class="line"></span><br><span class="line">    name: bar</span><br></pre></td></tr></table></figure>
<p>3种QoS优先级从有低到高（从左向右）：<br>Best-Effort pods -&gt; Burstable pods -&gt; Guaranteed pods</p>
<p>也就是说，QoS pods被kill掉场景与顺序如下： </p>
<ul>
<li>Best-Effort 类型的pods：系统用完了全部内存时，该类型pods会最先被kill掉。</li>
<li>Burstable类型pods：系统用完了全部内存，且没有Best-Effort container可以被kill时，该类型pods会被kill掉。</li>
<li>Guaranteed pods：系统用完了全部内存、且没有Burstable与Best-Effort container可以被kill，该类型的pods会被kill掉。<br>注：如果pod进程因使用超过预先设定的limites而非Node资源紧张情况，系统倾向于在其原所在的机器上重启该container或本机或其他重新创建一个pod。</li>
</ul>
<p>具体到k8s的配置中，大致要配置两个地方，一个是namesapce中所有容器的cpu/内存的总和request和limit，另一个是namespace中默认的容器cpu/内存的request，limit值(在容器创建时没有指定request，limit时使用默认值)。这两类资源一个叫做ResourceQuota，一个叫做LimitRange ，参考示例：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">LimitRange</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">mem-limit-range</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">  - default:</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">512</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">    defaultRequest:</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">256</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Container</span></span><br></pre></td></tr></table></figure>
<p>以上为一个LimitRange的yaml文件，指定namespace并创建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f limitrange-defaults.yaml --namespace=default</span><br></pre></td></tr></table></figure></p>
<p>当在创建pod的时候，如果没有指定具体的request和limit，就会按照默认值。</p>
<p>再创建一个ResourceQuota：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">mem-cpu-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  hard:</span></span><br><span class="line">    <span class="string">requests.cpu:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="string">requests.memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line">    <span class="string">limits.cpu:</span> <span class="string">"2"</span></span><br><span class="line">    <span class="string">limits.memory:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">    pods:</span> <span class="string">"10"</span></span><br><span class="line"><span class="attr">    persistentvolumeclaims:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="string">services.loadbalancers:</span> <span class="string">"2"</span></span><br><span class="line">    <span class="string">services.nodeports:</span> <span class="string">"10"</span></span><br></pre></td></tr></table></figure></p>
<p>同样，指定namespace并创建，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f resourcequota-default.yaml --namespace=default</span><br></pre></td></tr></table></figure></p>
<p>由配置文件可以看出，除了指定cpu/内存的request和limit，还可以指定pod数量以及其他一些api resource的数量。<br>更多示例参考<a href="https://k8smeetup.github.io/docs/tasks/administer-cluster/memory-default-namespace/" target="_blank" rel="noopener">Manage Memory, CPU, and API Resources</a></p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://oj6ydypm2.bkt.clouddn.com/Kubernetes%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86_%E4%B8%81%E6%B5%B7%E6%B4%8B.pdf" target="_blank" rel="noopener">Kubernetes中的资源管理 </a></p>
<p><a href="http://cizixs.com/2017/03/10/kubernetes-intro-scheduler" target="_blank" rel="noopener">kubernetes 简介：调度器和调度算法</a></p>
<p><a href="https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/" target="_blank" rel="noopener">How does the Kubernetes scheduler work</a></p>
<p><a href="http://blog.csdn.net/yan234280533/article/details/62965993" target="_blank" rel="noopener">Kubernetes计算资源管理–requests和limits</a></p>
<p><a href="http://dockone.io/article/2592" target="_blank" rel="noopener">Kubernetes之服务质量保证（QoS）</a></p>
<p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/" target="_blank" rel="noopener">Configure Quality of Service for Pods</a></p>
<p><a href="https://github.com/eBay/Kubernetes/blob/master/docs/proposals/resource-qos.md" target="_blank" rel="noopener">resource-qos.md</a></p>
<p><a href="https://k8smeetup.github.io/docs/tasks/administer-cluster/memory-default-namespace/" target="_blank" rel="noopener">Manage Memory, CPU, and API Resources</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/" itemprop="url">
                  flask-- SQLAlchemy 与 flask-sqlalchemy记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-10-24T12:20:10+08:00">
                2017-10-24
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="SQLAchemy"><a href="#SQLAchemy" class="headerlink" title="SQLAchemy"></a>SQLAchemy</h2><p>在flask开发中，涉及到数据库的交互，一般都会想到SQLAchemy。SQLAchemy是一个python的SQL工具集，它实现了ORM,类似于Java web开发中的hibernete。</p>
<h2 id="flask-sqlachemy"><a href="#flask-sqlachemy" class="headerlink" title="flask-sqlachemy"></a>flask-sqlachemy</h2><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://opennebula.org/opennebula-vs-openstack-user-needs-vs-vendor-driven/" target="_blank" rel="noopener">OpenNebula vs. OpenStack: User Needs vs. Vendor Driven </a></p>
<p><a href="https://opennebula.org/comparing-opennebula-and-openstack-two-different-views-on-the-cloud/" target="_blank" rel="noopener">Comparing OpenNebula and OpenStack: Two Different Views on the Cloud</a></p>
<p><a href="https://www.slideshare.net/opennebula/opennebula-414-handson-tutorial?ref=https://opennebula.org/documentation/tutorials/" target="_blank" rel="noopener">OpenNebula 4.14 Hands-on Tutorial</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/09/12/openNebula-vs-openstack/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/12/openNebula-vs-openstack/" itemprop="url">
                  Openstack-- OpenStack 与OpenNebula的前世今生
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-12T16:25:30+08:00">
                2017-09-12
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/09/12/openNebula-vs-openstack/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/09/12/openNebula-vs-openstack/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引出"><a href="#引出" class="headerlink" title="引出"></a>引出</h2><p>本篇博文说实话感觉滞后太多，因为关于云计算开源框架的争论在前几年还甚嚣尘上，自从openstack风卷残云之势席卷整个云计算圈之后，之前的一些老前辈貌似都默默退出了，这也包括本文的一个主角OpenNebula。其实OpenNebula一直都在，只不过OpenStack的名气太大，以至于其他框架都被无意间忽略了。不过随着OpenStack项目的飞速膨胀，复杂度与可维护性也随之疯涨，OpenNebula这个一直秉承“Simplicity,Flexibility”的框架也得到了不少人的拥趸，本文就主要讲讲这个本应在几年前讨论的话题：OpenStack 与OpenNebula，孰优孰劣。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-09-12-opennebula-vs-openstack.jpg" alt="openstack vs opennebula"></p>
<h2 id="OpenStack-VS-OpenNebula"><a href="#OpenStack-VS-OpenNebula" class="headerlink" title="OpenStack VS OpenNebula"></a>OpenStack VS OpenNebula</h2><h3 id="定位的区别"><a href="#定位的区别" class="headerlink" title="定位的区别"></a>定位的区别</h3><p>Openstack从一开始就是打算跟AWS正面刚，所以一直借鉴（抄袭）AWS,可以认为是走公有云（提供基础设施）路线的，而OpenNebula则是走企业云（数据中心虚拟化）路线的，是打算跟vmware干仗的。但随着各个厂商的站队，OpenStack突然红了，但是红了也是有代价的，OpenStack的发展方向开始被Foundation控制，而这个Foundation是由各大厂商（以red hat为首）控制的，所以OpenStack其实是面向这些巨头们的，当然其中不乏各大公司的博弈，君不见早期launchpad上多个顽疾似的bug一直无人问津，而各大厂商的适配driver却早已开发完善。OpenNebula一直有一个核心开发组织掌握着开发的整体方向，相对于OpenStack,”铜臭味”没那么重，但也因为没有巨头的摇旗呐喊，所以开发进度不是很快。<br>所以，按照OpenNebula开发者的话说：Openstack是服务于foundation的，而OpenNebula才是真正服务于用户的。</p>
<h3 id="整体架构的区别"><a href="#整体架构的区别" class="headerlink" title="整体架构的区别"></a>整体架构的区别</h3><p>这个区别对于技术人员来说就更感兴趣了。因为刚开始的定位不同，自然架构也是天壤之别，先上两张图直观感受下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20170912openstack-arch.jpg" alt="openstack-arch"></p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20170912-opennebula-arch.jpg" alt="opennebula"></p>
<p>一张图就可以大致了解到，如今的OpenStack俨然是一个庞然大物，不过好在各个子项目分工明确，划分成层之后如下图（图比较老了，有些项目已经毕业了）：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20170913-openstack-layer.jpg" alt="openstack-arch"></p>
<p>再看下具体部署情况：</p>
<p>openstack的典型部署架构：<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20170913085313-openstack-deploy.jpg" alt="openstack-deploy"></p>
<p>opennebula的典型部署架构：<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20170913085402-open-nebula-deploy.jpg" alt="opennebula-deploy"></p>
<p>可以看出，在计算节点openstack要部署很多agent(包括nova-compute)，而opennebula只要保证可以SSH连接以及有hypervisor就可以，是一种无侵入式的设计。</p>
<h3 id="支持服务的区别"><a href="#支持服务的区别" class="headerlink" title="支持服务的区别"></a>支持服务的区别</h3><p>网上一张截图（时效性不保证）：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20170913090037-feature.jpg" alt="opennebula-vs-openstack"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Openstack因为各大厂商力推的原因，走的是大而全的路线，你能想到的云计算服务，openstack基本都有对应的子项目，但也造成了架构复杂，学习路线陡峭，后期维护成本高等现象。OpenNebula走的是小而美的路线，无侵入式设计，架构简单，容易上手，但是因为保证无侵入式，所以要大量调用shell，动用的编程语言包括C++,ruby，shell等，项目代码相对混乱，代码可读性不高。<br>综上所述，如果是建一个求稳定性，且物理环境一致，规模相对较大的私有云，Openstack是首选，而如果计算节点系统不一致，特别是计算节点有公有云的情况(也就是搭建混合云)，可以考虑OpenNebula。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://opennebula.org/opennebula-vs-openstack-user-needs-vs-vendor-driven/" target="_blank" rel="noopener">OpenNebula vs. OpenStack: User Needs vs. Vendor Driven </a></p>
<p><a href="https://opennebula.org/comparing-opennebula-and-openstack-two-different-views-on-the-cloud/" target="_blank" rel="noopener">Comparing OpenNebula and OpenStack: Two Different Views on the Cloud</a></p>
<p><a href="https://www.slideshare.net/opennebula/opennebula-414-handson-tutorial?ref=https://opennebula.org/documentation/tutorials/" target="_blank" rel="noopener">OpenNebula 4.14 Hands-on Tutorial</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/22/install-k8s.1.6.3-ha-version/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/22/install-k8s.1.6.3-ha-version/" itemprop="url">
                  Kubernetes-- 手动安装高可用k8s1.6问题汇总
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-22T14:25:20+08:00">
                2017-08-22
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/22/install-k8s.1.6.3-ha-version/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/22/install-k8s.1.6.3-ha-version/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="kubernetes-HA-整体架构"><a href="#kubernetes-HA-整体架构" class="headerlink" title="kubernetes HA 整体架构"></a>kubernetes HA 整体架构</h2><p>k8s的HA相对于<a href="https://zhangchenchen.github.io/2017/04/14/openstack-ha/">openstack的HA</a>要简单很多，主要包括以下三各方面：</p>
<ul>
<li>etcd的HA:创建HA集群,如果还不放心，可以使用分布式存储系统</li>
<li>apiserver(无状态服务)的HA:双活模式，前面加一个load balance</li>
<li>controller manager 和 scheduler的HA:主备模式。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/20170822-k8s-ha.jpg" alt="k8s-ha"></p>
<p>博主是按照<a href="http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html#42-kubelet部署" target="_blank" rel="noopener">Kubernetes 1.6 高可用集群部署</a>这篇博客一步步安装下来的，基于Kubernetes二进制包手动部署一个高可用的Kubernetes 1.6集群，将启用ApiServer的TLS双向认证和RBAC授权等安全机制，当然，也可以利用kubeadm创建一个k8s cluster后再扩展成高可用的，参考<a href="http://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/" target="_blank" rel="noopener">一步步打造基于Kubeadm的高可用Kubernetes集群</a>。<br>安装过程不再赘述，参考上述博文，主要讲下期间遇到的问题以及解决方法。</p>
<h2 id="etcd高可用集群部署"><a href="#etcd高可用集群部署" class="headerlink" title="etcd高可用集群部署"></a>etcd高可用集群部署</h2><p>在该过程中碰到的一个坑是，在创建tls密钥和证书的过程中。用的工具是cfssl,创建etcd证书签名请求配置文件的时候需要指定node节点的IP，当时希望可以外网访问，就使用的floating-ip,在etcd的systemd unit文件中自然也就使用了floating-ip，然而etcd却一直起不来，查看日志报错： listen tcp 172.16.21.55:2380: bind: cannot assign requested address.</p>
<p>监听端口失败，恍然大悟，openstack中的floating-ip是不会在虚拟机上创建一个新网卡的，而是通过l3-agent的转发实现。将floating-ip更改为内网IP 后问题解决。</p>
<h2 id="Kubernetes各组件TLS证书和密钥"><a href="#Kubernetes各组件TLS证书和密钥" class="headerlink" title="Kubernetes各组件TLS证书和密钥"></a>Kubernetes各组件TLS证书和密钥</h2><p>除了etcd是使用tls双向认证外，这里apiserver也启用了双向认证，这样的话，凡是与apiserver通信的组件都要创建对应的证书与密钥,所有的需要创建的组件包括：</p>
<ul>
<li>kube-apiserver</li>
<li>kubernetes-admin：RBAC相关，该证书拥有访问kube-apiserver的所有权限。</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>kubelet:每个节点都要配置</li>
<li>kube-proxy：每个节点都要配置</li>
</ul>
<h2 id="Kubernetes-Master集群部署"><a href="#Kubernetes-Master集群部署" class="headerlink" title="Kubernetes Master集群部署"></a>Kubernetes Master集群部署</h2><p>我们这里用openstack创建了一个load balance用于apiserver的高可用，如果是在裸机上，也有多种方案，比如，可以使用haproxy+keepalived的方案实现，也可以直接在各节点用nginx做反向代理，参考<a href="https://www.centos.bz/2017/08/k8s-kubernetes-1-7-3-calico-master-ha/#配置 KubeDNS" target="_blank" rel="noopener">kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明</a>。</p>
<h2 id="Kubernetes-Node节点部署"><a href="#Kubernetes-Node节点部署" class="headerlink" title="Kubernetes Node节点部署"></a>Kubernetes Node节点部署</h2><p>在部署Pod Network插件flannel碰到一个问题，执行完create命令后，pod一直处于containercreating状态，describe发现报错信息为：error syncing pod，没有多余的报错信息，而且容器压根就没有创建起来。挣扎了许久，最后直接在/var/log/messaging中暴力搜索了一下syncing pod，终于找到有用的信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.....</span><br><span class="line">Aug 22 13:12:33 host-10-10-10-52 dockerd: time=<span class="string">"2017-08-22T13:12:33.841538881+08:00"</span> level=error msg=<span class="string">"Handler for GET /v1.24/images/gcr.io/google_containers/pause-amd64:3.0/json returned error: No such image: gcr.io/google_containers/pause-amd64:3.0"</span></span><br></pre></td></tr></table></figure></p>
<p>通过翻墙将该镜像pull到本地后，一切解决。关于pause container的作用，直接引用<a href="https://groups.google.com/forum/#!topic/kubernetes-users/jVjv0QK4b_o" target="_blank" rel="noopener">What is the role of ‘pause’ container?</a></p>
<blockquote>
<blockquote>
<blockquote>
<p>The pause container is a container which holds the network namespace for the pod. It does nothing ‘useful’. (It’s actually just a little bit of assembly that goes to sleep and never wakes up)<br>This means that your ‘apache’ container can die, and come back to life, and all of the network setup will still be there. Normally if the last process in a network namespace dies the namespace would be destroyed and creating a new apache container would require creating all new network setup. With pause, you’ll always have that one last thing in the namespace.</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kubernetes.io/docs/admin/high-availability/" target="_blank" rel="noopener">Building High-Availability Clusters</a></p>
<p><a href="http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html#42-kubelet部署" target="_blank" rel="noopener">Kubernetes 1.6 高可用集群部署</a></p>
<p><a href="http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html" target="_blank" rel="noopener">ssl 双向认证和单向认证原理</a></p>
<p><a href="https://www.centos.bz/2017/08/k8s-kubernetes-1-7-3-calico-master-ha/#配置 KubeDNS" target="_blank" rel="noopener">kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/17/kubernetes-authentication-authorization-admission-control/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/17/kubernetes-authentication-authorization-admission-control/" itemprop="url">
                  Kubernetes-- 漫谈kubernetes 中的认证 & 授权 & 准入机制
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-17T15:37:30+08:00">
                2017-08-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/17/kubernetes-authentication-authorization-admission-control/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/17/kubernetes-authentication-authorization-admission-control/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>首先需要了解这三种机制的区别：简单来说，认证(Authenticating)是对客户端的认证，通俗点就是用户名密码验证，授权(Authorization)是对资源的授权，k8s中的资源无非是容器，最终其实就是容器的计算，网络，存储资源，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。准入(Admission Control)机制是一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。<br>在k8s中，这三种机制如下图：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-17-k8s-authorition.png" alt="k8s-authorization"></p>
<p>k8s的整体架构也是一个微服务的架构，所有的请求都是通过一个GateWay，也就是kube-apiserver这个组件（对外提供REST服务），由图中可以看出，k8s中客户端有两类，一种是普通用户，一种是集群内的Pod，这两种客户端的认证机制略有不同，后文会详述。但无论是哪一种，都需要依次经过认证，授权，准入这三个机制。</p>
<h2 id="kubernetes-中的认证机制"><a href="#kubernetes-中的认证机制" class="headerlink" title="kubernetes 中的认证机制"></a>kubernetes 中的认证机制</h2><p>需要注意的是，kubernetes虽然提供了多种认证机制，但并没有提供user 实体信息的存储，也就是说，账户体系需要我们自己去做维护。当然，也可以接入第三方账户体系（如谷歌账户），也可以使用开源的keystone去做整合。kubernetes 支持多种认证机制，可以配置成多个认证体制共存，这样，只要有一个认证通过，这个request就认证通过了。下面介绍下官网列举的几种常见认证机制：</p>
<h3 id="X509-Client-Certs"><a href="#X509-Client-Certs" class="headerlink" title="X509 Client Certs"></a>X509 Client Certs</h3><p>也叫作双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。默认在kubeadm创建的集群中是enabled的，可以在master node上查看kube-apiserver的pod配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/manifests/kube-apiserver.json</span></span><br><span class="line">.................</span><br><span class="line">containers<span class="string">": [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        "</span>name<span class="string">": "</span>kube-apiserver<span class="string">",</span></span><br><span class="line"><span class="string">        "</span>image<span class="string">": "</span>gcr.io/google_containers/kube-apiserver-amd64:v1.5.2<span class="string">",</span></span><br><span class="line"><span class="string">        "</span><span class="built_in">command</span><span class="string">": [</span></span><br><span class="line"><span class="string">          "</span>kube-apiserver<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--insecure-bind-address=127.0.0.1<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--service-cluster-ip-range=10.96.0.0/12<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--service-account-key-file=/etc/kubernetes/pki/apiserver-key.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--client-ca-file=/etc/kubernetes/pki/ca.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--tls-cert-file=/etc/kubernetes/pki/apiserver.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--token-auth-file=/etc/kubernetes/pki/tokens.csv<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--secure-port=6443<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--allow-privileged<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--advertise-address=192.168.61.100<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--anonymous-auth=<span class="literal">false</span><span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--etcd-servers=http://127.0.0.1:2379<span class="string">"</span></span><br><span class="line"><span class="string">        ],</span></span><br></pre></td></tr></table></figure>
<p>相关的三个启动参数：</p>
<ul>
<li>client-ca-file: 指定CA根证书文件为/etc/kubernetes/pki/ca.pem，内置CA公钥用于验证某证书是否是CA签发的证书</li>
<li>tls-private-key-file: 指定ApiServer私钥文件为/etc/kubernetes/pki/apiserver-key.pem</li>
<li>tls-cert-file：指定ApiServer证书文件为/etc/kubernetes/pki/apiserver.pem</li>
</ul>
<p>只要有这三个启动参数，就说明开启了https的认证方式，这时，如果在集群外访问 <a href="https://masterIP:6443/api" target="_blank" rel="noopener">https://masterIP:6443/api</a> 会提示Unauthorized，只有在客户端配置相关认证才可以访问,客户端的认证证书生成与操作可以参考<a href="https://kubernetes.io/docs/admin/authentication/#x509-client-certs" target="_blank" rel="noopener">Creating Certificates</a>。证书的生成是kubeadm使用openssl自动生成的，如果是手动配置双向认证，相对比较麻烦，主要配置流程如下：</p>
<ul>
<li>生成根证书、API Server服务端证书、服务端私钥、各个组件所用的客户端证书和客户端私钥。</li>
<li>修改 Kubernetes 各个服务进程的启动参数，启用双向认证模式.</li>
</ul>
<p>详细配置可以参考<a href="http://www.cnblogs.com/breg/p/5923604.html" target="_blank" rel="noopener">Kubernetes集群安全配置案例</a></p>
<p>注意，在启动参数中还有一个参数：–insecure-bind-address=127.0.0.1，这个参数主要用与master node上的其他核心组件，比如kube-scheduler，kube-controller-manager通过masterIP:8080与APIserver直接通信，而不用通过双向认证。这一点可以从他们的启动参数–master=127.0.0.1:8080看出。</p>
<h3 id="Static-Token-File"><a href="#Static-Token-File" class="headerlink" title="Static Token File"></a>Static Token File</h3><p>静态token文件认证，同样，在kubeadm创建的集群中也是默认enabled的，比如，上面的apiserver启动参数中，我们可以看到有参数 ：–token-auth-file=/etc/kubernetes/pki/tokens.csv ，这个静态token文件的格式为 token,user,uid,”group1,group2,group3”，如下示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/pki/tokens.csv</span></span><br><span class="line">7db2f1c02d721320,kubeadm-node-csr,0615e0ac-7d70-11e7-ad94-fa163eb9dfdd,system:kubelet-bootstrap</span><br></pre></td></tr></table></figure>
<p>客户端请求的时候需要在http header中加入：”Authorization: Bearer THETOKEN”，如下实例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -k --header <span class="string">"Authorization: Bearer 7db2f1c02d721320"</span> https://192.168.21.34:6443/api</span><br></pre></td></tr></table></figure>
<p>或者使用brctl:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl --server=https://192.168.21.34:6443 \</span><br><span class="line">--token=7db2f1c02d721320 \</span><br><span class="line">--insecure-skip-tls-verify=<span class="literal">true</span> \</span><br><span class="line">cluster-info</span><br></pre></td></tr></table></figure>
<p>注意，如果该静态token文件更改的话，需要重启apiserver。</p>
<h3 id="Bootstrap-Tokens"><a href="#Bootstrap-Tokens" class="headerlink" title="Bootstrap Tokens"></a>Bootstrap Tokens</h3><p>bootstrap token认证目前处于alpha阶段，目前主要是kubeadm创建k8s集群时使用。使用这种认证方式，k8s会动态的管理一种type为bootstrap token的token，这些token作为secret放在kube-system namespace中。controller-manager中的tokencleaner controller会在bootstrap token 过期时进行删除。<br>使用这种认证方式，apiserver的启动参数中需要有–experimental-bootstrap-token-auth，Controller Manager的启动参数中有–controllers=*,tokencleaner 类似参数。</p>
<h3 id="Static-Password-File"><a href="#Static-Password-File" class="headerlink" title="Static Password File"></a>Static Password File</h3><p>比较简单，kubeadm默认没有开启，生产环境也不建议使用。<br>apiserver启动参数指定–basic_auth_file=/etc/kubernetes/basic_auth。然后在指定的文件中加入用户名密码等就可以了，文件格式为password,user,uid,”group1,group2,group3”。</p>
<h3 id="Service-Account-Tokens"><a href="#Service-Account-Tokens" class="headerlink" title="Service Account Tokens"></a>Service Account Tokens</h3><p>Service Account Token 是一种比较特殊的认证机制，适用于上文中提到的pod内部服务需要访问apiserver的认证情况，默认enabled。<br>还是看上文中apiserver 的启动配置参数有–service-account-key-file=/etc/kubernetes/pki/apiserver-key.pem，如果没有指明文件，默认使用–tls-private-key-file的值，即API Server的私钥。<br>service accout本身是作为一种资源在k8s集群中，我们可以通过命令行获取：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master pki]<span class="comment"># kubectl get serviceaccount --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME        SECRETS   AGE</span><br><span class="line">default       default     1         7d</span><br><span class="line"><span class="keyword">for</span>-test      default     1         3d</span><br><span class="line">kube-system   default     1         7d</span><br><span class="line">kube-system   weave-net   1         7d</span><br><span class="line">sock-shop     default     1         7d</span><br></pre></td></tr></table></figure>
<p>可以看到k8s集群为所有的namespace创建了一个默认的service account，利用命令describe会发现service account只是关联了一个secret作为token，也就是service-account-token。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master pki]<span class="comment"># kubectl describe serviceaccount/default -n kube-system</span></span><br><span class="line">Name:           default</span><br><span class="line">Namespace:      kube-system</span><br><span class="line">Labels:         &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Image pull secrets:     &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Mountable secrets:      default-token-nbldr</span><br><span class="line"></span><br><span class="line">Tokens:                 default-token-nbldr</span><br><span class="line"></span><br><span class="line">[root@k8s-master pki]<span class="comment">#  kubectl get secret default-token-nbldr -o yaml -n kube-system</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFM0</span><br><span class="line">  ........................略....................</span><br><span class="line">  namespace: a3ViZS1zeXN0ZW0=</span><br><span class="line">  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0..................................</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/service-account.name: default</span><br><span class="line">    kubernetes.io/service-account.uid: 67aae699-7d70-11e7-a8a9-fa163eb9dfdd</span><br><span class="line">  creationTimestamp: 2017-08-10T02:05:38Z</span><br><span class="line">  name: default-token-nbldr</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: <span class="string">"88"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/secrets/default-token-nbldr</span><br><span class="line">  uid: 67b20b73-7d70-11e7-a8a9-fa163eb9dfdd</span><br><span class="line"><span class="built_in">type</span>: kubernetes.io/service-account-token</span><br></pre></td></tr></table></figure>
<p>可以看到service-account-token的secret资源包含的数据有三部分：</p>
<ul>
<li><p>ca.crt，这是API Server的CA公钥证书，用于Pod中的Process对API Server的服务端数字证书进行校验时使用的；</p>
</li>
<li><p>namespace，这是Secret所在namespace的值的base64编码：# echo -n “kube-system”|base64 =&gt; “a3ViZS1zeXN0ZW0=”</p>
</li>
<li><p>token：该token就是由service-account-key-file的值签署(sign)生成。</p>
</li>
</ul>
<p>这种认证方式主要由k8s集群自己管理，用户用到的情况比较少。我们创建一个pod时，默认就会将该namespace对应的默认service account token mount到Pod中，所以无需我们操作便可以直接与apiserver通信，相关示例参考<a href="http://tonybai.com/2017/03/03/access-api-server-from-a-pod-through-serviceaccount/" target="_blank" rel="noopener">在Kubernetes Pod中使用Service Account访问API Server</a>，当然也可以指定多个service account token,参考<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" rel="noopener">Configure Service Accounts for Pods</a>。</p>
<h3 id="OpenID-Connect-Tokens"><a href="#OpenID-Connect-Tokens" class="headerlink" title="OpenID Connect Tokens"></a>OpenID Connect Tokens</h3><p>类似 OAuth2的认证方式，大致认证过程如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-17-k8s-openid-token.jpg" alt="openID"></p>
<p>除了以上几种认证方式外，还有几种比如Webhook Token Authentication，Keystone Password等，详情见<a href="https://kubernetes.io/docs/admin/authentication/#x509-client-certs" target="_blank" rel="noopener">官网</a>。</p>
<h2 id="kubernetes-中的授权机制"><a href="#kubernetes-中的授权机制" class="headerlink" title="kubernetes 中的授权机制"></a>kubernetes 中的授权机制</h2><p>k8s中的授权策略也支持开启多个授权插件，只要一个验证通过即可。k8s授权处理主要是根据以下请求属性：</p>
<ul>
<li>user, group, extra</li>
<li>API、请求方法（如get、post、update、patch和delete）和请求路径（如/api）</li>
<li>请求资源和子资源</li>
<li>Namespace</li>
<li>API Group</li>
</ul>
<p>目前k8s支持的授权模式主要有以下几种：</p>
<ul>
<li>Node Authorization</li>
<li>ABAC Authorization</li>
<li>RBAC Authorization</li>
<li>Webhook Authorization</li>
</ul>
<h3 id="Node-Authorization"><a href="#Node-Authorization" class="headerlink" title="Node Authorization"></a>Node Authorization</h3><p>1.7+版本才release的一种授权机制，通过配合NodeRestriction control准入控制插件来限制kubelet访问node，endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源。配置方式为：<br>–authorization-mode=Node,RBAC –admission-control=…,NodeRestriction,…</p>
<h3 id="ABAC-Authorization"><a href="#ABAC-Authorization" class="headerlink" title="ABAC Authorization"></a>ABAC Authorization</h3><p>ABAC(Attribute-based access control),使用这种模式需要配置参数：<br>–authorization-mode=ABAC  –authorization-policy-file=SOME_FILENAME。<br>这种模式的实现相对比较生硬，就是在master node保存一份policy文件，指定不用用户（或用户组）对不同资源的访问权限,当修改该文件后，需要重启apiserver,跟openstack 的ABAC类似。policy文件的格式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Alice can do anything to all resources:</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"apiVersion"</span>: <span class="string">"abac.authorization.kubernetes.io/v1beta1"</span>,</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"spec"</span>: &#123;</span><br><span class="line">        <span class="string">"user"</span>: <span class="string">"alice"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"resource"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"apiGroup"</span>: <span class="string">"*"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># Kubelet can read any pods:</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"apiVersion"</span>: <span class="string">"abac.authorization.kubernetes.io/v1beta1"</span>,</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"spec"</span>: &#123;</span><br><span class="line">        <span class="string">"user"</span>: <span class="string">"kubelet"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"resource"</span>: <span class="string">"pods"</span>,</span><br><span class="line">        <span class="string">"readonly"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Kubelet can read and write events:</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"apiVersion"</span>: <span class="string">"abac.authorization.kubernetes.io/v1beta1"</span>,</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"spec"</span>: &#123;</span><br><span class="line">        <span class="string">"user"</span>: <span class="string">"kubelet"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"resource"</span>: <span class="string">"events"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用这种模式需要配置参数：<br>–authorization-mode=ABAC  –authorization-policy-file=SOME_FILENAME</p>
<h3 id="RBAC-Authorization"><a href="#RBAC-Authorization" class="headerlink" title="RBAC Authorization"></a>RBAC Authorization</h3><p>RBAC（Role-Based Access Control）依然处于Beta阶段，通过启动参数–authorization-mode=RBAC，使用kubeadm安装k8s默认会enabled。<br>RBAC API定义了四个资源对象用于描述RBAC中用户和资源之间的连接权限：</p>
<ul>
<li>Role</li>
<li>ClusterRole</li>
<li>RoleBinding</li>
<li>ClusterRoleBinding</li>
</ul>
<p>Role是定义在某个Namespace下的资源，在这个具体的Namespace下使用。 ClusterRole与Role相似，只是ClusterRole是整个集群范围内使用的。<br>RoleBinding把Role绑定到账户主体Subject，让Subject继承Role所在namespace下的权限。 ClusterRoleBinding把ClusterRole绑定到Subject，让Subject集成ClusterRole在整个集群中的权限。</p>
<p>我们可以通过kubectl命令获取对应的Role相关资源进行增删改查：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl get roles --all-namespaces</span><br><span class="line"></span><br><span class="line">kubectl get ClusterRoles</span><br><span class="line"></span><br><span class="line">kubectl get rolebinding --all-namespaces</span><br><span class="line"></span><br><span class="line">kubectl get clusterrolebinding</span><br></pre></td></tr></table></figure></p>
<p>API Server已经创建一系列ClusterRole和ClusterRoleBinding。这些资源对象中名称以system:开头的，表示这个资源对象属于Kubernetes系统基础设施。 也就说RBAC默认的集群角色已经完成足够的覆盖，让集群可以完全在 RBAC的管理下运行。 修改这些资源对象可能会引起未知的后果，例如对于system:node这个ClusterRole定义了kubelet进程的权限，如果这个角色被修改，可能导致kubelet无法工作。</p>
<h3 id="Webhook-Authorization"><a href="#Webhook-Authorization" class="headerlink" title="Webhook Authorization"></a>Webhook Authorization</h3><p>用户在外部提供 HTTPS 授权服务，然后配置 apiserver 调用该服务去进行授权。apiserver配置参数：<br>–authorization-webhook-config-file=SOME_FILENAME<br>配置文件的格式跟kubeconfig的格式类似，具体参考<a href="https://kubernetes.io/docs/admin/authorization/webhook/" target="_blank" rel="noopener">官方文档</a></p>
<h2 id="kubernetes-中的准入机制"><a href="#kubernetes-中的准入机制" class="headerlink" title="kubernetes 中的准入机制"></a>kubernetes 中的准入机制</h2><p>Kubernetes的Admission Control实际上是一个准入控制器(Admission Controller)插件列表，发送到APIServer的请求都需要经过这个列表中的每个准入控制器插件的检查，如果某一个控制器插件准入失败，就准入失败。<br>控制器插件如下：</p>
<ul>
<li>AlwaysAdmit：允许所有请求通过</li>
<li>AlwaysPullImages：在启动容器之前总是去下载镜像，相当于每当容器启动前做一次用于是否有权使用该容器镜像的检查</li>
<li>AlwaysDeny：禁止所有请求通过，用于测试</li>
<li>DenyEscalatingExec：拒绝exec和attach命令到有升级特权的Pod的终端用户访问。如果集中包含升级特权的容器，而要限制终端用户在这些容器中执行命令的能力，推荐使用此插件</li>
<li>ImagePolicyWebhook</li>
<li>ServiceAccount：这个插件实现了serviceAccounts等等自动化，如果使用ServiceAccount对象，强烈推荐使用这个插件</li>
<li>SecurityContextDeny：将Pod定义中定义了的SecurityContext选项全部失效。SecurityContext包含在容器中定义了操作系统级别的安全选型如fsGroup，selinux等选项</li>
<li>ResourceQuota：用于namespace上的配额管理，它会观察进入的请求，确保在namespace上的配额不超标。推荐将这个插件放到准入控制器列表的最后一个。ResourceQuota准入控制器既可以限制某个namespace中创建资源的数量，又可以限制某个namespace中被Pod请求的资源总量。ResourceQuota准入控制器和ResourceQuota资源对象一起可以实现资源配额管理。</li>
<li>LimitRanger：用于Pod和容器上的配额管理，它会观察进入的请求，确保Pod和容器上的配额不会超标。准入控制器LimitRanger和资源对象LimitRange一起实现资源限制管理</li>
<li>NamespaceLifecycle：当一个请求是在一个不存在的namespace下创建资源对象时，该请求会被拒绝。当删除一个namespace时，将会删除该namespace下的所有资源对象</li>
<li>DefaultStorageClass</li>
<li>DefaultTolerationSeconds</li>
<li>PodSecurityPolicy</li>
</ul>
<p>当Kubernetes版本&gt;=1.6.0，官方建议使用这些插件：<br>–admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds<br>当Kubernetes版本&gt;=1.4.0，官方建议使用这些插件：<br>–admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota<br>以上是标准的准入插件，如果是自己定制的话，k8s1.7版 出了两个alpha features, Initializers 和 External Admission Webhooks，详情可以参考<a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/" target="_blank" rel="noopener">Dynamic Admission Control</a>.</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kubernetes.io/docs/admin/authentication/#x509-client-certs" target="_blank" rel="noopener">Authenticating</a></p>
<p><a href="https://kubernetes.io/docs/admin/authorization/" target="_blank" rel="noopener">authorization</a></p>
<p><a href="http://blog.frognew.com/2017/01/kubernetes-api-server-authc.html" target="_blank" rel="noopener">Kubernetes集群安全：Api Server认证</a></p>
<p><a href="http://blog.frognew.com/2017/05/kubernetes-apiserver-admission-control.html" target="_blank" rel="noopener">Kubernetes集群安全：准入控制Admission Control</a></p>
<p><a href="http://blog.frognew.com/2017/04/kubernetes-1.6-rbac.html" target="_blank" rel="noopener">Kubernetes 1.6新特性学习：RBAC授权</a></p>
<p><a href="http://tonybai.com/2017/03/03/access-api-server-from-a-pod-through-serviceaccount/" target="_blank" rel="noopener">在Kubernetes Pod中使用Service Account访问API Server</a></p>
<p><a href="http://blog.csdn.net/yan234280533/article/details/76359199" target="_blank" rel="noopener"> kubernetes安全控制认证与授权(二)</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/14/install-kubernete-by-kubreadm/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/14/install-kubernete-by-kubreadm/" itemprop="url">
                  Kubernete-- 利用kubeadm 搭建一个kubernate集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-14T15:38:10+08:00">
                2017-08-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/14/install-kubernete-by-kubreadm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/14/install-kubernete-by-kubreadm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>利用 kubeadm 搭建一个四节点的k8s测试集群</li>
<li>利用harbor搭建一个单节点的私有镜像仓库</li>
<li>k8s集群与私有镜像仓库整合</li>
<li>部署dashboard</li>
</ul>
<h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>准备以下5个节点，一个为k8s的master节点，3个为node节点，最后一个作为私有仓库镜像，系统为centos7.2：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-14-node.png" alt="five-nodes"></p>
<p>注：k8s的安装方式有很多，kubeadm安装方式是独立节点安装的官方推荐方式，简单可重复，但不适用于生产环境，因为没有做HA，不过可以在安装完之后继续优化做HA，参考<a href="http://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/" target="_blank" rel="noopener">一步步打造基于Kubeadm的高可用Kubernetes集群</a>,后续会跟进这一块。</p>
<h2 id="kubernete-集群安装"><a href="#kubernete-集群安装" class="headerlink" title="kubernete 集群安装"></a>kubernete 集群安装</h2><h3 id="k8s所有节点需要执行的操作"><a href="#k8s所有节点需要执行的操作" class="headerlink" title="k8s所有节点需要执行的操作"></a>k8s所有节点需要执行的操作</h3><p>所有节点都要安装以下组件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker：容器运行时，被Kubernetes依赖</span><br><span class="line">kubelet：Kubernetes核心组件，运行在集群中的所有节点上，用来启动容器和pods</span><br><span class="line">kubectl：命令行工具，k8s客户端，用来控制集群，只需要安装到kube-master上,当然，也可以安装到其他节点，然后配置指定master。</span><br><span class="line">kubeadm：集群安装工具</span><br></pre></td></tr></table></figure>
<p>首先，安装docker,k8s官方建议版本为1.12，1.13以及17.03+版本还没有测试。所以这里也安装1.12版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tee /etc/yum.repos.d/docker.repo &lt;&lt;-<span class="string">'EOF'</span></span><br><span class="line">[dockerrepo]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://yum.dockerproject.org/repo/main/centos/7/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://yum.dockerproject.org/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">yum update -y </span><br><span class="line"></span><br><span class="line">yum install -y docker-engine-1.12.6 docker-engine-selinux-1.12.6</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> docker &amp;&amp; systemctl start docker</span><br></pre></td></tr></table></figure>
<p>注：这里有个小坑，就是k8s dashboard在某些版本RH内核下会启动失败，参考<a href="https://github.com/rancher/rancher/issues/7436" target="_blank" rel="noopener">issue</a>。</p>
<p>接下来，安装kubectl, kubelet, kubeadm以及一些依赖包。</p>
<p>先把依赖包装上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ebtables socat</span><br></pre></td></tr></table></figure></p>
<p>kubectl 的安装比较简单，参考<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">Install and Set Up kubectl</a>,可以直接下载可执行文件然后添加权限，扔到master节点的/usr/local/bin/目录下即可，注意版本要与k8s版本匹配(注：也可以直接在下文同其他三个组件一起rpm包安装)。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl</span><br><span class="line"></span><br><span class="line">chmod +x ./kubectl</span><br><span class="line"></span><br><span class="line">sudo mv ./kubectl /usr/<span class="built_in">local</span>/bin/kubectl</span><br></pre></td></tr></table></figure>
<p>因为kubelet, kubeadm的rpm安装包在gce上，需要翻墙。</p>
<p>如果服务器可以翻墙，可以直接通过yum命令安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</span><br><span class="line">        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y kubelet kubeadm</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure></p>
<p>如果不能翻墙，只能先下载下来，然后安装，需要安装的rpm包url地址可以在这个网页中找到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/primary.xml</span><br></pre></td></tr></table></figure></p>
<p>我们这里只需要安装三个rpm包，kubeadm, kubelet以及kubernetes-cni，可以直接搜索上面的网页然后找到合适版本的rpm包。我们这里安装最新版本1.7.3,对应的地址如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">https://packages.cloud.google.com/yum/pool/f7ec56b0f36a81c0f91bcf26e05f23088082b468b77dac576dc505444dd8cd48-kubeadm-1.7.3-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">https://packages.cloud.google.com/yum/pool/28b76e6e1c2ec397a9b6111045316a0943da73dd5602ee8e53752cdca62409e6-kubelet-1.7.3-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">https://packages.cloud.google.com/yum/pool/e7a4403227dd24036f3b0615663a371c4e07a95be5fee53505e647fd8ae58aa6-kubernetes-cni-0.5.1-0.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>将这三个rpm包打包上传到四个节点上，并安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tar xzvf /tmp/kubernetes-el7-x86_64.tar.gz</span><br><span class="line">kubernetes-el7-x86_64/</span><br><span class="line">kubernetes-el7-x86_64/567600102f687e0f27bd1fd3d8211ec1cb12e71742221526bb4e14a412f4fdb5-kubernetes-cni-0.5.0.1-0.07a8a2.x86_64.rpm</span><br><span class="line">kubernetes-el7-x86_64/5612db97409141d7fd839e734d9ad3864dcc16a630b2a91c312589a0a0d960d0-kubeadm-1.6.0-0.alpha.0.2074.a092d8e0f95f52.x86_64.rpm</span><br><span class="line">kubernetes-el7-x86_64/8a299eb1db946b2bdf01c5d5c58ef959e7a9d9a0dd706e570028ebb14d48c42e-kubelet-1.5.1-0.x86_64.rpm</span><br><span class="line">kubernetes-el7-x86_64/93af9d0fbd67365fa5bf3f85e3d36060138a62ab77e133e35f6cadc1fdc15299-kubectl-1.5.1-0.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> kubernetes-el7-x86_64/</span><br><span class="line"></span><br><span class="line">rpm -ivh *</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure>
<p>接下来开始基于Kubeadm 创建k8s集群，不过在开始之前，我们先准备下需要用到的镜像，因为kubeadm创建的k8s集群中的kub-api, kube-scheduler, kube-proxy, kube-controller-manager,etcd等服务都是直接拉取镜像跑在k8s集群中，为了避免安装过程中下载镜像浪费太多时间，这里先把镜像下载好。各个版本需要下载的镜像版本也不一样。参考如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-14-k8s-image.png" alt="k8s-image"></p>
<p>我们直接用的最新版1.7.3，如果服务器可以翻墙，直接拉取镜像：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">images=(kube-proxy-amd64:v1.7.3 kube-scheduler-amd64:v1.7.3 kube-controller-manager-amd64:v1.7.3 kube-apiserver-amd64:v1.7.3 etcd-amd64:3.0.17 k8s-dns-sidecar-amd64:1.14.4 pause-amd64:3.0 k8s-dns-kube-dns-amd64:1.14.4 k8s-dns-dnsmasq-nanny-amd64:1.14.4)</span><br><span class="line"><span class="keyword">for</span> imageName <span class="keyword">in</span> <span class="variable">$&#123;images[@]&#125;</span> ; <span class="keyword">do</span></span><br><span class="line">  docker pull gcr.io/google_containers/<span class="variable">$imageName</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>如果不能翻墙，可以先翻墙下载下来，然后push到dockerhub上，再pull下来,注意pull下来之后，还是要更改tag为gcr.io/google_containers/$imageName形式。</p>
<h3 id="master-节点安装"><a href="#master-节点安装" class="headerlink" title="master 节点安装"></a>master 节点安装</h3><p>在master节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init</span><br></pre></td></tr></table></figure>
<p>执行完成后，会输出一个token，node节点安装时会用到。<br>这里有一个小坑：该过程一直卡在“[apiclient] Created API client, waiting for the control plane to become ready” ，可以去message里找相关log，一般是两种情况导致，一种是用了proxy，一种是<a href="https://github.com/kubernetes/kubernetes/issues/43800" target="_blank" rel="noopener">cgroup-driver配置错误</a>，我这边有一次是因为下载的镜像不对，kubeadm默认应该是安转最新版本，比如kubeadm1.6.x会安装1.6.9的相关组件（api-server-1.6.9.controller-manager-1.6.9等），而kubeadm1.7.x会默认安装1.7.x里面的最高版本（此时是1.7.4），所以要下载合适版本的镜像。</p>
<h3 id="node-节点安装"><a href="#node-节点安装" class="headerlink" title="node 节点安装"></a>node 节点安装</h3><p>在各node节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join --token=976234.e91451d4305bc282 172.16.21.53</span><br></pre></td></tr></table></figure>
<p>全部执行完成后，在master节点验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                   STATUS         AGE</span><br><span class="line">k8s-master.novalocal   Ready,master   4d</span><br><span class="line">k8s-node1.novalocal    Ready          4d</span><br><span class="line">k8s-node2.novalocal    Ready          4d</span><br><span class="line">k8s-node3.novalocal    Ready          4d</span><br></pre></td></tr></table></figure>
<h3 id="部署pod网络"><a href="#部署pod网络" class="headerlink" title="部署pod网络"></a>部署pod网络</h3><p>这里选择Weave Net。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl apply -f https://git.io/weave-kube</span></span><br></pre></td></tr></table></figure>
<p>等待一段时间，利用下列命令查看部署情况。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods --all-namespaces</span></span><br></pre></td></tr></table></figure>
<h3 id="部署sock-shop微服务demo"><a href="#部署sock-shop微服务demo" class="headerlink" title="部署sock-shop微服务demo"></a>部署sock-shop微服务demo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl create namespace sock-shop</span></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl apply -n sock-shop -f "https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span></span><br></pre></td></tr></table></figure>
<p>查看服务部署情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -n sock-shop</span></span><br></pre></td></tr></table></figure></p>
<p>访问172.16.21.34:30001验证。</p>
<h2 id="Harbor-安装"><a href="#Harbor-安装" class="headerlink" title="Harbor 安装"></a>Harbor 安装</h2><p>比较简单，参考<a href="https://github.com/vmware/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">harbor doc</a>。</p>
<p>注意：docker 默认连接镜像使用https，而harbor默认安装是走的http，所以需要修改/etc/docker/daemon.json，添加</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"registry-mirrors"</span>: [<span class="string">"&lt;your accelerate address&gt;"</span>],</span><br><span class="line">    <span class="string">"insecure-registries"</span>: [<span class="string">"172.16.21.44"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="k8s-添加私有镜像"><a href="#k8s-添加私有镜像" class="headerlink" title="k8s 添加私有镜像"></a>k8s 添加私有镜像</h2><p>官方给出了三种解决方案：</p>
<ul>
<li>在node节点配置私有镜像的认证登录文件，其实相当于在node本地执行docker login后，在/.docker目录下生成的一个config.json文件。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ~/.docker/config.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"auths"</span>: &#123;</span><br><span class="line">        <span class="string">"registry.cn-hangzhou.aliyuncs.com/xxxx/rbd-rest-api"</span>: &#123;</span><br><span class="line">            <span class="string">"auth"</span>: <span class="string">"xxxxyyyyzzzz"</span>   <span class="comment">#一个base64编码结果，不太安全</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这种方法比较繁琐，而且不安全，不推荐。</p>
<ul>
<li>利用kubectl创建docker-registry的secret</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL</span><br><span class="line"></span><br><span class="line">kubectl get secret --all-namespaces   <span class="comment">#查看创建的secret</span></span><br></pre></td></tr></table></figure>
<p>在写dockerfile的时候指定imagePullSecrets即可，示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ./deployment-with-secret.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment-from-harbor</span><br><span class="line">  namespace: kube-public</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-for-test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx-for-test</span><br><span class="line">        image: 172.16.21.253:10080/aisino-lib/docker.io/nginx:latest</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-k8s-secret</span><br></pre></td></tr></table></figure>
<ul>
<li>通过secret yaml文件创建pull image所用的secret,其实跟上述方法类似，不过是用yaml文件创建的secret.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat myregistrykey.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: myregistrykey</span><br><span class="line">  namespace: awesomeapps</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: &#123;base64 -w 0 ~/.docker/config.json&#125;</span><br><span class="line"><span class="built_in">type</span>: kubernetes.io/dockerconfigjson</span><br></pre></td></tr></table></figure>
<p>其中，dockerconfigjson后面的数据就是docker login后生成的config.json文件的base64编码输出（-w 0让base64输出在单行上，避免折行）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f myregistrykey.yaml</span><br></pre></td></tr></table></figure>
<p>secret使用方式与第二种方式一样，不过kubectl和yaml创建的两个secret的类型略有不同，前者是kubernetes.io/dockercfg，后者是kubernetes.io/dockerconfigjson。</p>
<h2 id="部署dashboard"><a href="#部署dashboard" class="headerlink" title="部署dashboard"></a>部署dashboard</h2><p>由<a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">README</a> 文件可知，有两种部署方式，如果是没有安装RBAC权限控制的，可以执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://git.io/kube-dashboard</span><br></pre></td></tr></table></figure>
<p>如果有RBAC的，可以执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://git.io/kube-dashboard-no-rbac</span><br></pre></td></tr></table></figure>
<p>kubeadm安装方式自从1.6+版之后自动安装RBAC，所以需要选择第二种。如果权限问题依旧（注：一般是报错serviceaccount:kube-system:default” cannot list statefulsets.apps in the namespace “default”.）可以根据该<a href="https://github.com/kubernetes/dashboard/issues/1803" target="_blank" rel="noopener">issue</a>,添加一个权限。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat dashboard-rbac.yml</span></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: dashboard-admin</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin </span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: default</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create -f dashboard-rbac.yml</span></span><br></pre></td></tr></table></figure>
<p>注：如果想外部可以直接访问dashboard，需要修改下yaml文件，将最后的service配置修改为nodePort,示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.......................</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - nodePort: 30002</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 9090</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br></pre></td></tr></table></figure>
<p>这样便可以直接<a href="http://NODEIP:30002访问。关于port，nodePort" target="_blank" rel="noopener">http://NODEIP:30002访问。关于port，nodePort</a>, targetPort,可以参考<a href="http://blog.csdn.net/xinghun_4/article/details/50492041" target="_blank" rel="noopener">kubernetes中port、target port、node port的对比分析，以及kube-proxy代理</a></p>
<h2 id="部署Heapster-监控与统计"><a href="#部署Heapster-监控与统计" class="headerlink" title="部署Heapster 监控与统计"></a>部署Heapster 监控与统计</h2><p>Heapster是一个容器集群监控和性能分析工具，天然支持Kubernetes和CoreOS。<br>这里使用influxDB作为Heapster的后端存储部署，参考<a href="https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md" target="_blank" rel="noopener">安装文档</a>.<br>首先下载对应版本的相关yaml文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubernetes/heapster/archive/v1.3.0.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压并直接部署即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf v1.3.0.tar.gz</span><br><span class="line"><span class="built_in">cd</span> heapster-1.3.0/deploy/kube-config/influxdb</span><br><span class="line"></span><br><span class="line">kubectl create -f ./*</span><br></pre></td></tr></table></figure>
<p>该过程会pull相关镜像，同样，可以先翻墙pull下来再push到私有镜像仓库再使用。<br>最终完成后，所有pods都running,可以看到dashboard的界面多了仪表盘。</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-16-dashboard.png" alt="dashboard"></p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/" target="_blank" rel="noopener">k8s-doc-Installing kubeadm</a></p>
<p><a href="http://yoyolive.com/2017/02/27/Kubernetes-1-5-3-Local-Install/" target="_blank" rel="noopener">CentOS 7 安装Kubernetes 1.5.3 集群(本地安装)</a></p>
<p><a href="http://tonybai.com/2016/11/16/how-to-pull-images-from-private-registry-on-kubernetes-cluster/?utm_source=rss" target="_blank" rel="noopener">Kubernetes从Private Registry中拉取容器镜像的方法</a></p>
<p><a href="https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry" target="_blank" rel="noopener">k8s-doc-Images</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/08/openstack-resource-segeration/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/08/openstack-resource-segeration/" itemprop="url">
                  Openstack-- openstack 中的资源分离策略
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-08T13:54:20+08:00">
                2017-08-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/08/openstack-resource-segeration/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/08/openstack-resource-segeration/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="为什么做资源的分离"><a href="#为什么做资源的分离" class="headerlink" title="为什么做资源的分离"></a>为什么做资源的分离</h2><p>openstack 作为一个云计算框架，需要统筹计算，存储，网络等资源，本身就已经够复杂了。如果是在一个非常大的环境中，节点众多，且复杂多样，这个时候就有必要根据这些节点的差异做一些逻辑上的分离，以达到不同资源的区分，这样，做水平扩展的时候也可以根据这个逻辑分区针对性的进行扩展。</p>
<h2 id="openstack-中的资源分离策略"><a href="#openstack-中的资源分离策略" class="headerlink" title="openstack 中的资源分离策略"></a>openstack 中的资源分离策略</h2><p>openstack 的资源分离策略一定程度上借鉴了AWS的策略，整体来说，如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-08-rc-se.png" alt="resource-segeration"></p>
<h3 id="Infrastructure-segregation"><a href="#Infrastructure-segregation" class="headerlink" title="Infrastructure segregation"></a>Infrastructure segregation</h3><p>主要是理解Regions, cells, Host aggregates, Availability zones这几个概念。</p>
<ul>
<li>Regions:借鉴自AWS，更像是一个地理上的概念（比如北京的数据中心可以作为一个region,南京的数据中心作为另一个region），每个region有自己独立的endpoint，regions之间完全隔离，不同regions之间可以共享keystone/dashboard。openstack默认新建一个region，即RegionOne，如果还想建立第二个Region，可以利用 keystone endpoint­create 命令添加。<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-08-region1.png" alt="region"></li>
</ul>
<ul>
<li>Cells:cell主要是为了解决openstack 的扩展性以及规模瓶颈而引进的概念。当openstack达到一定规模后，依赖比较强的database以及AMQP便成为整个系统的瓶颈，引入cell后，每个cell有自己独立的database和AMQP。公司云平台只是一个小云，没有引入，所以不去深究。cell目前是有v1，v2两版，实现方式差异比较大，感兴趣可以参考<a href="https://www.ustack.com/news/what-is-nova-cells-v2/?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">Nova Cells V2如何帮助OpenStack集群突破性能瓶颈？</a>。</li>
<li>Host aggregates &amp;&amp; Availability zones：这两个概念有共通点，且要相互配合使用，都用来表示一组节点的集合，简单说，AZ(Availability zone)是一个面向用户的概念，(这里只讨论nova 范畴的AZ,cinder,neutron也有对应的AZ概念),AZ一般依据地址，网络部署或电力配置划分，可以是一个独立的机房，或者一个独立供电的机架等，用户在创建instance的时候可以指定AZ，从而使instance创建在指定的AZ中，而host aggregate是一个面向管理员的概念，主要用来给nova-scheduler调度使用，比如根据某一属性（例如含有固态硬盘）划分一个host aggregate，把所有含有固态硬盘的host都放到该host aggregate中，nova-scheduler调度时指定相关属性就可以调度到对应host aggregate中的host。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2018-08-08-az.png" alt="AZandHG"><br>如上图，有两个地理隔离的Region，四个AZ，以及若干个根据不同属性区分的host aggregate。host aggregate 创建的时候可以指定AZ（如果AZ没有就会自动创建一个AZ），一个host可以属于多个host aggregate，但只能属于一个AZ。如下为一个示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nova aggregate­create storage­optimized storage­optimized-AZ  <span class="comment"># 创建一个host aggregate</span></span><br><span class="line">nova aggregate­<span class="built_in">set</span>­metadata <span class="variable">$aggregateID</span> fast­storage=<span class="literal">true</span> <span class="comment"># 设置 aggregate 的metadate</span></span><br><span class="line">nova aggregate­add­host <span class="variable">$aggregateID</span> host­1 <span class="comment">#添加host到aggregate</span></span><br><span class="line">nova flavor­key <span class="variable">$flavorID</span> <span class="built_in">set</span> fast­storage=<span class="literal">true</span> <span class="comment">#添加flavor的 条件</span></span><br></pre></td></tr></table></figure>
<h3 id="Workload-segregation"><a href="#Workload-segregation" class="headerlink" title="Workload segregation"></a>Workload segregation</h3><p>负载这一块的策略是基于server-group来做的，相对比较简单。注意，这里的server-group不再是host的集合，而是instance的集合。比如，我要创建3个instance，因为这3个instance都比较吃内存，所以想要这3个instance在不同的host上，就可以这样做：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nova server-group-create --policy anti-affinity group-1 <span class="comment"># 创建server-group，注意policy指定策略是不在同一节点</span></span><br><span class="line">nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst1 <span class="comment">#创建instance1 </span></span><br><span class="line">nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst2 <span class="comment">#创建instance2 </span></span><br><span class="line">nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst3 <span class="comment">#创建instance3</span></span><br></pre></td></tr></table></figure>
<p>在nova配置文件中指定scheduler策略，有一个针对server-group的filter叫做ServerGroupAntiAffinityFilter。除了anti-affinity，还有 affinity策略，就是尽量让同一server-group的instance在同一个host上。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://www.jianshu.com/p/613d34ad6d51" target="_blank" rel="noopener">理解openstack中region、cell、availability zone、host aggregate 概念</a></p>
<p><a href="http://happylab.blog.51cto.com/1730296/1739180" target="_blank" rel="noopener">openstack运维实战系列(十二)之nova aggregate资源分组</a></p>
<p><a href="https://www.openstack.org/assets/presentation-media/divideandconquer-2.pdf" target="_blank" rel="noopener">DIVIDE AND CONQUER:RESOURCE SEGREGATION IN THE OPENSTACK<br>CLOUD</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/05/intro-to-etcd/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/05/intro-to-etcd/" itemprop="url">
                  Kubernetes-- 关于ETCD && 服务发现的一些记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-05T14:28:00+08:00">
                2017-08-05
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/05/intro-to-etcd/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/05/intro-to-etcd/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="ETCD简介"><a href="#ETCD简介" class="headerlink" title="ETCD简介"></a>ETCD简介</h2><p>ETCD是一个可靠的键值对分布式存储系统，适用场景是用来存储那些写少读多，结构简单，但又比较重要的数据，这里的比较重要是指要保证数据的高可用，一致性。第一个想到的自然就是配置数据的存储与管理。当然，也可以存储其他满足上述要求的数据，比如在k8s中，ETCD会用来存储k8s各items(pods, rc, rs, deployment等)的状态。随着微服务架构的日益火热，ETCD也常作为服务发现的组件来使用。<br>ETCD 有以下几个特性：</p>
<ul>
<li>使用简单：友好的API(grpc)</li>
<li>安全：支持TLS双向通信</li>
<li>快速：支持10,000 写/秒</li>
<li>可靠： 采用raft协议作分布式选举</li>
</ul>
<h2 id="ETCD-安装"><a href="#ETCD-安装" class="headerlink" title="ETCD 安装"></a>ETCD 安装</h2><p>因为是go写的，直接下载二进制文件执行即可，如果想要尝试最新版的，可以去下载源码，自己build,参考<a href="https://coreos.com/etcd/docs/latest/dl_build.html" target="_blank" rel="noopener">Download and build</a>。<br>如果是构造一个cluster，也比较简单，参考<a href="https://coreos.com/etcd/docs/latest/demo.html" target="_blank" rel="noopener">Demo</a></p>
<p>简单记录下常用的启动参数：</p>
<ul>
<li>name：节点名称，默认为 default，在集群中应该保持唯一，一般使用 hostname</li>
<li>data-dir：服务运行数据保存的路径，默认为 ${name}.etcd</li>
<li>snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘</li>
<li>heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms</li>
<li>eletion-timeout：重新投票的超时时间，如果 follower在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms</li>
<li>listen-peer-urls:和同伴通信的地址，比如 <a href="http://ip:2380" target="_blank" rel="noopener">http://ip:2380</a> ，如果有多个，使用逗号分隔。需要所有节点都能够访问， 所以不要使用 localhost！</li>
<li>listen-client-urls: 对外提供服务的地址：比如 <a href="http://ip:2379,http://127.0.0.1:2379" target="_blank" rel="noopener">http://ip:2379,http://127.0.0.1:2379</a><br>，客户端会连接到这里和 etcd 交互</li>
<li>advertise-client-urls: 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点</li>
<li>initial-advertise-peer-urls: 该节点同伴监听地址，这个值会告诉集群中其他节点</li>
<li>initial-cluster: 集群中所有节点的信息，格式为 node1=<a href="http://ip1:2380,node2=http://ip2:2380,....." target="_blank" rel="noopener">http://ip1:2380,node2=http://ip2:2380,.....</a>. 注意：这里的 node1是节点的–name指定的名字；后面的 ip1:2380是 –initial-advertise-peer-urls<br>指定的值。</li>
<li>initial-cluster-state：新建集群的时候，这个值为 new，假如已经存在的集群，这个值为 existing。</li>
<li>initial-cluster-token：创建集群的 token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点uuid；否则会导致多个集群之间的冲突，造成未知的错误。</li>
</ul>
<p>当然，还有比如tls设置，使用etcd discovery/dns discovery 进行etcd的启动等，配置详情见官网。</p>
<h2 id="ETCD-使用"><a href="#ETCD-使用" class="headerlink" title="ETCD 使用"></a>ETCD 使用</h2><p>一般通过两种方式，rest api或者通过命令行（本质也是rest api）,下面简单介绍下这两种方式。在介绍前，先要弄懂几个概念：</p>
<ul>
<li>member： 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node上的其它应用程序提供服务。</li>
<li>Cluster： Cluster 由多个 member 组成。每个 member 中的 node 遵循 raft共识协议来复制日志。Cluster 接收来自 member的提案消息，将其提交并存储于本地磁盘。</li>
<li>Peer： 同一 Cluster 中的其它 member。</li>
</ul>
<h3 id="命令行方式示例"><a href="#命令行方式示例" class="headerlink" title="命令行方式示例"></a>命令行方式示例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> put foo <span class="string">"Hello World!"</span>  <span class="comment"># 写入操作</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get foo  <span class="comment"># 读取操作</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> --write-out=<span class="string">"json"</span> get foo  <span class="comment"># 以json的方式输出</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get web --prefix  <span class="comment"># 获取所有前缀是web 的key的value</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> del key  <span class="comment"># 删除</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> txn --interactive  <span class="comment"># 将多个命令封装在一个事务中</span></span><br><span class="line"></span><br><span class="line">compares:</span><br><span class="line">value(<span class="string">"user1"</span>) = <span class="string">"bad"</span>      </span><br><span class="line"></span><br><span class="line">success requests (get, put, delete):</span><br><span class="line">del user1  </span><br><span class="line"></span><br><span class="line">failure requests (get, put, delete):</span><br><span class="line">put user1 good</span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> watch stock1  <span class="comment"># 监视某个值</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> lease grant 300 <span class="comment">#设定租约</span></span><br><span class="line"><span class="comment"># lease 2be7547fbc6a5afa granted with TTL(300s)</span></span><br><span class="line"></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> put sample value --lease=2be7547fbc6a5afa <span class="comment"># 绑定租约</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get sample <span class="comment"># 租约期限内可以获取值</span></span><br><span class="line"></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> lease keep-alive 2be7547fbc6a5afa <span class="comment"># 维持租约</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> lease revoke 2be7547fbc6a5afa  <span class="comment"># 撤销租约</span></span><br><span class="line"><span class="comment"># or after 300 seconds</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get sample <span class="comment"># 撤销租约或者300s后获取不到值</span></span><br><span class="line">$ etcdctl --write-out=table --endpoints=<span class="variable">$ENDPOINTS</span> endpoint status <span class="comment"># 集群状态</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> endpoint health</span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> snapshot save my.db <span class="comment"># 快照</span></span><br></pre></td></tr></table></figure>
<h3 id="rest-api-示例"><a href="#rest-api-示例" class="headerlink" title="rest api 示例"></a>rest api 示例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">curl http://127.0.0.1:2379/v2/keys/message <span class="comment"># 获取某个key的value</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"action"</span>: <span class="string">"get"</span>,</span><br><span class="line">    <span class="string">"node"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 2,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 2,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello world"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">curl http://127.0.0.1:2379/v2/keys/message -XPUT -d value=<span class="string">"Hello etcd"</span> <span class="comment"># 更改 </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"action"</span>: <span class="string">"set"</span>,</span><br><span class="line">    <span class="string">"node"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 3,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 3,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello etcd"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"prevNode"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 2,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello world"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 2</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">curl http://127.0.0.1:2379/v2/keys/message -XDELETE  <span class="comment">#删除 </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"action"</span>: <span class="string">"delete"</span>,</span><br><span class="line">    <span class="string">"node"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 3,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 4</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"prevNode"</span>: &#123;</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello etcd"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 3,</span><br><span class="line">        <span class="string">"createdIndex"</span>: 3</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="架构-amp-原理介绍"><a href="#架构-amp-原理介绍" class="headerlink" title="架构 &amp; 原理介绍"></a>架构 &amp; 原理介绍</h2><p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/etcd-arch-20170927110531.jpg" alt="etcd-arch"><br>etcd主要分为四个部分:</p>
<ul>
<li>HTTP Server： 用于处理用户发送的API请求以及其它etcd节点的同步与心跳信息请求。</li>
<li>Store：用于处理etcd支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是etcd对用户提供的大多数API功能的具体实现。</li>
<li>Raft：Raft强一致性算法的具体实现，是etcd的核心。</li>
<li>WAL：Write Ahead Log（预写式日志），是etcd的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd就通过WAL进行持久化存储。WAL中，所有的数据提交前都会事先记录日志。Snapshot是为了防止数据过多而进行的状态快照；Entry表示存储的具体日志内容。<br>通常，一个用户的请求发送过来，会经由HTTP Server转发给Store进行具体的事务处理，如果涉及到节点的修改，则交给Raft模块进行状态的变更、日志的记录，然后再同步给别的etcd节点以确认数据提交，最后进行数据的提交，再次同步。</li>
</ul>
<p>关于集群状态机的转变可以参考<a href="http://www.infoq.com/cn/articles/coreos-analyse-etcd" target="_blank" rel="noopener">CoreOS 实战：剖析 etcd</a></p>
<h2 id="服务发现工具对比"><a href="#服务发现工具对比" class="headerlink" title="服务发现工具对比"></a>服务发现工具对比</h2><p>引自<a href="http://dockone.io/article/667" target="_blank" rel="noopener">服务发现：Zookeeper vs etcd vs Consul</a>：</p>
<blockquote>
<blockquote>
<blockquote>
<p>所有这些工具都是基于相似的原则和架构，它们在节点上运行，需要仲裁来运行，并且都是强一致性的，都提供某种形式的键/值对存储。<br>Zookeeper是其中最老态龙钟的一个，使用年限显示出了其复杂性、资源利用和尽力达成的目标，它是为了与我们评估的其他工具所处的不同时代而设计的（即使它不是老得太多）。<br>etcd、Registrator和Confd是一个非常简单但非常强大的组合，可以解决大部分问题，如果不是全部满足服务发现需要的话。它还展示了我们可以通过组合非常简单和特定的工具来获得强大的服务发现能力，它们中的每一个都执行一个非常具体的任务，通过精心设计的API进行通讯，具备相对自治工作的能力，从架构和功能途径方面都是微服务方式。<br>Consul的不同之处在于无需第三方工具就可以原生支持多数据中心和健康检查，这并不意味着使用第三方工具不好。实际上，在这篇博客里我们通过选择那些表现更佳同时不会引入不必要的功能的的工具，尽力组合不同的工具。使用正确的工具可以获得最好的结果。如果工具引入了工作不需要的特性，那么工作效率反而会下降，另一方面，如果工具没有提供工作所需要的特性也是没有用的。Consul很好地权衡了权重，用尽量少的东西很好的达成了目标。<br>Consul使用gossip来传播集群信息的方式，使其比etcd更易于搭建，特别是对于大的数据中心。将存储数据作为服务的能力使其比etcd仅仅只有健/值对存储的特性更加完整、更有用（即使Consul也有该选项）。虽然我们可以在etcd中通过插入多个键来达成相同的目标，Consul的服务实现了一个更紧凑的结果，通常只需要一次查询就可以获得与服务相关的所有数据。除此之外，Registrator很好地实现了Consul的两个协议，使其合二为一，特别是添加Consul-template到了拼图中。Consul的Web UI更是锦上添花般地提供了服务和健康检查的可视化途径。</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://www.liuhaihua.cn/archives/404914.html" target="_blank" rel="noopener">etcd 使用入门</a></p>
<p><a href="https://coreos.com/etcd/docs/latest/v2/api.html" target="_blank" rel="noopener">etcd API</a></p>
<p><a href="https://poweruphosting.com/blog/etcd-tutorial/" target="_blank" rel="noopener">Etcd Tutorial- The Ultimate Reliable Key Value Storage for Networks</a></p>
<p><a href="https://luyiisme.github.io/2017/04/22/spring-cloud-service-discovery-products/" target="_blank" rel="noopener">服务发现比较:Consul vs Zookeeper vs Etcd vs Eureka</a></p>
<p><a href="http://www.infoq.com/cn/articles/coreos-analyse-etcd" target="_blank" rel="noopener">CoreOS 实战：剖析 etcd</a></p>
<p><a href="http://lihaoquan.me/2016/6/24/learning-etcd-1.html" target="_blank" rel="noopener">深入学习Etcd</a></p>
<p><a href="http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle" target="_blank" rel="noopener">etcd：从应用场景到实现原理的全方位解读</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/03/intro-to-harbor/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/03/intro-to-harbor/" itemprop="url">
                  Docker-- 关于Harbor 的一些记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-03T11:59:11+08:00">
                2017-08-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/03/intro-to-harbor/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/03/intro-to-harbor/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Harbor-简介"><a href="#Harbor-简介" class="headerlink" title="Harbor 简介"></a>Harbor 简介</h2><p><a href="https://github.com/vmware/harbor" target="_blank" rel="noopener">Harbor</a> 是 Vmware China研发开源的企业级私有容器Registry,基于docker官方的解决方案<a href="https://github.com/docker/distribution" target="_blank" rel="noopener">Distribution</a>。目前来说，已经成为企业级私有容器仓库的首选（主要是可供选择的本来就不多，可能是因为镜像管理不像容器编排那样是必争之地）。相对于Docker Distribution，Harbor添加了安全，认证，管理等功能，性能以及安全性都得到提升，主要 features如下：</p>
<ul>
<li>基于角色的访问控制：用户和镜像仓库通过project来组织管理，一个用户对同一project的不同镜像仓库会有不同处理权限。</li>
<li>基于策略的镜像复制：多个registry实例可以实现镜像同步复制，对于load-balancing,高可用，多数据中心，混合云的情景非常有用。</li>
<li>支持 LDAP/AD</li>
<li>镜像删除 &amp; 垃圾回收</li>
<li>镜像的认证</li>
<li>友好的UI</li>
<li>日志审计：所有的操作都是可追踪的</li>
<li>RESTful API</li>
<li>易部署 </li>
</ul>
<h2 id="Harbor-架构与原理"><a href="#Harbor-架构与原理" class="headerlink" title="Harbor 架构与原理"></a>Harbor 架构与原理</h2><h3 id="Harbor-整体架构"><a href="#Harbor-整体架构" class="headerlink" title="Harbor 整体架构"></a>Harbor 整体架构</h3><p>Harbor 是以容器的方式运行，以docker-compose的规范形式组织各个组件，并通过docker-compose工具进行启停。<br>Harbor共有五个组件，分别如下：</p>
<ul>
<li>Proxy:Harbor服务的所有请求都由该服务接受并转发，其实就是一个前置的反向代理Nginx，类似于微服务概念中的API-gateway.</li>
<li>Registry: 即docker 官方的Registry镜像生成的容器示例，真正负责存贮镜像的地方，处理docker pull/push。针对不同的用户对不同的镜像操作权限不同，registry强制每个请求必须含有一个token以验证权限，如果没有token，会返回一个token服务地址。</li>
<li>Core Services: 主要提供UI,webhook(设置在registry上以获取镜像状态)，token服务。</li>
<li>Database:数据库服务Mysql，存储用户，权限，审计日志，镜像信息等。</li>
<li>Log Collector: 日志收集，跑一个Rsylogd服务。</li>
</ul>
<p>架构图如下：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-03.harbor-arc.jpg" alt="harbor-arch"></p>
<p>这五个容器之间通过docker-link相连，即通过容器名字互相访问，暴露Proxy服务的端口给终端用户访问。</p>
<h3 id="Harbor-工作原理"><a href="#Harbor-工作原理" class="headerlink" title="Harbor 工作原理"></a>Harbor 工作原理</h3><p>以docker login 与 docker push为例讲解：</p>
<p>客户端 输入docker login 之后，流程如下图：<br><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-03-harbor-flow1.jpg" alt="docker-login-flow"></p>
<p>(a) 首先，这个登录请求会被Proxy容器接收到，根据预先设置的匹配规则，该请求会被转发给后端Registry容器。<br>(b) Registry接收到请求后，解析请求，因为配置了基于token的认证，所以会查找token，发现请求没有token 后，返回错误代码401以及token服务的地址URL。<br>(c) Docker客户端接收到错误请求后，转而向token服务地址发送请求，并根据HTTP协议的BasicAuthentication 规范，将用户名密码组合并编码，放在请求头部(header)。<br>(d) 同样，该请求会先发到Proxy容器，继而转发给ui/token的容器,该容器接受请求，将请求头解码，获取到用户名密码。<br>(e) ui/token的容器获取到用户名密码后，通过查询数据库进行比对验证(如果是LDAP 的认证方式,就是与LDAP服务进行校验)，比对成功后，返回成功的状态码，并用密钥生成token，一并发送给Docker客户端。</p>
<p>客户端 登陆成功后，输入docker push xxxxxx 之后，流程如下图（便于说明省略Docker client与Proxy之间通信）：</p>
<p><img src="https://raw.githubusercontent.com/zhangchenchen/zhangchenchen.github.io/hexo/images/2017-08-03-harbor-flow-2.jpg" alt="docker-push-flow"></p>
<p>(a) 同样，首先与Registery通信，返回一个token服务的地址URL.<br>(b) Docker客户端会与token服务通信，指明要申请一个push image操作的token。<br>(c) token服务访问数据库验证当前用户是否有该操作的权限，如果有，会将image信息以及push操作进行编码，用私钥签名，生成token返回给Docker客户端。<br>(d) Docker客户端再次与Registry通信，不过这次会将token放到请求header中，Registry收到请求后利用公钥解码并核对，核对成功，便可以开始push 操作了。</p>
<h2 id="Harbor-安装"><a href="#Harbor-安装" class="headerlink" title="Harbor 安装"></a>Harbor 安装</h2><p>比较简单：</p>
<ol>
<li>准备：python&gt;=2.7, docker&gt;=1.10, docker-compose&gt;=1.6.0</li>
<li>下载离线安装包</li>
<li>修改配置文件 harbor.cfg</li>
<li>执行脚本install.sh </li>
</ol>
<p>参考<a href="https://github.com/vmware/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">Installation and Configuration Guide</a></p>
<h2 id="Harbor-高可用"><a href="#Harbor-高可用" class="headerlink" title="Harbor 高可用"></a>Harbor 高可用</h2><p>对于Harbor高可用方案，目前并没有最佳实践，不过我看issues上有不少相关内容，可以参考<a href="https://github.com/vmware/harbor/issues/327" target="_blank" rel="noopener">Harbor HA feature design proposal/discussion</a>。<br>其实，私有云相对来说对镜像的请求并非高频，在做HA的时候还是结合实际情况，切勿为了HA而HA,还要综合考量成本，安全等因素。</p>
<p>这部分内容暂且留个坑，以后再写。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://segmentfault.com/a/1190000007705296" target="_blank" rel="noopener">VMware Harbor：基于 Docker Distribution 的企业级 Registry 服务</a></p>
<p><a href="http://tonybai.com/2017/06/09/setup-a-high-availability-private-registry-based-on-harbor-and-cephfs/" target="_blank" rel="noopener">基于Harbor和CephFS搭建高可用Private Registr</a></p>
<p><a href="https://github.com/vmware/harbor" target="_blank" rel="noopener">vmware/harbor</a></p>
<p><a href="http://jaminzhang.github.io/docker/Enterprise-class-private-Docker-Container-Registry-Harbor-deploying/" target="_blank" rel="noopener">Docker 企业级私有镜像仓库 Harbor 部署</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="pekingzcc" />
          <p class="site-author-name" itemprop="name">pekingzcc</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">99</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">46</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/zhangchenchen" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">pekingzcc</span>
</div>


<div class="powered-by">
  powered by <a class="theme-link" href="https://hexo.io">Hexo</a> 
</div>

<div class="theme-info">
  theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'pekingzcc';
      var disqus_identifier = 'page/3/index.html';

      var disqus_title = "";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      

    </script>
  









  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  

  


</body>
</html>
