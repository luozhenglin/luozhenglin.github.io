<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="Solar">
<meta property="og:url" content="https://zhangchenchen.github.io/page/3/index.html">
<meta property="og:site_name" content="Solar">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Solar">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://zhangchenchen.github.io/page/3/"/>





  <title> Solar </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-92407570-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Solar</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/" itemprop="url">
                  flask-- SQLAlchemy 与 flask-sqlalchemy记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-10-24T12:20:10+08:00">
                2017-10-24
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/24/SQLAlchemy-&&-flask-sqlalchemy/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="SQLAchemy"><a href="#SQLAchemy" class="headerlink" title="SQLAchemy"></a>SQLAchemy</h2><p>在flask开发中，涉及到数据库的交互，一般都会想到SQLAchemy。SQLAchemy是一个python的SQL工具集，它实现了ORM,类似于Java web开发中的hibernete。</p>
<h2 id="flask-sqlachemy"><a href="#flask-sqlachemy" class="headerlink" title="flask-sqlachemy"></a>flask-sqlachemy</h2><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://opennebula.org/opennebula-vs-openstack-user-needs-vs-vendor-driven/" target="_blank" rel="noopener">OpenNebula vs. OpenStack: User Needs vs. Vendor Driven </a></p>
<p><a href="https://opennebula.org/comparing-opennebula-and-openstack-two-different-views-on-the-cloud/" target="_blank" rel="noopener">Comparing OpenNebula and OpenStack: Two Different Views on the Cloud</a></p>
<p><a href="https://www.slideshare.net/opennebula/opennebula-414-handson-tutorial?ref=https://opennebula.org/documentation/tutorials/" target="_blank" rel="noopener">OpenNebula 4.14 Hands-on Tutorial</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/09/12/openNebula-vs-openstack/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/12/openNebula-vs-openstack/" itemprop="url">
                  Openstack-- OpenStack 与OpenNebula的前世今生
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-12T16:25:30+08:00">
                2017-09-12
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/09/12/openNebula-vs-openstack/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/09/12/openNebula-vs-openstack/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引出"><a href="#引出" class="headerlink" title="引出"></a>引出</h2><p>本篇博文说实话感觉滞后太多，因为关于云计算开源框架的争论在前几年还甚嚣尘上，自从openstack风卷残云之势席卷整个云计算圈之后，之前的一些老前辈貌似都默默退出了，这也包括本文的一个主角OpenNebula。其实OpenNebula一直都在，只不过OpenStack的名气太大，以至于其他框架都被无意间忽略了。不过随着OpenStack项目的飞速膨胀，复杂度与可维护性也随之疯涨，OpenNebula这个一直秉承“Simplicity,Flexibility”的框架也得到了不少人的拥趸，本文就主要讲讲这个本应在几年前讨论的话题：OpenStack 与OpenNebula，孰优孰劣。</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-09-12-opennebula-vs-openstack.jpg" alt="openstack vs opennebula"></p>
<h2 id="OpenStack-VS-OpenNebula"><a href="#OpenStack-VS-OpenNebula" class="headerlink" title="OpenStack VS OpenNebula"></a>OpenStack VS OpenNebula</h2><h3 id="定位的区别"><a href="#定位的区别" class="headerlink" title="定位的区别"></a>定位的区别</h3><p>Openstack从一开始就是打算跟AWS正面刚，所以一直借鉴（抄袭）AWS,可以认为是走公有云（提供基础设施）路线的，而OpenNebula则是走企业云（数据中心虚拟化）路线的，是打算跟vmware干仗的。但随着各个厂商的站队，OpenStack突然红了，但是红了也是有代价的，OpenStack的发展方向开始被Foundation控制，而这个Foundation是由各大厂商（以red hat为首）控制的，所以OpenStack其实是面向这些巨头们的，当然其中不乏各大公司的博弈，君不见早期launchpad上多个顽疾似的bug一直无人问津，而各大厂商的适配driver却早已开发完善。OpenNebula一直有一个核心开发组织掌握着开发的整体方向，相对于OpenStack,”铜臭味”没那么重，但也因为没有巨头的摇旗呐喊，所以开发进度不是很快。<br>所以，按照OpenNebula开发者的话说：Openstack是服务于foundation的，而OpenNebula才是真正服务于用户的。</p>
<h3 id="整体架构的区别"><a href="#整体架构的区别" class="headerlink" title="整体架构的区别"></a>整体架构的区别</h3><p>这个区别对于技术人员来说就更感兴趣了。因为刚开始的定位不同，自然架构也是天壤之别，先上两张图直观感受下：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/20170912openstack-arch.jpg" alt="openstack-arch"></p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/20170912-opennebula-arch.jpg" alt="opennebula"></p>
<p>一张图就可以大致了解到，如今的OpenStack俨然是一个庞然大物，不过好在各个子项目分工明确，划分成层之后如下图（图比较老了，有些项目已经毕业了）：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/20170913-openstack-layer.jpg" alt="openstack-arch"></p>
<p>再看下具体部署情况：</p>
<p>openstack的典型部署架构：<br><img src="http://7xrnwq.com1.z0.glb.clouddn.com/20170913085313-openstack-deploy.jpg" alt="openstack-deploy"></p>
<p>opennebula的典型部署架构：<br><img src="http://7xrnwq.com1.z0.glb.clouddn.com/20170913085402-open-nebula-deploy.jpg" alt="opennebula-deploy"></p>
<p>可以看出，在计算节点openstack要部署很多agent(包括nova-compute)，而opennebula只要保证可以SSH连接以及有hypervisor就可以，是一种无侵入式的设计。</p>
<h3 id="支持服务的区别"><a href="#支持服务的区别" class="headerlink" title="支持服务的区别"></a>支持服务的区别</h3><p>网上一张截图（时效性不保证）：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/20170913090037-feature.jpg" alt="opennebula-vs-openstack"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Openstack因为各大厂商力推的原因，走的是大而全的路线，你能想到的云计算服务，openstack基本都有对应的子项目，但也造成了架构复杂，学习路线陡峭，后期维护成本高等现象。OpenNebula走的是小而美的路线，无侵入式设计，架构简单，容易上手，但是因为保证无侵入式，所以要大量调用shell，动用的编程语言包括C++,ruby，shell等，项目代码相对混乱，代码可读性不高。<br>综上所述，如果是建一个求稳定性，且物理环境一致，规模相对较大的私有云，Openstack是首选，而如果计算节点系统不一致，特别是计算节点有公有云的情况(也就是搭建混合云)，可以考虑OpenNebula。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://opennebula.org/opennebula-vs-openstack-user-needs-vs-vendor-driven/" target="_blank" rel="noopener">OpenNebula vs. OpenStack: User Needs vs. Vendor Driven </a></p>
<p><a href="https://opennebula.org/comparing-opennebula-and-openstack-two-different-views-on-the-cloud/" target="_blank" rel="noopener">Comparing OpenNebula and OpenStack: Two Different Views on the Cloud</a></p>
<p><a href="https://www.slideshare.net/opennebula/opennebula-414-handson-tutorial?ref=https://opennebula.org/documentation/tutorials/" target="_blank" rel="noopener">OpenNebula 4.14 Hands-on Tutorial</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/22/install-k8s.1.6.3-ha-version/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/22/install-k8s.1.6.3-ha-version/" itemprop="url">
                  Kubernetes-- 手动安装高可用k8s1.6问题汇总
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-22T14:25:20+08:00">
                2017-08-22
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/22/install-k8s.1.6.3-ha-version/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/22/install-k8s.1.6.3-ha-version/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="kubernetes-HA-整体架构"><a href="#kubernetes-HA-整体架构" class="headerlink" title="kubernetes HA 整体架构"></a>kubernetes HA 整体架构</h2><p>k8s的HA相对于<a href="https://zhangchenchen.github.io/2017/04/14/openstack-ha/">openstack的HA</a>要简单很多，主要包括以下三各方面：</p>
<ul>
<li>etcd的HA:创建HA集群,如果还不放心，可以使用分布式存储系统</li>
<li>apiserver(无状态服务)的HA:双活模式，前面加一个load balance</li>
<li>controller manager 和 scheduler的HA:主备模式。</li>
</ul>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/20170822-k8s-ha.jpg" alt="k8s-ha"></p>
<p>博主是按照<a href="http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html#42-kubelet部署" target="_blank" rel="noopener">Kubernetes 1.6 高可用集群部署</a>这篇博客一步步安装下来的，基于Kubernetes二进制包手动部署一个高可用的Kubernetes 1.6集群，将启用ApiServer的TLS双向认证和RBAC授权等安全机制，当然，也可以利用kubeadm创建一个k8s cluster后再扩展成高可用的，参考<a href="http://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/" target="_blank" rel="noopener">一步步打造基于Kubeadm的高可用Kubernetes集群</a>。<br>安装过程不再赘述，参考上述博文，主要讲下期间遇到的问题以及解决方法。</p>
<h2 id="etcd高可用集群部署"><a href="#etcd高可用集群部署" class="headerlink" title="etcd高可用集群部署"></a>etcd高可用集群部署</h2><p>在该过程中碰到的一个坑是，在创建tls密钥和证书的过程中。用的工具是cfssl,创建etcd证书签名请求配置文件的时候需要指定node节点的IP，当时希望可以外网访问，就使用的floating-ip,在etcd的systemd unit文件中自然也就使用了floating-ip，然而etcd却一直起不来，查看日志报错： listen tcp 172.16.21.55:2380: bind: cannot assign requested address.</p>
<p>监听端口失败，恍然大悟，openstack中的floating-ip是不会在虚拟机上创建一个新网卡的，而是通过l3-agent的转发实现。将floating-ip更改为内网IP 后问题解决。</p>
<h2 id="Kubernetes各组件TLS证书和密钥"><a href="#Kubernetes各组件TLS证书和密钥" class="headerlink" title="Kubernetes各组件TLS证书和密钥"></a>Kubernetes各组件TLS证书和密钥</h2><p>除了etcd是使用tls双向认证外，这里apiserver也启用了双向认证，这样的话，凡是与apiserver通信的组件都要创建对应的证书与密钥,所有的需要创建的组件包括：</p>
<ul>
<li>kube-apiserver</li>
<li>kubernetes-admin：RBAC相关，该证书拥有访问kube-apiserver的所有权限。</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>kubelet:每个节点都要配置</li>
<li>kube-proxy：每个节点都要配置</li>
</ul>
<h2 id="Kubernetes-Master集群部署"><a href="#Kubernetes-Master集群部署" class="headerlink" title="Kubernetes Master集群部署"></a>Kubernetes Master集群部署</h2><p>我们这里用openstack创建了一个load balance用于apiserver的高可用，如果是在裸机上，也有多种方案，比如，可以使用haproxy+keepalived的方案实现，也可以直接在各节点用nginx做反向代理，参考<a href="https://www.centos.bz/2017/08/k8s-kubernetes-1-7-3-calico-master-ha/#配置 KubeDNS" target="_blank" rel="noopener">kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明</a>。</p>
<h2 id="Kubernetes-Node节点部署"><a href="#Kubernetes-Node节点部署" class="headerlink" title="Kubernetes Node节点部署"></a>Kubernetes Node节点部署</h2><p>在部署Pod Network插件flannel碰到一个问题，执行完create命令后，pod一直处于containercreating状态，describe发现报错信息为：error syncing pod，没有多余的报错信息，而且容器压根就没有创建起来。挣扎了许久，最后直接在/var/log/messaging中暴力搜索了一下syncing pod，终于找到有用的信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.....</span><br><span class="line">Aug 22 13:12:33 host-10-10-10-52 dockerd: time=<span class="string">"2017-08-22T13:12:33.841538881+08:00"</span> level=error msg=<span class="string">"Handler for GET /v1.24/images/gcr.io/google_containers/pause-amd64:3.0/json returned error: No such image: gcr.io/google_containers/pause-amd64:3.0"</span></span><br></pre></td></tr></table></figure></p>
<p>通过翻墙将该镜像pull到本地后，一切解决。关于pause container的作用，直接引用<a href="https://groups.google.com/forum/#!topic/kubernetes-users/jVjv0QK4b_o" target="_blank" rel="noopener">What is the role of ‘pause’ container?</a></p>
<blockquote>
<blockquote>
<blockquote>
<p>The pause container is a container which holds the network namespace for the pod. It does nothing ‘useful’. (It’s actually just a little bit of assembly that goes to sleep and never wakes up)<br>This means that your ‘apache’ container can die, and come back to life, and all of the network setup will still be there. Normally if the last process in a network namespace dies the namespace would be destroyed and creating a new apache container would require creating all new network setup. With pause, you’ll always have that one last thing in the namespace.</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kubernetes.io/docs/admin/high-availability/" target="_blank" rel="noopener">Building High-Availability Clusters</a></p>
<p><a href="http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html#42-kubelet部署" target="_blank" rel="noopener">Kubernetes 1.6 高可用集群部署</a></p>
<p><a href="http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html" target="_blank" rel="noopener">ssl 双向认证和单向认证原理</a></p>
<p><a href="https://www.centos.bz/2017/08/k8s-kubernetes-1-7-3-calico-master-ha/#配置 KubeDNS" target="_blank" rel="noopener">kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/17/kubernetes-authentication-authorization-admission-control/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/17/kubernetes-authentication-authorization-admission-control/" itemprop="url">
                  Kubernetes-- 漫谈kubernetes 中的认证 & 授权 & 准入机制
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-17T15:37:30+08:00">
                2017-08-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/17/kubernetes-authentication-authorization-admission-control/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/17/kubernetes-authentication-authorization-admission-control/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>首先需要了解这三种机制的区别：简单来说，认证(Authenticating)是对客户端的认证，通俗点就是用户名密码验证，授权(Authorization)是对资源的授权，k8s中的资源无非是容器，最终其实就是容器的计算，网络，存储资源，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。准入(Admission Control)机制是一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。<br>在k8s中，这三种机制如下图：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-08-17-k8s-authorition.png" alt="k8s-authorization"></p>
<p>k8s的整体架构也是一个微服务的架构，所有的请求都是通过一个GateWay，也就是kube-apiserver这个组件（对外提供REST服务），由图中可以看出，k8s中客户端有两类，一种是普通用户，一种是集群内的Pod，这两种客户端的认证机制略有不同，后文会详述。但无论是哪一种，都需要依次经过认证，授权，准入这三个机制。</p>
<h2 id="kubernetes-中的认证机制"><a href="#kubernetes-中的认证机制" class="headerlink" title="kubernetes 中的认证机制"></a>kubernetes 中的认证机制</h2><p>需要注意的是，kubernetes虽然提供了多种认证机制，但并没有提供user 实体信息的存储，也就是说，账户体系需要我们自己去做维护。当然，也可以接入第三方账户体系（如谷歌账户），也可以使用开源的keystone去做整合。kubernetes 支持多种认证机制，可以配置成多个认证体制共存，这样，只要有一个认证通过，这个request就认证通过了。下面介绍下官网列举的几种常见认证机制：</p>
<h3 id="X509-Client-Certs"><a href="#X509-Client-Certs" class="headerlink" title="X509 Client Certs"></a>X509 Client Certs</h3><p>也叫作双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。默认在kubeadm创建的集群中是enabled的，可以在master node上查看kube-apiserver的pod配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/manifests/kube-apiserver.json</span></span><br><span class="line">.................</span><br><span class="line">containers<span class="string">": [</span></span><br><span class="line"><span class="string">      &#123;</span></span><br><span class="line"><span class="string">        "</span>name<span class="string">": "</span>kube-apiserver<span class="string">",</span></span><br><span class="line"><span class="string">        "</span>image<span class="string">": "</span>gcr.io/google_containers/kube-apiserver-amd64:v1.5.2<span class="string">",</span></span><br><span class="line"><span class="string">        "</span><span class="built_in">command</span><span class="string">": [</span></span><br><span class="line"><span class="string">          "</span>kube-apiserver<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--insecure-bind-address=127.0.0.1<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--service-cluster-ip-range=10.96.0.0/12<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--service-account-key-file=/etc/kubernetes/pki/apiserver-key.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--client-ca-file=/etc/kubernetes/pki/ca.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--tls-cert-file=/etc/kubernetes/pki/apiserver.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--token-auth-file=/etc/kubernetes/pki/tokens.csv<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--secure-port=6443<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--allow-privileged<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--advertise-address=192.168.61.100<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname<span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--anonymous-auth=<span class="literal">false</span><span class="string">",</span></span><br><span class="line"><span class="string">          "</span>--etcd-servers=http://127.0.0.1:2379<span class="string">"</span></span><br><span class="line"><span class="string">        ],</span></span><br></pre></td></tr></table></figure>
<p>相关的三个启动参数：</p>
<ul>
<li>client-ca-file: 指定CA根证书文件为/etc/kubernetes/pki/ca.pem，内置CA公钥用于验证某证书是否是CA签发的证书</li>
<li>tls-private-key-file: 指定ApiServer私钥文件为/etc/kubernetes/pki/apiserver-key.pem</li>
<li>tls-cert-file：指定ApiServer证书文件为/etc/kubernetes/pki/apiserver.pem</li>
</ul>
<p>只要有这三个启动参数，就说明开启了https的认证方式，这时，如果在集群外访问 <a href="https://masterIP:6443/api" target="_blank" rel="noopener">https://masterIP:6443/api</a> 会提示Unauthorized，只有在客户端配置相关认证才可以访问,客户端的认证证书生成与操作可以参考<a href="https://kubernetes.io/docs/admin/authentication/#x509-client-certs" target="_blank" rel="noopener">Creating Certificates</a>。证书的生成是kubeadm使用openssl自动生成的，如果是手动配置双向认证，相对比较麻烦，主要配置流程如下：</p>
<ul>
<li>生成根证书、API Server服务端证书、服务端私钥、各个组件所用的客户端证书和客户端私钥。</li>
<li>修改 Kubernetes 各个服务进程的启动参数，启用双向认证模式.</li>
</ul>
<p>详细配置可以参考<a href="http://www.cnblogs.com/breg/p/5923604.html" target="_blank" rel="noopener">Kubernetes集群安全配置案例</a></p>
<p>注意，在启动参数中还有一个参数：–insecure-bind-address=127.0.0.1，这个参数主要用与master node上的其他核心组件，比如kube-scheduler，kube-controller-manager通过masterIP:8080与APIserver直接通信，而不用通过双向认证。这一点可以从他们的启动参数–master=127.0.0.1:8080看出。</p>
<h3 id="Static-Token-File"><a href="#Static-Token-File" class="headerlink" title="Static Token File"></a>Static Token File</h3><p>静态token文件认证，同样，在kubeadm创建的集群中也是默认enabled的，比如，上面的apiserver启动参数中，我们可以看到有参数 ：–token-auth-file=/etc/kubernetes/pki/tokens.csv ，这个静态token文件的格式为 token,user,uid,”group1,group2,group3”，如下示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/pki/tokens.csv</span></span><br><span class="line">7db2f1c02d721320,kubeadm-node-csr,0615e0ac-7d70-11e7-ad94-fa163eb9dfdd,system:kubelet-bootstrap</span><br></pre></td></tr></table></figure>
<p>客户端请求的时候需要在http header中加入：”Authorization: Bearer THETOKEN”，如下实例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -k --header <span class="string">"Authorization: Bearer 7db2f1c02d721320"</span> https://192.168.21.34:6443/api</span><br></pre></td></tr></table></figure>
<p>或者使用brctl:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl --server=https://192.168.21.34:6443 \</span><br><span class="line">--token=7db2f1c02d721320 \</span><br><span class="line">--insecure-skip-tls-verify=<span class="literal">true</span> \</span><br><span class="line">cluster-info</span><br></pre></td></tr></table></figure>
<p>注意，如果该静态token文件更改的话，需要重启apiserver。</p>
<h3 id="Bootstrap-Tokens"><a href="#Bootstrap-Tokens" class="headerlink" title="Bootstrap Tokens"></a>Bootstrap Tokens</h3><p>bootstrap token认证目前处于alpha阶段，目前主要是kubeadm创建k8s集群时使用。使用这种认证方式，k8s会动态的管理一种type为bootstrap token的token，这些token作为secret放在kube-system namespace中。controller-manager中的tokencleaner controller会在bootstrap token 过期时进行删除。<br>使用这种认证方式，apiserver的启动参数中需要有–experimental-bootstrap-token-auth，Controller Manager的启动参数中有–controllers=*,tokencleaner 类似参数。</p>
<h3 id="Static-Password-File"><a href="#Static-Password-File" class="headerlink" title="Static Password File"></a>Static Password File</h3><p>比较简单，kubeadm默认没有开启，生产环境也不建议使用。<br>apiserver启动参数指定–basic_auth_file=/etc/kubernetes/basic_auth。然后在指定的文件中加入用户名密码等就可以了，文件格式为password,user,uid,”group1,group2,group3”。</p>
<h3 id="Service-Account-Tokens"><a href="#Service-Account-Tokens" class="headerlink" title="Service Account Tokens"></a>Service Account Tokens</h3><p>Service Account Token 是一种比较特殊的认证机制，适用于上文中提到的pod内部服务需要访问apiserver的认证情况，默认enabled。<br>还是看上文中apiserver 的启动配置参数有–service-account-key-file=/etc/kubernetes/pki/apiserver-key.pem，如果没有指明文件，默认使用–tls-private-key-file的值，即API Server的私钥。<br>service accout本身是作为一种资源在k8s集群中，我们可以通过命令行获取：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master pki]<span class="comment"># kubectl get serviceaccount --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME        SECRETS   AGE</span><br><span class="line">default       default     1         7d</span><br><span class="line"><span class="keyword">for</span>-test      default     1         3d</span><br><span class="line">kube-system   default     1         7d</span><br><span class="line">kube-system   weave-net   1         7d</span><br><span class="line">sock-shop     default     1         7d</span><br></pre></td></tr></table></figure>
<p>可以看到k8s集群为所有的namespace创建了一个默认的service account，利用命令describe会发现service account只是关联了一个secret作为token，也就是service-account-token。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master pki]<span class="comment"># kubectl describe serviceaccount/default -n kube-system</span></span><br><span class="line">Name:           default</span><br><span class="line">Namespace:      kube-system</span><br><span class="line">Labels:         &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Image pull secrets:     &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Mountable secrets:      default-token-nbldr</span><br><span class="line"></span><br><span class="line">Tokens:                 default-token-nbldr</span><br><span class="line"></span><br><span class="line">[root@k8s-master pki]<span class="comment">#  kubectl get secret default-token-nbldr -o yaml -n kube-system</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFM0</span><br><span class="line">  ........................略....................</span><br><span class="line">  namespace: a3ViZS1zeXN0ZW0=</span><br><span class="line">  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0..................................</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/service-account.name: default</span><br><span class="line">    kubernetes.io/service-account.uid: 67aae699-7d70-11e7-a8a9-fa163eb9dfdd</span><br><span class="line">  creationTimestamp: 2017-08-10T02:05:38Z</span><br><span class="line">  name: default-token-nbldr</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: <span class="string">"88"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/secrets/default-token-nbldr</span><br><span class="line">  uid: 67b20b73-7d70-11e7-a8a9-fa163eb9dfdd</span><br><span class="line"><span class="built_in">type</span>: kubernetes.io/service-account-token</span><br></pre></td></tr></table></figure>
<p>可以看到service-account-token的secret资源包含的数据有三部分：</p>
<ul>
<li><p>ca.crt，这是API Server的CA公钥证书，用于Pod中的Process对API Server的服务端数字证书进行校验时使用的；</p>
</li>
<li><p>namespace，这是Secret所在namespace的值的base64编码：# echo -n “kube-system”|base64 =&gt; “a3ViZS1zeXN0ZW0=”</p>
</li>
<li><p>token：该token就是由service-account-key-file的值签署(sign)生成。</p>
</li>
</ul>
<p>这种认证方式主要由k8s集群自己管理，用户用到的情况比较少。我们创建一个pod时，默认就会将该namespace对应的默认service account token mount到Pod中，所以无需我们操作便可以直接与apiserver通信，相关示例参考<a href="http://tonybai.com/2017/03/03/access-api-server-from-a-pod-through-serviceaccount/" target="_blank" rel="noopener">在Kubernetes Pod中使用Service Account访问API Server</a>，当然也可以指定多个service account token,参考<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/" target="_blank" rel="noopener">Configure Service Accounts for Pods</a>。</p>
<h3 id="OpenID-Connect-Tokens"><a href="#OpenID-Connect-Tokens" class="headerlink" title="OpenID Connect Tokens"></a>OpenID Connect Tokens</h3><p>类似 OAuth2的认证方式，大致认证过程如下：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-08-17-k8s-openid-token.jpg" alt="openID"></p>
<p>除了以上几种认证方式外，还有几种比如Webhook Token Authentication，Keystone Password等，详情见<a href="https://kubernetes.io/docs/admin/authentication/#x509-client-certs" target="_blank" rel="noopener">官网</a>。</p>
<h2 id="kubernetes-中的授权机制"><a href="#kubernetes-中的授权机制" class="headerlink" title="kubernetes 中的授权机制"></a>kubernetes 中的授权机制</h2><p>k8s中的授权策略也支持开启多个授权插件，只要一个验证通过即可。k8s授权处理主要是根据以下请求属性：</p>
<ul>
<li>user, group, extra</li>
<li>API、请求方法（如get、post、update、patch和delete）和请求路径（如/api）</li>
<li>请求资源和子资源</li>
<li>Namespace</li>
<li>API Group</li>
</ul>
<p>目前k8s支持的授权模式主要有以下几种：</p>
<ul>
<li>Node Authorization</li>
<li>ABAC Authorization</li>
<li>RBAC Authorization</li>
<li>Webhook Authorization</li>
</ul>
<h3 id="Node-Authorization"><a href="#Node-Authorization" class="headerlink" title="Node Authorization"></a>Node Authorization</h3><p>1.7+版本才release的一种授权机制，通过配合NodeRestriction control准入控制插件来限制kubelet访问node，endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源。配置方式为：<br>–authorization-mode=Node,RBAC –admission-control=…,NodeRestriction,…</p>
<h3 id="ABAC-Authorization"><a href="#ABAC-Authorization" class="headerlink" title="ABAC Authorization"></a>ABAC Authorization</h3><p>ABAC(Attribute-based access control),使用这种模式需要配置参数：<br>–authorization-mode=ABAC  –authorization-policy-file=SOME_FILENAME。<br>这种模式的实现相对比较生硬，就是在master node保存一份policy文件，指定不用用户（或用户组）对不同资源的访问权限,当修改该文件后，需要重启apiserver,跟openstack 的ABAC类似。policy文件的格式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Alice can do anything to all resources:</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"apiVersion"</span>: <span class="string">"abac.authorization.kubernetes.io/v1beta1"</span>,</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"spec"</span>: &#123;</span><br><span class="line">        <span class="string">"user"</span>: <span class="string">"alice"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"resource"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"apiGroup"</span>: <span class="string">"*"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># Kubelet can read any pods:</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"apiVersion"</span>: <span class="string">"abac.authorization.kubernetes.io/v1beta1"</span>,</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"spec"</span>: &#123;</span><br><span class="line">        <span class="string">"user"</span>: <span class="string">"kubelet"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"resource"</span>: <span class="string">"pods"</span>,</span><br><span class="line">        <span class="string">"readonly"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Kubelet can read and write events:</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"apiVersion"</span>: <span class="string">"abac.authorization.kubernetes.io/v1beta1"</span>,</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"Policy"</span>,</span><br><span class="line">    <span class="string">"spec"</span>: &#123;</span><br><span class="line">        <span class="string">"user"</span>: <span class="string">"kubelet"</span>,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"resource"</span>: <span class="string">"events"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用这种模式需要配置参数：<br>–authorization-mode=ABAC  –authorization-policy-file=SOME_FILENAME</p>
<h3 id="RBAC-Authorization"><a href="#RBAC-Authorization" class="headerlink" title="RBAC Authorization"></a>RBAC Authorization</h3><p>RBAC（Role-Based Access Control）依然处于Beta阶段，通过启动参数–authorization-mode=RBAC，使用kubeadm安装k8s默认会enabled。<br>RBAC API定义了四个资源对象用于描述RBAC中用户和资源之间的连接权限：</p>
<ul>
<li>Role</li>
<li>ClusterRole</li>
<li>RoleBinding</li>
<li>ClusterRoleBinding</li>
</ul>
<p>Role是定义在某个Namespace下的资源，在这个具体的Namespace下使用。 ClusterRole与Role相似，只是ClusterRole是整个集群范围内使用的。<br>RoleBinding把Role绑定到账户主体Subject，让Subject继承Role所在namespace下的权限。 ClusterRoleBinding把ClusterRole绑定到Subject，让Subject集成ClusterRole在整个集群中的权限。</p>
<p>我们可以通过kubectl命令获取对应的Role相关资源进行增删改查：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl get roles --all-namespaces</span><br><span class="line"></span><br><span class="line">kubectl get ClusterRoles</span><br><span class="line"></span><br><span class="line">kubectl get rolebinding --all-namespaces</span><br><span class="line"></span><br><span class="line">kubectl get clusterrolebinding</span><br></pre></td></tr></table></figure></p>
<p>API Server已经创建一系列ClusterRole和ClusterRoleBinding。这些资源对象中名称以system:开头的，表示这个资源对象属于Kubernetes系统基础设施。 也就说RBAC默认的集群角色已经完成足够的覆盖，让集群可以完全在 RBAC的管理下运行。 修改这些资源对象可能会引起未知的后果，例如对于system:node这个ClusterRole定义了kubelet进程的权限，如果这个角色被修改，可能导致kubelet无法工作。</p>
<h3 id="Webhook-Authorization"><a href="#Webhook-Authorization" class="headerlink" title="Webhook Authorization"></a>Webhook Authorization</h3><p>用户在外部提供 HTTPS 授权服务，然后配置 apiserver 调用该服务去进行授权。apiserver配置参数：<br>–authorization-webhook-config-file=SOME_FILENAME<br>配置文件的格式跟kubeconfig的格式类似，具体参考<a href="https://kubernetes.io/docs/admin/authorization/webhook/" target="_blank" rel="noopener">官方文档</a></p>
<h2 id="kubernetes-中的准入机制"><a href="#kubernetes-中的准入机制" class="headerlink" title="kubernetes 中的准入机制"></a>kubernetes 中的准入机制</h2><p>Kubernetes的Admission Control实际上是一个准入控制器(Admission Controller)插件列表，发送到APIServer的请求都需要经过这个列表中的每个准入控制器插件的检查，如果某一个控制器插件准入失败，就准入失败。<br>控制器插件如下：</p>
<ul>
<li>AlwaysAdmit：允许所有请求通过</li>
<li>AlwaysPullImages：在启动容器之前总是去下载镜像，相当于每当容器启动前做一次用于是否有权使用该容器镜像的检查</li>
<li>AlwaysDeny：禁止所有请求通过，用于测试</li>
<li>DenyEscalatingExec：拒绝exec和attach命令到有升级特权的Pod的终端用户访问。如果集中包含升级特权的容器，而要限制终端用户在这些容器中执行命令的能力，推荐使用此插件</li>
<li>ImagePolicyWebhook</li>
<li>ServiceAccount：这个插件实现了serviceAccounts等等自动化，如果使用ServiceAccount对象，强烈推荐使用这个插件</li>
<li>SecurityContextDeny：将Pod定义中定义了的SecurityContext选项全部失效。SecurityContext包含在容器中定义了操作系统级别的安全选型如fsGroup，selinux等选项</li>
<li>ResourceQuota：用于namespace上的配额管理，它会观察进入的请求，确保在namespace上的配额不超标。推荐将这个插件放到准入控制器列表的最后一个。ResourceQuota准入控制器既可以限制某个namespace中创建资源的数量，又可以限制某个namespace中被Pod请求的资源总量。ResourceQuota准入控制器和ResourceQuota资源对象一起可以实现资源配额管理。</li>
<li>LimitRanger：用于Pod和容器上的配额管理，它会观察进入的请求，确保Pod和容器上的配额不会超标。准入控制器LimitRanger和资源对象LimitRange一起实现资源限制管理</li>
<li>NamespaceLifecycle：当一个请求是在一个不存在的namespace下创建资源对象时，该请求会被拒绝。当删除一个namespace时，将会删除该namespace下的所有资源对象</li>
<li>DefaultStorageClass</li>
<li>DefaultTolerationSeconds</li>
<li>PodSecurityPolicy</li>
</ul>
<p>当Kubernetes版本&gt;=1.6.0，官方建议使用这些插件：<br>–admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds<br>当Kubernetes版本&gt;=1.4.0，官方建议使用这些插件：<br>–admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota<br>以上是标准的准入插件，如果是自己定制的话，k8s1.7版 出了两个alpha features, Initializers 和 External Admission Webhooks，详情可以参考<a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/" target="_blank" rel="noopener">Dynamic Admission Control</a>.</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kubernetes.io/docs/admin/authentication/#x509-client-certs" target="_blank" rel="noopener">Authenticating</a></p>
<p><a href="https://kubernetes.io/docs/admin/authorization/" target="_blank" rel="noopener">authorization</a></p>
<p><a href="http://blog.frognew.com/2017/01/kubernetes-api-server-authc.html" target="_blank" rel="noopener">Kubernetes集群安全：Api Server认证</a></p>
<p><a href="http://blog.frognew.com/2017/05/kubernetes-apiserver-admission-control.html" target="_blank" rel="noopener">Kubernetes集群安全：准入控制Admission Control</a></p>
<p><a href="http://blog.frognew.com/2017/04/kubernetes-1.6-rbac.html" target="_blank" rel="noopener">Kubernetes 1.6新特性学习：RBAC授权</a></p>
<p><a href="http://tonybai.com/2017/03/03/access-api-server-from-a-pod-through-serviceaccount/" target="_blank" rel="noopener">在Kubernetes Pod中使用Service Account访问API Server</a></p>
<p><a href="http://blog.csdn.net/yan234280533/article/details/76359199" target="_blank" rel="noopener"> kubernetes安全控制认证与授权(二)</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/14/install-kubernete-by-kubreadm/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/14/install-kubernete-by-kubreadm/" itemprop="url">
                  Kubernete-- 利用kubeadm 搭建一个kubernate集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-14T15:38:10+08:00">
                2017-08-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/14/install-kubernete-by-kubreadm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/14/install-kubernete-by-kubreadm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>利用 kubeadm 搭建一个四节点的k8s测试集群</li>
<li>利用harbor搭建一个单节点的私有镜像仓库</li>
<li>k8s集群与私有镜像仓库整合</li>
<li>部署dashboard</li>
</ul>
<h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>准备以下5个节点，一个为k8s的master节点，3个为node节点，最后一个作为私有仓库镜像，系统为centos7.2：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-08-14-node.png" alt="five-nodes"></p>
<p>注：k8s的安装方式有很多，kubeadm安装方式是独立节点安装的官方推荐方式，简单可重复，但不适用于生产环境，因为没有做HA，不过可以在安装完之后继续优化做HA，参考<a href="http://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/" target="_blank" rel="noopener">一步步打造基于Kubeadm的高可用Kubernetes集群</a>,后续会跟进这一块。</p>
<h2 id="kubernete-集群安装"><a href="#kubernete-集群安装" class="headerlink" title="kubernete 集群安装"></a>kubernete 集群安装</h2><h3 id="k8s所有节点需要执行的操作"><a href="#k8s所有节点需要执行的操作" class="headerlink" title="k8s所有节点需要执行的操作"></a>k8s所有节点需要执行的操作</h3><p>所有节点都要安装以下组件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker：容器运行时，被Kubernetes依赖</span><br><span class="line">kubelet：Kubernetes核心组件，运行在集群中的所有节点上，用来启动容器和pods</span><br><span class="line">kubectl：命令行工具，k8s客户端，用来控制集群，只需要安装到kube-master上,当然，也可以安装到其他节点，然后配置指定master。</span><br><span class="line">kubeadm：集群安装工具</span><br></pre></td></tr></table></figure>
<p>首先，安装docker,k8s官方建议版本为1.12，1.13以及17.03+版本还没有测试。所以这里也安装1.12版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tee /etc/yum.repos.d/docker.repo &lt;&lt;-<span class="string">'EOF'</span></span><br><span class="line">[dockerrepo]</span><br><span class="line">name=Docker Repository</span><br><span class="line">baseurl=https://yum.dockerproject.org/repo/main/centos/7/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://yum.dockerproject.org/gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">yum update -y </span><br><span class="line"></span><br><span class="line">yum install -y docker-engine-1.12.6 docker-engine-selinux-1.12.6</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> docker &amp;&amp; systemctl start docker</span><br></pre></td></tr></table></figure>
<p>注：这里有个小坑，就是k8s dashboard在某些版本RH内核下会启动失败，参考<a href="https://github.com/rancher/rancher/issues/7436" target="_blank" rel="noopener">issue</a>。</p>
<p>接下来，安装kubectl, kubelet, kubeadm以及一些依赖包。</p>
<p>先把依赖包装上：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ebtables socat</span><br></pre></td></tr></table></figure></p>
<p>kubectl 的安装比较简单，参考<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">Install and Set Up kubectl</a>,可以直接下载可执行文件然后添加权限，扔到master节点的/usr/local/bin/目录下即可，注意版本要与k8s版本匹配(注：也可以直接在下文同其他三个组件一起rpm包安装)。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl</span><br><span class="line"></span><br><span class="line">chmod +x ./kubectl</span><br><span class="line"></span><br><span class="line">sudo mv ./kubectl /usr/<span class="built_in">local</span>/bin/kubectl</span><br></pre></td></tr></table></figure>
<p>因为kubelet, kubeadm的rpm安装包在gce上，需要翻墙。</p>
<p>如果服务器可以翻墙，可以直接通过yum命令安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</span><br><span class="line">        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y kubelet kubeadm</span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure></p>
<p>如果不能翻墙，只能先下载下来，然后安装，需要安装的rpm包url地址可以在这个网页中找到：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/primary.xml</span><br></pre></td></tr></table></figure></p>
<p>我们这里只需要安装三个rpm包，kubeadm, kubelet以及kubernetes-cni，可以直接搜索上面的网页然后找到合适版本的rpm包。我们这里安装最新版本1.7.3,对应的地址如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">https://packages.cloud.google.com/yum/pool/f7ec56b0f36a81c0f91bcf26e05f23088082b468b77dac576dc505444dd8cd48-kubeadm-1.7.3-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">https://packages.cloud.google.com/yum/pool/28b76e6e1c2ec397a9b6111045316a0943da73dd5602ee8e53752cdca62409e6-kubelet-1.7.3-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">https://packages.cloud.google.com/yum/pool/e7a4403227dd24036f3b0615663a371c4e07a95be5fee53505e647fd8ae58aa6-kubernetes-cni-0.5.1-0.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>将这三个rpm包打包上传到四个节点上，并安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tar xzvf /tmp/kubernetes-el7-x86_64.tar.gz</span><br><span class="line">kubernetes-el7-x86_64/</span><br><span class="line">kubernetes-el7-x86_64/567600102f687e0f27bd1fd3d8211ec1cb12e71742221526bb4e14a412f4fdb5-kubernetes-cni-0.5.0.1-0.07a8a2.x86_64.rpm</span><br><span class="line">kubernetes-el7-x86_64/5612db97409141d7fd839e734d9ad3864dcc16a630b2a91c312589a0a0d960d0-kubeadm-1.6.0-0.alpha.0.2074.a092d8e0f95f52.x86_64.rpm</span><br><span class="line">kubernetes-el7-x86_64/8a299eb1db946b2bdf01c5d5c58ef959e7a9d9a0dd706e570028ebb14d48c42e-kubelet-1.5.1-0.x86_64.rpm</span><br><span class="line">kubernetes-el7-x86_64/93af9d0fbd67365fa5bf3f85e3d36060138a62ab77e133e35f6cadc1fdc15299-kubectl-1.5.1-0.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> kubernetes-el7-x86_64/</span><br><span class="line"></span><br><span class="line">rpm -ivh *</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure>
<p>接下来开始基于Kubeadm 创建k8s集群，不过在开始之前，我们先准备下需要用到的镜像，因为kubeadm创建的k8s集群中的kub-api, kube-scheduler, kube-proxy, kube-controller-manager,etcd等服务都是直接拉取镜像跑在k8s集群中，为了避免安装过程中下载镜像浪费太多时间，这里先把镜像下载好。各个版本需要下载的镜像版本也不一样。参考如下：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-08-14-k8s-image.png" alt="k8s-image"></p>
<p>我们直接用的最新版1.7.3，如果服务器可以翻墙，直接拉取镜像：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">images=(kube-proxy-amd64:v1.7.3 kube-scheduler-amd64:v1.7.3 kube-controller-manager-amd64:v1.7.3 kube-apiserver-amd64:v1.7.3 etcd-amd64:3.0.17 k8s-dns-sidecar-amd64:1.14.4 pause-amd64:3.0 k8s-dns-kube-dns-amd64:1.14.4 k8s-dns-dnsmasq-nanny-amd64:1.14.4)</span><br><span class="line"><span class="keyword">for</span> imageName <span class="keyword">in</span> <span class="variable">$&#123;images[@]&#125;</span> ; <span class="keyword">do</span></span><br><span class="line">  docker pull gcr.io/google_containers/<span class="variable">$imageName</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>如果不能翻墙，可以先翻墙下载下来，然后push到dockerhub上，再pull下来,注意pull下来之后，还是要更改tag为gcr.io/google_containers/$imageName形式。</p>
<h3 id="master-节点安装"><a href="#master-节点安装" class="headerlink" title="master 节点安装"></a>master 节点安装</h3><p>在master节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init</span><br></pre></td></tr></table></figure>
<p>执行完成后，会输出一个token，node节点安装时会用到。<br>这里有一个小坑：该过程一直卡在“[apiclient] Created API client, waiting for the control plane to become ready” ，可以去message里找相关log，一般是两种情况导致，一种是用了proxy，一种是<a href="https://github.com/kubernetes/kubernetes/issues/43800" target="_blank" rel="noopener">cgroup-driver配置错误</a>，我这边有一次是因为下载的镜像不对，kubeadm默认应该是安转最新版本，比如kubeadm1.6.x会安装1.6.9的相关组件（api-server-1.6.9.controller-manager-1.6.9等），而kubeadm1.7.x会默认安装1.7.x里面的最高版本（此时是1.7.4），所以要下载合适版本的镜像。</p>
<h3 id="node-节点安装"><a href="#node-节点安装" class="headerlink" title="node 节点安装"></a>node 节点安装</h3><p>在各node节点执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join --token=976234.e91451d4305bc282 172.16.21.53</span><br></pre></td></tr></table></figure>
<p>全部执行完成后，在master节点验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME                   STATUS         AGE</span><br><span class="line">k8s-master.novalocal   Ready,master   4d</span><br><span class="line">k8s-node1.novalocal    Ready          4d</span><br><span class="line">k8s-node2.novalocal    Ready          4d</span><br><span class="line">k8s-node3.novalocal    Ready          4d</span><br></pre></td></tr></table></figure>
<h3 id="部署pod网络"><a href="#部署pod网络" class="headerlink" title="部署pod网络"></a>部署pod网络</h3><p>这里选择Weave Net。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl apply -f https://git.io/weave-kube</span></span><br></pre></td></tr></table></figure>
<p>等待一段时间，利用下列命令查看部署情况。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods --all-namespaces</span></span><br></pre></td></tr></table></figure>
<h3 id="部署sock-shop微服务demo"><a href="#部署sock-shop微服务demo" class="headerlink" title="部署sock-shop微服务demo"></a>部署sock-shop微服务demo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl create namespace sock-shop</span></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl apply -n sock-shop -f "https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span></span><br></pre></td></tr></table></figure>
<p>查看服务部署情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -n sock-shop</span></span><br></pre></td></tr></table></figure></p>
<p>访问172.16.21.34:30001验证。</p>
<h2 id="Harbor-安装"><a href="#Harbor-安装" class="headerlink" title="Harbor 安装"></a>Harbor 安装</h2><p>比较简单，参考<a href="https://github.com/vmware/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">harbor doc</a>。</p>
<p>注意：docker 默认连接镜像使用https，而harbor默认安装是走的http，所以需要修改/etc/docker/daemon.json，添加</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"registry-mirrors"</span>: [<span class="string">"&lt;your accelerate address&gt;"</span>],</span><br><span class="line">    <span class="string">"insecure-registries"</span>: [<span class="string">"172.16.21.44"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="k8s-添加私有镜像"><a href="#k8s-添加私有镜像" class="headerlink" title="k8s 添加私有镜像"></a>k8s 添加私有镜像</h2><p>官方给出了三种解决方案：</p>
<ul>
<li>在node节点配置私有镜像的认证登录文件，其实相当于在node本地执行docker login后，在/.docker目录下生成的一个config.json文件。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ~/.docker/config.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"auths"</span>: &#123;</span><br><span class="line">        <span class="string">"registry.cn-hangzhou.aliyuncs.com/xxxx/rbd-rest-api"</span>: &#123;</span><br><span class="line">            <span class="string">"auth"</span>: <span class="string">"xxxxyyyyzzzz"</span>   <span class="comment">#一个base64编码结果，不太安全</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这种方法比较繁琐，而且不安全，不推荐。</p>
<ul>
<li>利用kubectl创建docker-registry的secret</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL</span><br><span class="line"></span><br><span class="line">kubectl get secret --all-namespaces   <span class="comment">#查看创建的secret</span></span><br></pre></td></tr></table></figure>
<p>在写dockerfile的时候指定imagePullSecrets即可，示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ./deployment-with-secret.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment-from-harbor</span><br><span class="line">  namespace: kube-public</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-for-test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx-for-test</span><br><span class="line">        image: 172.16.21.253:10080/aisino-lib/docker.io/nginx:latest</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor-k8s-secret</span><br></pre></td></tr></table></figure>
<ul>
<li>通过secret yaml文件创建pull image所用的secret,其实跟上述方法类似，不过是用yaml文件创建的secret.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat myregistrykey.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: myregistrykey</span><br><span class="line">  namespace: awesomeapps</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: &#123;base64 -w 0 ~/.docker/config.json&#125;</span><br><span class="line"><span class="built_in">type</span>: kubernetes.io/dockerconfigjson</span><br></pre></td></tr></table></figure>
<p>其中，dockerconfigjson后面的数据就是docker login后生成的config.json文件的base64编码输出（-w 0让base64输出在单行上，避免折行）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f myregistrykey.yaml</span><br></pre></td></tr></table></figure>
<p>secret使用方式与第二种方式一样，不过kubectl和yaml创建的两个secret的类型略有不同，前者是kubernetes.io/dockercfg，后者是kubernetes.io/dockerconfigjson。</p>
<h2 id="部署dashboard"><a href="#部署dashboard" class="headerlink" title="部署dashboard"></a>部署dashboard</h2><p>由<a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">README</a> 文件可知，有两种部署方式，如果是没有安装RBAC权限控制的，可以执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://git.io/kube-dashboard</span><br></pre></td></tr></table></figure>
<p>如果有RBAC的，可以执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f https://git.io/kube-dashboard-no-rbac</span><br></pre></td></tr></table></figure>
<p>kubeadm安装方式自从1.6+版之后自动安装RBAC，所以需要选择第二种。如果权限问题依旧（注：一般是报错serviceaccount:kube-system:default” cannot list statefulsets.apps in the namespace “default”.）可以根据该<a href="https://github.com/kubernetes/dashboard/issues/1803" target="_blank" rel="noopener">issue</a>,添加一个权限。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat dashboard-rbac.yml</span></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: dashboard-admin</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin </span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: default</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create -f dashboard-rbac.yml</span></span><br></pre></td></tr></table></figure>
<p>注：如果想外部可以直接访问dashboard，需要修改下yaml文件，将最后的service配置修改为nodePort,示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.......................</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - nodePort: 30002</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 9090</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br></pre></td></tr></table></figure>
<p>这样便可以直接<a href="http://NODEIP:30002访问。关于port，nodePort" target="_blank" rel="noopener">http://NODEIP:30002访问。关于port，nodePort</a>, targetPort,可以参考<a href="http://blog.csdn.net/xinghun_4/article/details/50492041" target="_blank" rel="noopener">kubernetes中port、target port、node port的对比分析，以及kube-proxy代理</a></p>
<h2 id="部署Heapster-监控与统计"><a href="#部署Heapster-监控与统计" class="headerlink" title="部署Heapster 监控与统计"></a>部署Heapster 监控与统计</h2><p>Heapster是一个容器集群监控和性能分析工具，天然支持Kubernetes和CoreOS。<br>这里使用influxDB作为Heapster的后端存储部署，参考<a href="https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md" target="_blank" rel="noopener">安装文档</a>.<br>首先下载对应版本的相关yaml文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubernetes/heapster/archive/v1.3.0.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压并直接部署即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf v1.3.0.tar.gz</span><br><span class="line"><span class="built_in">cd</span> heapster-1.3.0/deploy/kube-config/influxdb</span><br><span class="line"></span><br><span class="line">kubectl create -f ./*</span><br></pre></td></tr></table></figure>
<p>该过程会pull相关镜像，同样，可以先翻墙pull下来再push到私有镜像仓库再使用。<br>最终完成后，所有pods都running,可以看到dashboard的界面多了仪表盘。</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-08-16-dashboard.png" alt="dashboard"></p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/" target="_blank" rel="noopener">k8s-doc-Installing kubeadm</a></p>
<p><a href="http://yoyolive.com/2017/02/27/Kubernetes-1-5-3-Local-Install/" target="_blank" rel="noopener">CentOS 7 安装Kubernetes 1.5.3 集群(本地安装)</a></p>
<p><a href="http://tonybai.com/2016/11/16/how-to-pull-images-from-private-registry-on-kubernetes-cluster/?utm_source=rss" target="_blank" rel="noopener">Kubernetes从Private Registry中拉取容器镜像的方法</a></p>
<p><a href="https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry" target="_blank" rel="noopener">k8s-doc-Images</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/08/openstack-resource-segeration/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/08/openstack-resource-segeration/" itemprop="url">
                  Openstack-- openstack 中的资源分离策略
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-08T13:54:20+08:00">
                2017-08-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/08/openstack-resource-segeration/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/08/openstack-resource-segeration/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="为什么做资源的分离"><a href="#为什么做资源的分离" class="headerlink" title="为什么做资源的分离"></a>为什么做资源的分离</h2><p>openstack 作为一个云计算框架，需要统筹计算，存储，网络等资源，本身就已经够复杂了。如果是在一个非常大的环境中，节点众多，且复杂多样，这个时候就有必要根据这些节点的差异做一些逻辑上的分离，以达到不同资源的区分，这样，做水平扩展的时候也可以根据这个逻辑分区针对性的进行扩展。</p>
<h2 id="openstack-中的资源分离策略"><a href="#openstack-中的资源分离策略" class="headerlink" title="openstack 中的资源分离策略"></a>openstack 中的资源分离策略</h2><p>openstack 的资源分离策略一定程度上借鉴了AWS的策略，整体来说，如下：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-08-08-rc-se.png" alt="resource-segeration"></p>
<h3 id="Infrastructure-segregation"><a href="#Infrastructure-segregation" class="headerlink" title="Infrastructure segregation"></a>Infrastructure segregation</h3><p>主要是理解Regions, cells, Host aggregates, Availability zones这几个概念。</p>
<ul>
<li>Regions:借鉴自AWS，更像是一个地理上的概念（比如北京的数据中心可以作为一个region,南京的数据中心作为另一个region），每个region有自己独立的endpoint，regions之间完全隔离，不同regions之间可以共享keystone/dashboard。openstack默认新建一个region，即RegionOne，如果还想建立第二个Region，可以利用 keystone endpoint­create 命令添加。<br><img src="http://oeptotikb.bkt.clouddn.com/2017-08-08-region1.png" alt="region"></li>
</ul>
<ul>
<li>Cells:cell主要是为了解决openstack 的扩展性以及规模瓶颈而引进的概念。当openstack达到一定规模后，依赖比较强的database以及AMQP便成为整个系统的瓶颈，引入cell后，每个cell有自己独立的database和AMQP。公司云平台只是一个小云，没有引入，所以不去深究。cell目前是有v1，v2两版，实现方式差异比较大，感兴趣可以参考<a href="https://www.ustack.com/news/what-is-nova-cells-v2/?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">Nova Cells V2如何帮助OpenStack集群突破性能瓶颈？</a>。</li>
<li>Host aggregates &amp;&amp; Availability zones：这两个概念有共通点，且要相互配合使用，都用来表示一组节点的集合，简单说，AZ(Availability zone)是一个面向用户的概念，(这里只讨论nova 范畴的AZ,cinder,neutron也有对应的AZ概念),AZ一般依据地址，网络部署或电力配置划分，可以是一个独立的机房，或者一个独立供电的机架等，用户在创建instance的时候可以指定AZ，从而使instance创建在指定的AZ中，而host aggregate是一个面向管理员的概念，主要用来给nova-scheduler调度使用，比如根据某一属性（例如含有固态硬盘）划分一个host aggregate，把所有含有固态硬盘的host都放到该host aggregate中，nova-scheduler调度时指定相关属性就可以调度到对应host aggregate中的host。</li>
</ul>
<p><img src="http://oeptotikb.bkt.clouddn.com/2018-08-08-az.png" alt="AZandHG"><br>如上图，有两个地理隔离的Region，四个AZ，以及若干个根据不同属性区分的host aggregate。host aggregate 创建的时候可以指定AZ（如果AZ没有就会自动创建一个AZ），一个host可以属于多个host aggregate，但只能属于一个AZ。如下为一个示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nova aggregate­create storage­optimized storage­optimized-AZ  <span class="comment"># 创建一个host aggregate</span></span><br><span class="line">nova aggregate­<span class="built_in">set</span>­metadata <span class="variable">$aggregateID</span> fast­storage=<span class="literal">true</span> <span class="comment"># 设置 aggregate 的metadate</span></span><br><span class="line">nova aggregate­add­host <span class="variable">$aggregateID</span> host­1 <span class="comment">#添加host到aggregate</span></span><br><span class="line">nova flavor­key <span class="variable">$flavorID</span> <span class="built_in">set</span> fast­storage=<span class="literal">true</span> <span class="comment">#添加flavor的 条件</span></span><br></pre></td></tr></table></figure>
<h3 id="Workload-segregation"><a href="#Workload-segregation" class="headerlink" title="Workload segregation"></a>Workload segregation</h3><p>负载这一块的策略是基于server-group来做的，相对比较简单。注意，这里的server-group不再是host的集合，而是instance的集合。比如，我要创建3个instance，因为这3个instance都比较吃内存，所以想要这3个instance在不同的host上，就可以这样做：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nova server-group-create --policy anti-affinity group-1 <span class="comment"># 创建server-group，注意policy指定策略是不在同一节点</span></span><br><span class="line">nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst1 <span class="comment">#创建instance1 </span></span><br><span class="line">nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst2 <span class="comment">#创建instance2 </span></span><br><span class="line">nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst3 <span class="comment">#创建instance3</span></span><br></pre></td></tr></table></figure>
<p>在nova配置文件中指定scheduler策略，有一个针对server-group的filter叫做ServerGroupAntiAffinityFilter。除了anti-affinity，还有 affinity策略，就是尽量让同一server-group的instance在同一个host上。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://www.jianshu.com/p/613d34ad6d51" target="_blank" rel="noopener">理解openstack中region、cell、availability zone、host aggregate 概念</a></p>
<p><a href="http://happylab.blog.51cto.com/1730296/1739180" target="_blank" rel="noopener">openstack运维实战系列(十二)之nova aggregate资源分组</a></p>
<p><a href="https://www.openstack.org/assets/presentation-media/divideandconquer-2.pdf" target="_blank" rel="noopener">DIVIDE AND CONQUER:RESOURCE SEGREGATION IN THE OPENSTACK<br>CLOUD</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/05/intro-to-etcd/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/05/intro-to-etcd/" itemprop="url">
                  Kubernetes-- 关于ETCD && 服务发现的一些记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-05T14:28:00+08:00">
                2017-08-05
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/05/intro-to-etcd/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/05/intro-to-etcd/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="ETCD简介"><a href="#ETCD简介" class="headerlink" title="ETCD简介"></a>ETCD简介</h2><p>ETCD是一个可靠的键值对分布式存储系统，适用场景是用来存储那些写少读多，结构简单，但又比较重要的数据，这里的比较重要是指要保证数据的高可用，一致性。第一个想到的自然就是配置数据的存储与管理。当然，也可以存储其他满足上述要求的数据，比如在k8s中，ETCD会用来存储k8s各items(pods, rc, rs, deployment等)的状态。随着微服务架构的日益火热，ETCD也常作为服务发现的组件来使用。<br>ETCD 有以下几个特性：</p>
<ul>
<li>使用简单：友好的API(grpc)</li>
<li>安全：支持TLS双向通信</li>
<li>快速：支持10,000 写/秒</li>
<li>可靠： 采用raft协议作分布式选举</li>
</ul>
<h2 id="ETCD-安装"><a href="#ETCD-安装" class="headerlink" title="ETCD 安装"></a>ETCD 安装</h2><p>因为是go写的，直接下载二进制文件执行即可，如果想要尝试最新版的，可以去下载源码，自己build,参考<a href="https://coreos.com/etcd/docs/latest/dl_build.html" target="_blank" rel="noopener">Download and build</a>。<br>如果是构造一个cluster，也比较简单，参考<a href="https://coreos.com/etcd/docs/latest/demo.html" target="_blank" rel="noopener">Demo</a></p>
<p>简单记录下常用的启动参数：</p>
<ul>
<li>name：节点名称，默认为 default，在集群中应该保持唯一，一般使用 hostname</li>
<li>data-dir：服务运行数据保存的路径，默认为 ${name}.etcd</li>
<li>snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘</li>
<li>heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms</li>
<li>eletion-timeout：重新投票的超时时间，如果 follower在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms</li>
<li>listen-peer-urls:和同伴通信的地址，比如 <a href="http://ip:2380" target="_blank" rel="noopener">http://ip:2380</a> ，如果有多个，使用逗号分隔。需要所有节点都能够访问， 所以不要使用 localhost！</li>
<li>listen-client-urls: 对外提供服务的地址：比如 <a href="http://ip:2379,http://127.0.0.1:2379" target="_blank" rel="noopener">http://ip:2379,http://127.0.0.1:2379</a><br>，客户端会连接到这里和 etcd 交互</li>
<li>advertise-client-urls: 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点</li>
<li>initial-advertise-peer-urls: 该节点同伴监听地址，这个值会告诉集群中其他节点</li>
<li>initial-cluster: 集群中所有节点的信息，格式为 node1=<a href="http://ip1:2380,node2=http://ip2:2380,....." target="_blank" rel="noopener">http://ip1:2380,node2=http://ip2:2380,.....</a>. 注意：这里的 node1是节点的–name指定的名字；后面的 ip1:2380是 –initial-advertise-peer-urls<br>指定的值。</li>
<li>initial-cluster-state：新建集群的时候，这个值为 new，假如已经存在的集群，这个值为 existing。</li>
<li>initial-cluster-token：创建集群的 token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点uuid；否则会导致多个集群之间的冲突，造成未知的错误。</li>
</ul>
<p>当然，还有比如tls设置，使用etcd discovery/dns discovery 进行etcd的启动等，配置详情见官网。</p>
<h2 id="ETCD-使用"><a href="#ETCD-使用" class="headerlink" title="ETCD 使用"></a>ETCD 使用</h2><p>一般通过两种方式，rest api或者通过命令行（本质也是rest api）,下面简单介绍下这两种方式。在介绍前，先要弄懂几个概念：</p>
<ul>
<li>member： 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node上的其它应用程序提供服务。</li>
<li>Cluster： Cluster 由多个 member 组成。每个 member 中的 node 遵循 raft共识协议来复制日志。Cluster 接收来自 member的提案消息，将其提交并存储于本地磁盘。</li>
<li>Peer： 同一 Cluster 中的其它 member。</li>
</ul>
<h3 id="命令行方式示例"><a href="#命令行方式示例" class="headerlink" title="命令行方式示例"></a>命令行方式示例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> put foo <span class="string">"Hello World!"</span>  <span class="comment"># 写入操作</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get foo  <span class="comment"># 读取操作</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> --write-out=<span class="string">"json"</span> get foo  <span class="comment"># 以json的方式输出</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get web --prefix  <span class="comment"># 获取所有前缀是web 的key的value</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> del key  <span class="comment"># 删除</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> txn --interactive  <span class="comment"># 将多个命令封装在一个事务中</span></span><br><span class="line"></span><br><span class="line">compares:</span><br><span class="line">value(<span class="string">"user1"</span>) = <span class="string">"bad"</span>      </span><br><span class="line"></span><br><span class="line">success requests (get, put, delete):</span><br><span class="line">del user1  </span><br><span class="line"></span><br><span class="line">failure requests (get, put, delete):</span><br><span class="line">put user1 good</span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> watch stock1  <span class="comment"># 监视某个值</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> lease grant 300 <span class="comment">#设定租约</span></span><br><span class="line"><span class="comment"># lease 2be7547fbc6a5afa granted with TTL(300s)</span></span><br><span class="line"></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> put sample value --lease=2be7547fbc6a5afa <span class="comment"># 绑定租约</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get sample <span class="comment"># 租约期限内可以获取值</span></span><br><span class="line"></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> lease keep-alive 2be7547fbc6a5afa <span class="comment"># 维持租约</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> lease revoke 2be7547fbc6a5afa  <span class="comment"># 撤销租约</span></span><br><span class="line"><span class="comment"># or after 300 seconds</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> get sample <span class="comment"># 撤销租约或者300s后获取不到值</span></span><br><span class="line">$ etcdctl --write-out=table --endpoints=<span class="variable">$ENDPOINTS</span> endpoint status <span class="comment"># 集群状态</span></span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> endpoint health</span><br><span class="line">$ etcdctl --endpoints=<span class="variable">$ENDPOINTS</span> snapshot save my.db <span class="comment"># 快照</span></span><br></pre></td></tr></table></figure>
<h3 id="rest-api-示例"><a href="#rest-api-示例" class="headerlink" title="rest api 示例"></a>rest api 示例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">curl http://127.0.0.1:2379/v2/keys/message <span class="comment"># 获取某个key的value</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"action"</span>: <span class="string">"get"</span>,</span><br><span class="line">    <span class="string">"node"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 2,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 2,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello world"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">curl http://127.0.0.1:2379/v2/keys/message -XPUT -d value=<span class="string">"Hello etcd"</span> <span class="comment"># 更改 </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"action"</span>: <span class="string">"set"</span>,</span><br><span class="line">    <span class="string">"node"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 3,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 3,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello etcd"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"prevNode"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 2,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello world"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 2</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">curl http://127.0.0.1:2379/v2/keys/message -XDELETE  <span class="comment">#删除 </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"action"</span>: <span class="string">"delete"</span>,</span><br><span class="line">    <span class="string">"node"</span>: &#123;</span><br><span class="line">        <span class="string">"createdIndex"</span>: 3,</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 4</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"prevNode"</span>: &#123;</span><br><span class="line">        <span class="string">"key"</span>: <span class="string">"/message"</span>,</span><br><span class="line">        <span class="string">"value"</span>: <span class="string">"Hello etcd"</span>,</span><br><span class="line">        <span class="string">"modifiedIndex"</span>: 3,</span><br><span class="line">        <span class="string">"createdIndex"</span>: 3</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="架构-amp-原理介绍"><a href="#架构-amp-原理介绍" class="headerlink" title="架构 &amp; 原理介绍"></a>架构 &amp; 原理介绍</h2><p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/etcd-arch-20170927110531.jpg" alt="etcd-arch"><br>etcd主要分为四个部分:</p>
<ul>
<li>HTTP Server： 用于处理用户发送的API请求以及其它etcd节点的同步与心跳信息请求。</li>
<li>Store：用于处理etcd支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是etcd对用户提供的大多数API功能的具体实现。</li>
<li>Raft：Raft强一致性算法的具体实现，是etcd的核心。</li>
<li>WAL：Write Ahead Log（预写式日志），是etcd的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd就通过WAL进行持久化存储。WAL中，所有的数据提交前都会事先记录日志。Snapshot是为了防止数据过多而进行的状态快照；Entry表示存储的具体日志内容。<br>通常，一个用户的请求发送过来，会经由HTTP Server转发给Store进行具体的事务处理，如果涉及到节点的修改，则交给Raft模块进行状态的变更、日志的记录，然后再同步给别的etcd节点以确认数据提交，最后进行数据的提交，再次同步。</li>
</ul>
<p>关于集群状态机的转变可以参考<a href="http://www.infoq.com/cn/articles/coreos-analyse-etcd" target="_blank" rel="noopener">CoreOS 实战：剖析 etcd</a></p>
<h2 id="服务发现工具对比"><a href="#服务发现工具对比" class="headerlink" title="服务发现工具对比"></a>服务发现工具对比</h2><p>引自<a href="http://dockone.io/article/667" target="_blank" rel="noopener">服务发现：Zookeeper vs etcd vs Consul</a>：</p>
<blockquote>
<blockquote>
<blockquote>
<p>所有这些工具都是基于相似的原则和架构，它们在节点上运行，需要仲裁来运行，并且都是强一致性的，都提供某种形式的键/值对存储。<br>Zookeeper是其中最老态龙钟的一个，使用年限显示出了其复杂性、资源利用和尽力达成的目标，它是为了与我们评估的其他工具所处的不同时代而设计的（即使它不是老得太多）。<br>etcd、Registrator和Confd是一个非常简单但非常强大的组合，可以解决大部分问题，如果不是全部满足服务发现需要的话。它还展示了我们可以通过组合非常简单和特定的工具来获得强大的服务发现能力，它们中的每一个都执行一个非常具体的任务，通过精心设计的API进行通讯，具备相对自治工作的能力，从架构和功能途径方面都是微服务方式。<br>Consul的不同之处在于无需第三方工具就可以原生支持多数据中心和健康检查，这并不意味着使用第三方工具不好。实际上，在这篇博客里我们通过选择那些表现更佳同时不会引入不必要的功能的的工具，尽力组合不同的工具。使用正确的工具可以获得最好的结果。如果工具引入了工作不需要的特性，那么工作效率反而会下降，另一方面，如果工具没有提供工作所需要的特性也是没有用的。Consul很好地权衡了权重，用尽量少的东西很好的达成了目标。<br>Consul使用gossip来传播集群信息的方式，使其比etcd更易于搭建，特别是对于大的数据中心。将存储数据作为服务的能力使其比etcd仅仅只有健/值对存储的特性更加完整、更有用（即使Consul也有该选项）。虽然我们可以在etcd中通过插入多个键来达成相同的目标，Consul的服务实现了一个更紧凑的结果，通常只需要一次查询就可以获得与服务相关的所有数据。除此之外，Registrator很好地实现了Consul的两个协议，使其合二为一，特别是添加Consul-template到了拼图中。Consul的Web UI更是锦上添花般地提供了服务和健康检查的可视化途径。</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://www.liuhaihua.cn/archives/404914.html" target="_blank" rel="noopener">etcd 使用入门</a></p>
<p><a href="https://coreos.com/etcd/docs/latest/v2/api.html" target="_blank" rel="noopener">etcd API</a></p>
<p><a href="https://poweruphosting.com/blog/etcd-tutorial/" target="_blank" rel="noopener">Etcd Tutorial- The Ultimate Reliable Key Value Storage for Networks</a></p>
<p><a href="https://luyiisme.github.io/2017/04/22/spring-cloud-service-discovery-products/" target="_blank" rel="noopener">服务发现比较:Consul vs Zookeeper vs Etcd vs Eureka</a></p>
<p><a href="http://www.infoq.com/cn/articles/coreos-analyse-etcd" target="_blank" rel="noopener">CoreOS 实战：剖析 etcd</a></p>
<p><a href="http://lihaoquan.me/2016/6/24/learning-etcd-1.html" target="_blank" rel="noopener">深入学习Etcd</a></p>
<p><a href="http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle" target="_blank" rel="noopener">etcd：从应用场景到实现原理的全方位解读</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/08/03/intro-to-harbor/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/08/03/intro-to-harbor/" itemprop="url">
                  Docker-- 关于Harbor 的一些记录
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-03T11:59:11+08:00">
                2017-08-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/03/intro-to-harbor/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/03/intro-to-harbor/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Harbor-简介"><a href="#Harbor-简介" class="headerlink" title="Harbor 简介"></a>Harbor 简介</h2><p><a href="https://github.com/vmware/harbor" target="_blank" rel="noopener">Harbor</a> 是 Vmware China研发开源的企业级私有容器Registry,基于docker官方的解决方案<a href="https://github.com/docker/distribution" target="_blank" rel="noopener">Distribution</a>。目前来说，已经成为企业级私有容器仓库的首选（主要是可供选择的本来就不多，可能是因为镜像管理不像容器编排那样是必争之地）。相对于Docker Distribution，Harbor添加了安全，认证，管理等功能，性能以及安全性都得到提升，主要 features如下：</p>
<ul>
<li>基于角色的访问控制：用户和镜像仓库通过project来组织管理，一个用户对同一project的不同镜像仓库会有不同处理权限。</li>
<li>基于策略的镜像复制：多个registry实例可以实现镜像同步复制，对于load-balancing,高可用，多数据中心，混合云的情景非常有用。</li>
<li>支持 LDAP/AD</li>
<li>镜像删除 &amp; 垃圾回收</li>
<li>镜像的认证</li>
<li>友好的UI</li>
<li>日志审计：所有的操作都是可追踪的</li>
<li>RESTful API</li>
<li>易部署 </li>
</ul>
<h2 id="Harbor-架构与原理"><a href="#Harbor-架构与原理" class="headerlink" title="Harbor 架构与原理"></a>Harbor 架构与原理</h2><h3 id="Harbor-整体架构"><a href="#Harbor-整体架构" class="headerlink" title="Harbor 整体架构"></a>Harbor 整体架构</h3><p>Harbor 是以容器的方式运行，以docker-compose的规范形式组织各个组件，并通过docker-compose工具进行启停。<br>Harbor共有五个组件，分别如下：</p>
<ul>
<li>Proxy:Harbor服务的所有请求都由该服务接受并转发，其实就是一个前置的反向代理Nginx，类似于微服务概念中的API-gateway.</li>
<li>Registry: 即docker 官方的Registry镜像生成的容器示例，真正负责存贮镜像的地方，处理docker pull/push。针对不同的用户对不同的镜像操作权限不同，registry强制每个请求必须含有一个token以验证权限，如果没有token，会返回一个token服务地址。</li>
<li>Core Services: 主要提供UI,webhook(设置在registry上以获取镜像状态)，token服务。</li>
<li>Database:数据库服务Mysql，存储用户，权限，审计日志，镜像信息等。</li>
<li>Log Collector: 日志收集，跑一个Rsylogd服务。</li>
</ul>
<p>架构图如下：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-08-03.harbor-arc.jpg" alt="harbor-arch"></p>
<p>这五个容器之间通过docker-link相连，即通过容器名字互相访问，暴露Proxy服务的端口给终端用户访问。</p>
<h3 id="Harbor-工作原理"><a href="#Harbor-工作原理" class="headerlink" title="Harbor 工作原理"></a>Harbor 工作原理</h3><p>以docker login 与 docker push为例讲解：</p>
<p>客户端 输入docker login 之后，流程如下图：<br><img src="http://oeptotikb.bkt.clouddn.com/2017-08-03-harbor-flow1.jpg" alt="docker-login-flow"></p>
<p>(a) 首先，这个登录请求会被Proxy容器接收到，根据预先设置的匹配规则，该请求会被转发给后端Registry容器。<br>(b) Registry接收到请求后，解析请求，因为配置了基于token的认证，所以会查找token，发现请求没有token 后，返回错误代码401以及token服务的地址URL。<br>(c) Docker客户端接收到错误请求后，转而向token服务地址发送请求，并根据HTTP协议的BasicAuthentication 规范，将用户名密码组合并编码，放在请求头部(header)。<br>(d) 同样，该请求会先发到Proxy容器，继而转发给ui/token的容器,该容器接受请求，将请求头解码，获取到用户名密码。<br>(e) ui/token的容器获取到用户名密码后，通过查询数据库进行比对验证(如果是LDAP 的认证方式,就是与LDAP服务进行校验)，比对成功后，返回成功的状态码，并用密钥生成token，一并发送给Docker客户端。</p>
<p>客户端 登陆成功后，输入docker push xxxxxx 之后，流程如下图（便于说明省略Docker client与Proxy之间通信）：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-08-03-harbor-flow-2.jpg" alt="docker-push-flow"></p>
<p>(a) 同样，首先与Registery通信，返回一个token服务的地址URL.<br>(b) Docker客户端会与token服务通信，指明要申请一个push image操作的token。<br>(c) token服务访问数据库验证当前用户是否有该操作的权限，如果有，会将image信息以及push操作进行编码，用私钥签名，生成token返回给Docker客户端。<br>(d) Docker客户端再次与Registry通信，不过这次会将token放到请求header中，Registry收到请求后利用公钥解码并核对，核对成功，便可以开始push 操作了。</p>
<h2 id="Harbor-安装"><a href="#Harbor-安装" class="headerlink" title="Harbor 安装"></a>Harbor 安装</h2><p>比较简单：</p>
<ol>
<li>准备：python&gt;=2.7, docker&gt;=1.10, docker-compose&gt;=1.6.0</li>
<li>下载离线安装包</li>
<li>修改配置文件 harbor.cfg</li>
<li>执行脚本install.sh </li>
</ol>
<p>参考<a href="https://github.com/vmware/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">Installation and Configuration Guide</a></p>
<h2 id="Harbor-高可用"><a href="#Harbor-高可用" class="headerlink" title="Harbor 高可用"></a>Harbor 高可用</h2><p>对于Harbor高可用方案，目前并没有最佳实践，不过我看issues上有不少相关内容，可以参考<a href="https://github.com/vmware/harbor/issues/327" target="_blank" rel="noopener">Harbor HA feature design proposal/discussion</a>。<br>其实，私有云相对来说对镜像的请求并非高频，在做HA的时候还是结合实际情况，切勿为了HA而HA,还要综合考量成本，安全等因素。</p>
<p>这部分内容暂且留个坑，以后再写。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://segmentfault.com/a/1190000007705296" target="_blank" rel="noopener">VMware Harbor：基于 Docker Distribution 的企业级 Registry 服务</a></p>
<p><a href="http://tonybai.com/2017/06/09/setup-a-high-availability-private-registry-based-on-harbor-and-cephfs/" target="_blank" rel="noopener">基于Harbor和CephFS搭建高可用Private Registr</a></p>
<p><a href="https://github.com/vmware/harbor" target="_blank" rel="noopener">vmware/harbor</a></p>
<p><a href="http://jaminzhang.github.io/docker/Enterprise-class-private-Docker-Container-Registry-Harbor-deploying/" target="_blank" rel="noopener">Docker 企业级私有镜像仓库 Harbor 部署</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/07/20/few-problems-met-in-openstack/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/20/few-problems-met-in-openstack/" itemprop="url">
                  Openstack-- 最近遇到的几个Openstack问题小结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-20T16:59:11+08:00">
                2017-07-20
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/07/20/few-problems-met-in-openstack/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/20/few-problems-met-in-openstack/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="win7虚拟机CPU核数不变问题"><a href="#win7虚拟机CPU核数不变问题" class="headerlink" title="win7虚拟机CPU核数不变问题"></a>win7虚拟机CPU核数不变问题</h2><p>问题描述：虚拟机镜像为win7系统，配置为4cpu的时候，发现设备管理器显示为4cpu，但是任务管理器智能识别2cpu，如下图。</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-07-24-renwu.png" alt="shebei"></p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-07-24-shebei.png" alt="renwu"></p>
<p>问题解决：刚开始以为是win7系统引导项中限制到2核，更改后问题没有解决，更换镜像后问题依旧，后经过搜索得出答案:首先需要了解kvm的cpu虚拟化原理，可以参考<a href="http://www.cnblogs.com/sammyliu/p/4543597.html" target="_blank" rel="noopener">KVM 介绍（2）：CPU 和内存虚拟化</a>,简单来说，一个KVM虚拟机就是一个Linux qemu-kvm进程，内存就是该进程的地址空间的一部分，虚拟机的vcpu作为线程运行在该进程的上下文，逻辑关系如下：</p>
<p><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-07-24-kvm-topology.jpg" alt="kvm-topology"><br>我们用kvm命令创建虚拟机时有几个概念，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -m 2048 -smp 4,sockets=4,cores=1,threads=1 -drive file=win7_x64_pure</span><br></pre></td></tr></table></figure></p>
<p>其中，smp 指定为4 ，默认后面什么都不加的话，相当于是sockets=4,cores=1,threads=1，简单解释下socket是cpu的物理单位，core是每个cpu中的物理内核，thread是利用超线程技术实现的一个core虚拟化出的逻辑cpu个数。也就是说，客户机操作系统看到的cpu核数是上述三个数值的乘积，至于使用多socket，还是多core，可以参考<a href="http://frankdenneman.nl/2013/09/18/vcpu-configuration-performance-impact-between-virtual-sockets-and-virtual-cores/" target="_blank" rel="noopener">vCPU configuration. Performance impact between virtual sockets and virtual cores?</a>。<br>回到之前的问题，openstack默认的max-sockets是4，也就是说只要socket没有特殊指定，且小于等于4，那么逻辑核数就是socket的个数。比如，创建一个4核的kvm虚拟机，那么openstack默认对应kvm命令为kvm -smp 4,sockets=4,cores=1,threads=1。而在某些客户机操作系统会限制物理 CPU （这里即socket）的数目,比如win7操作系统限制2个win，Windows Server 2008 R2 Standard Edition 限制4个。这种情况下，我们就需要更改cpu topology,比如win7,为了实现4核，可以利用以下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kvm -m 2048 -smp 4,sockets=2,cores=1,threads=2 -drive file=win7_x64_pure</span><br></pre></td></tr></table></figure></p>
<p>更多cpu topology 的知识，参考<a href="https://wiki.openstack.org/wiki/VirtDriverGuestCPUTopology" target="_blank" rel="noopener">VirtDriverGuestCPUTopology</a>。<br>回到openstack的话，我们可以给镜像添加合适的 max-sockets来解决此类问题，在win 7的镜像里加上一个属性, hw_max_sockets=2即可。</p>
<h2 id="虚拟机调整配额失败"><a href="#虚拟机调整配额失败" class="headerlink" title="虚拟机调整配额失败"></a>虚拟机调整配额失败</h2><p>问题描述：如题，对已经启动的虚拟机调整配额，没有反应。<br>问题解决：找到对应计算节点，查看nova-compute日志发现错误原因，错误日志如下：<br><img src="http://7xrnwq.com1.z0.glb.clouddn.com/2017-07-24-error-log.png" alt="nova-compute-error"></p>
<p>无法ssh到目标节点，查阅相关资料后，得出resize命令需要设置nova用户在节点之间的passwordless authentication，解决如下：</p>
<ul>
<li>sudo -u nova ssh-keygen   # 生成nova的密钥</li>
<li><p>ssh-copy-id nova@<serverip> #复制公钥到目的节点</serverip></p>
<p>tips:复制公钥的过程中如果报错“This account is currently not available”，可能是因为用户nova的shell禁止登录了，修改/etc/passwd 中nova对应的shell登录部分由“/sbin /nologin”改成“/bin/bash”。</p>
</li>
</ul>
<h2 id="ceph-节点资源耗尽"><a href="#ceph-节点资源耗尽" class="headerlink" title="ceph 节点资源耗尽"></a>ceph 节点资源耗尽</h2><p>问题描述：公司有个生产环境的云平台，因为前期的滥用，导致存储资源耗费非常严重，已经出现2个计算节点near full，这会导致出现一系列问题，比如虚拟机卡顿，无法访问等问题。<br>解决方案：</p>
<ul>
<li>删除不用的虚拟机，镜像等垃圾文件，如果出现无法删除的情况，直接利用rbd命令尝试下，如果还是无法删除，应该是对应osd达到full状态而拒绝客户端操作命令了，只能换个方法。</li>
<li>如果条件允许的话，最好是增加OSD节点，然后再平衡就OK了，不过再平衡的时间太长，对于线上业务是无法接受的。</li>
<li>通过rewight命令手动修改对应osd wight值，比如，对于处于near full/full状态的osd，我们减小其对应的weight值，适当增大有较多剩余空间的osd的weight值。调整的过程中不要一下全部更改，需要调整一下，看下效果，避免因为改动过大出现大范围的再平衡导致时间过长。</li>
<li>在osd 再平衡期间，增加mon-osd-full-ratio/mon osd nearfull ratio值（未验证） </li>
</ul>
<h2 id="Failed-to-allocate-the-network-s-not-rescheduling"><a href="#Failed-to-allocate-the-network-s-not-rescheduling" class="headerlink" title="Failed to allocate the network(s), not rescheduling."></a>Failed to allocate the network(s), not rescheduling.</h2><p>创建虚拟机的时候报如标题所述错误，查看详细日志，可以看出是Virtual Interface creation failed.</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-08-09-CREATIO-INTERFACE-FAIL.png" alt="virtual-interface-fail"></p>
<p>自然想到可能是Virtual Interface 创建失败导致创建虚拟机失败，查看neutron-server日志，发现如下信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2017-08-09 11:21:08.499 154120 WARNING neutron.notifiers.nova [-] Nova returned NotFound <span class="keyword">for</span> event: [&#123;<span class="string">'tag'</span>: u<span class="string">'a653baaf-828d-4641-aabb-1a82c5163889'</span>, <span class="string">'name'</span>: <span class="string">'network-vif-deleted'</span>, <span class="string">'server_uuid'</span>: u<span class="string">'4fdf7471-bece-4d93-       a044-a4052284c69b'</span>&#125;]</span><br></pre></td></tr></table></figure></p>
<p>也就是说nova没有接受到network-vif-deleted的event，查看具体出错代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_create_domain_and_network</span><span class="params">(self, context, xml, instance, network_info,</span></span></span><br><span class="line"><span class="function"><span class="params">                               disk_info, block_device_info=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                               power_on=True, reboot=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                               vifs_already_plugged=False)</span>:</span></span><br><span class="line">   ...............................................................</span><br><span class="line">    <span class="keyword">except</span> eventlet.timeout.Timeout:</span><br><span class="line">        <span class="comment"># We never heard from Neutron</span></span><br><span class="line">        LOG.warn(_LW(<span class="string">'Timeout waiting for vif plugging callback for '</span></span><br><span class="line">                     <span class="string">'instance %(uuid)s'</span>), &#123;<span class="string">'uuid'</span>: instance.uuid&#125;,</span><br><span class="line">                 instance=instance)</span><br><span class="line">        <span class="keyword">if</span> CONF.vif_plugging_is_fatal:   <span class="comment">#关键在这行</span></span><br><span class="line">            <span class="keyword">if</span> guest:</span><br><span class="line">                guest.poweroff()</span><br><span class="line">            self.cleanup(context, instance, network_info=network_info,</span><br><span class="line">                         block_device_info=block_device_info)</span><br><span class="line">            <span class="keyword">raise</span> exception.VirtualInterfaceCreateException()</span><br></pre></td></tr></table></figure>
<p>可以看到在nova-compute调用_create_domain_and_network函数的时候，会一直等待vif 的创建eventlet（由openvswitch-agent创建并返回evetlet），等待timeout时间之后，如果配置文件中vif_plugging_is_fatal=True,就会创建失败并回滚，如果vif_plugging_is_fatal=False就会略过。<br>明白原理后就简单了，只需修改nova配置文件中vif_plugging_is_fatal=False就可以了，至于为什么nova没有收到eventlet，还有待深入。这一部分可以参考<a href="http://blog.csdn.net/bc_vnetwork/article/details/52231418" target="_blank" rel="noopener">nova network-vif-plugged事件分析1</a>.</p>
<p>注：排错过程有个小插曲，我修改完配置文件后再重启nova-compute还是没效果，果断打断点调试，结果发现断点直接略过了，百思不得其解，折腾半天后，发现是有一个残留的nova-compute的进程一直在跑，kill掉之后，再启动就可以了。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://www.cnblogs.com/sammyliu/p/4543597.html" target="_blank" rel="noopener">KVM 介绍（2）：CPU 和内存虚拟化</a></p>
<p><a href="http://www.pystack.org/openstack-windows7-cpu-core-count-display-inconsistencies/" target="_blank" rel="noopener">Openstack Windows7 CPU核数显示不一致</a></p>
<p><a href="http://frankdenneman.nl/2013/09/18/vcpu-configuration-performance-impact-between-virtual-sockets-and-virtual-cores/" target="_blank" rel="noopener">vCPU configuration. Performance impact between virtual sockets and virtual cores?</a></p>
<p><a href="http://lists.openstack.org/pipermail/openstack-operators/2013-January/002424.html" target="_blank" rel="noopener">Host key verification failed on VM Resize</a></p>
<p><a href="http://docs.ceph.com/docs/master/cephfs/full/" target="_blank" rel="noopener">HANDLING A FULL CEPH FILESYSTEM</a></p>
<p><a href="http://xiaoquqi.github.io/blog/2015/05/12/ceph-osd-is-full/" target="_blank" rel="noopener">Ceph集群磁盘没有剩余空间的解决方法</a></p>
<p><a href="http://blog.csdn.net/bc_vnetwork/article/details/52231418" target="_blank" rel="noopener">nova network-vif-plugged事件分析1</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://zhangchenchen.github.io/2017/06/30/tangle-with-nova-network/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="pekingzcc">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Solar">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Solar" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/30/tangle-with-nova-network/" itemprop="url">
                  Openstack-- nova-network 网络实现一窥
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-30T09:16:11+08:00">
                2017-06-30
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/06/30/tangle-with-nova-network/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/30/tangle-with-nova-network/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引出"><a href="#引出" class="headerlink" title="引出"></a>引出</h2><p>nova-network 是Neutron（或Quantum）之前，openstack 的网络管理项目，随着Neutron项目的愈发成熟，nova-network 也随之逐渐被弃用。<br>不过，其实 nova-network 还是非常好用的，相对neutron 来说，简单轻便，也经受过生产环境的验证。而且，可以通过简单配置将虚拟机二层网络与真实物理网络打通，实现利用fixed-ip就可以直接访问虚拟机的单网络环境。<br>本文主要总结下 nova-network 的网络管理以及实现原理，并与neutron 做一下比较。</p>
<h2 id="nova-network-网络类型"><a href="#nova-network-网络类型" class="headerlink" title="nova-network 网络类型"></a>nova-network 网络类型</h2><p>nova-network 支持两种网络类型（确切地说是三种，这里将Flat与FlatDHCP看做一类），分别是Flat/FlatDHCP 和 Vlan类型。</p>
<h3 id="FlatManager-and-FlatDHCPManager"><a href="#FlatManager-and-FlatDHCPManager" class="headerlink" title="FlatManager and FlatDHCPManager"></a>FlatManager and FlatDHCPManager</h3><p>顾名思义，flat即扁平化，也就是说所有的虚拟机都在一个大的二层网络空间内，这种网络模式一般生产环境中很少用，多用于POC阶段。<br>关于flat networking，我在openstack 上找到一篇<a href="https://wiki.openstack.org/wiki/UnderstandingFlatNetworking" target="_blank" rel="noopener">wiki</a>，讲解的比较清楚。可惜wiki 上只有讲解flat的，没有其他网络类型。</p>
<p>这里借上文中的一张图简单说下flat这种网络类型，以多节点，多网卡为例：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-06-30-FlatNetworkMultInterface.png" alt="multi-host-1"></p>
<p>由图看出，计算节点内都有一个网桥br100与 物理网卡eth0相连，计算节点内的虚拟机通过该网桥与控制节点的对应物理网卡连接实现通信。<br>虚拟机实现南北通信大致如下图：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-06-30-MultiInterfaceOutbound_2.png" alt="multi-host-2"></p>
<p>flatDHCP 其实就是在计算节点再运行一个dnsmasp来提供DHCP服务，就不需要借助外部的DHCP服务，两者对比大致如下：</p>
<p>flat networking :</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-06-30-flatdhcp.png" alt="flat"></p>
<p>flatDHCP networking:</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170630144105.png" alt="flat-dhcp"></p>
<h3 id="VlanManager"><a href="#VlanManager" class="headerlink" title="VlanManager"></a>VlanManager</h3><p>flat networking 有一个很大的缺点就是所有虚拟机都在一个二层网络，没有租户间的网络隔离，不够灵活，且广播风暴的影响太大，这种情况下，vlan 就出现了。</p>
<p>vlan 网络或给每个租户创建一个（或多个）二层网络，且这些二层网络相互隔离。不过这种网络类型需要支持vlan tag的交换机才可以使用。</p>
<h2 id="nova-network-网络实现原理"><a href="#nova-network-网络实现原理" class="headerlink" title="nova-network 网络实现原理"></a>nova-network 网络实现原理</h2><p>简单介绍下各自的实现原理</p>
<h3 id="Flat-FlatDHCP网络实现原理"><a href="#Flat-FlatDHCP网络实现原理" class="headerlink" title="Flat/FlatDHCP网络实现原理"></a>Flat/FlatDHCP网络实现原理</h3><p>Flat类型的基本上上文已经讲了，主要看下flatDHCP 的实现：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-06-30-flat-dhcp-real.png" alt="flat-dhcp"></p>
<p>跟flat类似，不过在计算节点的网桥上会有一个dnsmasq进程监听并实现该节点的虚拟机IP动态分配。<br>还有一点要注意的是，不同计算节点的虚拟机内网关也不同，比如，上图中，vm_1与vm_2的网关都是10.10.0.1，但在右边计算节点的vm_3 和vm_4的网关却是10.10.0.4。</p>
<h3 id="Vlan实现原理"><a href="#Vlan实现原理" class="headerlink" title="Vlan实现原理"></a>Vlan实现原理</h3><p>一图胜千言：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-06-30-vlanmanager-2-hosts-2-tenants.png" alt="multi-host-vlan"></p>
<p>上面这幅图就是vlan模式部署最为广泛的场景，即multi-node模式，也就是说不光是网络节点需要运行nova-network服务，计算节点也要运行nova-network（还需要运行nova-api以提供metadata服务），这样就避免出现SPOF,也可以达到分流的作用（跟neutron 的DVR很类似，估计DVR就是借鉴了这里）。<br>需要注意的是每个vlan网桥都是租户独占的，且会创建vlan接口如vlan102,依据802.1q协议打vlanid，与网关eth0连接。Dnsmasq监听网桥网关，负责fixedip的分配。switch port设定为chunk mode。eth0负责vm之间的数据通信，另一网卡如eth1负责外网访问。上图只是显示了一个网卡，如下图是一个多网卡的情况：</p>
<p><img src="http://oeptotikb.bkt.clouddn.com/2017-06-30-multi-host-vlan.png" alt="vlan-multi-host"></p>
<h2 id="nova-network-vs-neutron"><a href="#nova-network-vs-neutron" class="headerlink" title="nova-network vs neutron"></a>nova-network vs neutron</h2><p>之前总结过一篇关于<a href="https://zhangchenchen.github.io/2017/02/12/neutron-layer2-3-realization-discovry/">neutron 实现二三层网络的总结</a>,就功能性来说，nova-network 只支持两种网络类型，neutron增加了overlay network 的支持，如vxlan/gre, 突破了vlan id个数限制从而支持更多数目的二层网络。而且nova-network 即使是不同租户间，也不允许使用相同的网段，因为租户网络是通过ip+vlan的形式来区分，neutron 就完全可以。<br>但也不是nova-network 就一文不取，相比于neutron网络，虽说没有neutron那么多的功能插件，仅有bridge，但是其稳定性已得到大多数用户的验证，对于小规模的私有云(1千台虚机的规模)，nova-network是可以考虑的。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.mirantis.com/blog/openstack-networking-flatmanager-and-flatdhcpmanager/" target="_blank" rel="noopener">OpenStack Networking – FlatManager and FlatDHCPManager</a></p>
<p><a href="https://www.mirantis.com/blog/openstack-networking-vlanmanager/" target="_blank" rel="noopener">Openstack Networking for Scalability and Multi-tenancy with VlanManager</a></p>
<p><a href="https://github.com/gc3-uzh-ch/gridka-school/blob/master/tutorial/nova_network.rst" target="_blank" rel="noopener">Network service - easy version - nova-network</a></p>
<p><a href="https://docs.openstack.org/admin-guide/compute-networking-nova.html" target="_blank" rel="noopener">Networking with nova-network</a></p>
<p><a href="http://blog.csdn.net/beginning1126/article/details/41172365" target="_blank" rel="noopener">openstack 网络架构 nova-network + neutron</a></p>
<p><a href="https://wiki.openstack.org/wiki/UnderstandingFlatNetworking" target="_blank" rel="noopener">UnderstandingFlatNetworking</a></p>
<p><strong><em>本篇文章由<a href="https://zhangchenchen.github.io/">pekingzcc</a>采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可,转载请注明。</em></strong></p>
<p> <strong><em>END</em></strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="pekingzcc" />
          <p class="site-author-name" itemprop="name">pekingzcc</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">97</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">45</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/zhangchenchen" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">pekingzcc</span>
</div>


<div class="powered-by">
  powered by <a class="theme-link" href="https://hexo.io">Hexo</a> 
</div>

<div class="theme-info">
  theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'pekingzcc';
      var disqus_identifier = 'page/3/index.html';

      var disqus_title = "";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      

    </script>
  









  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  

  


</body>
</html>
