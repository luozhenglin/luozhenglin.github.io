<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Inf-- kafka 从入门到放弃]]></title>
    <url>%2F2018%2F06%2F03%2Fkafka-intro%2F</url>
    <content type="text"><![CDATA[概述工作中用到了kafka ,本篇博文整理一下学习到的kafka相关知识，内容多来自网上（主要是官网doc,个人博客），侵删。 kafka 是Linkedin 使用Scala 编写的一个开源的分布式消息系统，当时开发的初衷是为了解决网站活动流数据（比如网页浏览，点击，驻留时间等等）的数据转发处理。因为活动流数据是实时且流量比较大，可想而知kafka 的最大特点就是高吞吐率且容易扩展。kafka 的设计目标如下： 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。 同时支持离线数据处理和实时数据处理。 Scale out：支持在线水平扩展。 使用场景从不同的角度出发，kafka的作用也不同。 kafka作为一个消息系统，类比redis,rabbitmq等同类产品， kafka作为一个存储系统，因为进入kafak的消息都会被持久化所以可以作为一个存储系统， kafka作为一个流处理系统，对于流数据，可以在kafka里多次处理，多次流转。 相应的，kafka 的使用场景大致如下： 作为传统消息队列系统的替换。 做metric监控数据的收集处理。 做日志数据的收集处理。 流数据处理。 事件驱动架构的核心组件。 上图中可看出kafka的角色，相应的，kafka也提供了四种核心api: The Producer API allows an application to publish a stream of records to one or more Kafka topics. The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them. The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams. The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table. 整体架构与术语了解kafka，需要了解kafka 的一些术语,包括 producer,consumer,topic ,partition,broker等等。下面结合架构图统一解释： 首先kafka 有producer 和consumer的概念，一个生产数据，一个消费数据，中间是一个broker（也就是一个消息队列），producer是以push的方式向broker推数据，consumer以pull 的方式从broker消费数据。 producer 与consumer是通过topic 关联在一块的。topic类似于一个特定的分类。producer向某个指定的topic push数据，consumer从指定的topic消费数据。 partition 的引入是为了解决并发量的问题，一个topic （至少有一个partition）可以有多个partition，多个partition并发的处理一个topic的消息。每一个partition以顺序不变的方式保存消息。 broker 是kafka集群中维护发布消息的系统。每个broker针对每个topic可能包含0个或多个该topic的分区。假设，一个topic拥有N个分区，并且集群拥有N个broker,则每个broker会负责一个分区。 假设，一个topic拥有N个分区，并且集群拥有N+M个broker,则前N个broker每个处理一个分区，剩余的M个broker则不会处理任何分区 。 假设，一个topic拥有N个分区，并且集群拥有M个broker（M &lt; N），则这些分区会在所有的broker中进行均匀分配。每个broker可能会处理一个或多个分区。这种场景不推荐使用，因为会导致热点问题和负载不均衡问题。 Replicas of partition 分区副本仅仅是分区的备份，不会对副本分区进行读写操作，只是用来防止数据丢失。上图中topic2 就设置了 partition=2，副本会均衡地分布在broker中。 consumer group: 一个consumergroup 里面会有若干个consumer实例，对应一个topic，这几个consumer实例都会消费该topic 中的消息，如果consumer实例的数量等于对应topic的partition数量，那么一个consumer对应一个partition（推荐）。如果consumer实例的数量小于对应topic的partition数量，那么一个consumer可能对应多个partition。如果consumer实例的数量大于对应topic的partition数量，那么多出的consumer不会参与到topic的消息消费。 安装与使用参考quick start 原理 kafka 依赖zookeeper来实现负载均衡以及原数据的存取。 kafka之所以实现如此快的数据持久化，是因为磁盘io是顺序读写，在某些情况下，顺序磁盘访问能够比随即内存访问还要快（跟操作系统的预读，后写等技术有关）。 kafka 的consumer group 实现了consumer 的auto rebalance，之前是依赖zookeeper实现的，会出现脑裂的情况，后来专门开发了coordinator组件来实现，目前仍在不断改进。 kafka 的高性能有很多原因，包括消息的batch send,消息压缩（producer端和broker端都可做），ISR（in-sync replicas)机制，磁盘 append only，page cache等等。 参考文章kafka Documentation kafka入门教程 Kafka系列文章 分布式发布订阅消息系统 Kafka 架构设计 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kafka</tag>
        <tag>Inf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kuberbetes-- 部署生产级MySQL到Kubernetes集群中]]></title>
    <url>%2F2018%2F05%2F10%2Fmysql-on-k8s-in-production%2F</url>
    <content type="text"><![CDATA[概述K8s天然支持无状态应用的自动化运维（如HA,Scale等），对于有状态应用就相对比较麻烦了，本文以Mysql为例，详细梳理一下有状态应用在K8S中的部署运维。 Mysql HA 方案有状态应用在K8S中难以运维的原因是该应用本身就比较难运维（不然就不会有DBA 这样一个专门的职位了），当涉及到有状态应用时，你要考虑数据的存储，可用性，扩展性，事务，灾备等等。不过要想在K8S中部署生产级别的有状态应用，首先要知道在没有使用K8S时生产级别的应用架构方案。这里还是以Mysql为例，简单说下常用的几种生产环境中的Mysql 架构方案。本人不是专业的DBA,而且本身Mysql HA就是水很深的一个方向，理解有限，这里只谈大致方案，不说技术细节。Mysql HA架构有很多种，具体选型时要考虑架构的HA level，支撑应用的性质，具体的部署环境等。 这里根据Mysql DOC中的分类来具体说明（个人感觉这个分类也不是太精确），官网根据能达到的HA level分了三个层次，分别是： Data Replication. Clustered &amp; Virtualized Systems. Shared-Nothing, Geographically-Replicated Clusters. 这三个层次的可用性依次递增，不过复杂性也随之增加。 Data Replication这个级别的架构主要是 replication的方法实现mysql数据的复制冗余，具体方案有如下几种： MYSQL 主从或主主半同步复制 这种架构比较简单，使用原生半同步复制作为数据同步的依据,缺点是需要额外考虑haproxy、keepalived的高可用机制，而且完全依赖于半同步复制，如果半同步复制退化为异步复制，数据一致性无法得到保证，可以通过针对网络波动的半同步复制优化解决。 利用MHA 实现HAMHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案，它由日本DeNA公司youshimaton（现就职于Facebook公司）开发，是一套优秀的作为MySQL高可用性环境下故障切换和主从提升的高可用软件。它主要解决的是master fail-over的情况下实现秒级切换且保证数据一致性。 优点是操作非常简单，可以将任意slave提升为master，且MHA可以设定多个master,可用性提高，缺点是逻辑较为复杂，发生故障后排查问题，定位问题更加困难，且用perl开发，二次开发困难。除了MHA ,还有MMM(Master-Master replication managerfor Mysql，Mysql主主复制管理器)方案。 SAN共享存储实现HA 这种方案因为使用共享存储，不需要数据同步，不过专门的共享存储花销比较大，且需要专门的运维人员，适合一些土豪公司。 DRBD磁盘复制实现HADRBD是一种基于软件、基于网络的块复制存储解决方案，主要用于对服务器之间的磁盘、分区、逻辑卷等进行数据镜像，当用户将数据写入本地磁盘时，还会将数据发送到网络中另一台主机的磁盘上，这样的本地主机(主节点)与远程主机(备节点)的数据就可以保证实时同步。穷人版的SAN ,唯一不同的是没有使用SAN网络存储 ，而是使用local disk。由于是实时复制磁盘数据，性能会有影响。 Clustered &amp; Virtualized Systems大致有两种方案，一种是基于Mysql的NDB CLUSTER，另一种是MarianDB的Galera方案。 Mysql NDB CLUSTER 国内用NDB集群的公司非常少。NDB集群不需要依赖第三方组件，全部都使用官方组件，能保证数据的一致性，某个数据节点挂掉，其他数据节点依然可以提供服务，管理节点需要做冗余以防挂掉。缺点是：管理和配置都很复杂，而且某些SQL语句例如join语句需要避免。 MarianDB Galera Cluster MariaDB Galera Cluster 是一套在mysql innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到 各个节点上去。在数据方面完全兼容 MariaDB 和 MySQL。 Geographically-Replicated Clusters这种级别的Mysql集群是跨地域的数据中心，其实是上述解决方案的一种scale，但是规模更大，架构更加复杂，也没有一种统一的解决方案，这里不做讨论。 部署Mysql 到K8S集群To be continued 参考文章Best practices for MySQL High Availability Choosing the right MySQL High Availability Solution High Availability and Scalability 五大常见的MySQL高可用方案 mysql复制高可用方案总结 MySQL高可用各个技术的比较 用 Go 搭建 Kubernetes Operators mysql on kubernetes 容器化RDS：计算存储分离还是本地存储？ 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-- Docker storage driver 概述]]></title>
    <url>%2F2018%2F03%2F09%2Frecord-for-docker-storage-driver%2F</url>
    <content type="text"><![CDATA[概述Docker 配置的时候有一个很重要的配置项就是 storage driver选项，本篇博客详细介绍下storage driver这一配置项的相关内容。 背景首先是 storage driver出现的原因。我们知道容器的存储大致有两种，一种是在容器外的，比如 volume，不会随着容器的消亡而消失，有自己的生命周期。还有一种是容器内的，这种存储跟对应容器的生命周期是紧密结合在一起的。而我们要说的就是容器内的存储。本地的Docker引擎有一个Docker镜像层的缓存，镜像层是层层叠加的。当容器运行起来的时候就是在镜像层上起来的。基于同一个镜像运行的容器会共用一个镜像。那如何保证容器内操作后内容的独立呢，就要使用容器层以及写时复制（COW）技术。如下图： 最底层的基础镜像是ubuntu的系统镜像，再往上是分层的镜像层（比如dockerfile中的软件安装等），这些都是只读的镜像层。最上面才是可以读写的容器层，不同的容器有不同的容器层，共用相同的镜像层。当某个容器需要写操作时，会先将写的内容从镜像层复制到容器层，然后再写入（也就是写时复制），读的时候会从容器层开始，如果命中则读取，没有命中则依次往下读取。 至于以上原理的具体的实现，就是storage driver做的事。 storage driver 的种类以及选型注：本文是基于最新版Docker（V17.12），版本不同，选择也不同，具体参考官网。 目前为止，常用的storage driver有以下几种：AUFS，Btrfs，Device mapper，Overlayfs，ZFS，VFS。其中， VFS是接口的“原生”的实现，完全没有使用联合文件系统或者写时复制技术，而是将所有的分层依次拷贝到静态的子文件夹中，然后将最终结果挂载到容器的根文件系统。它并不适合实际或者生产环境使用，但是对于需要进行简单验证的场景，或者需要测试Docker引擎的其他部件的场景，是很有价值的。 Btrfs和ZFS针对特定的文件系统，也就是 backing filesystem必须相应的是Btrfs和ZFS。 AUFS和Overlayfs（包括Overlayfs2）都是在原有文件系统上基于联合挂载实现，而Device mapper是所有的镜像和容器存储在它自己的虚拟设备上，这些虚拟设备是一些支持写时复制策略的快照设备。 具体的选择策略要根据linux 发行版，docker版本以及文件系统来决定： 对于Docker 社区版本来说，不同linux发行版的选择如下： 对于不同的文件系统，推荐如下： 综上，选择时可以如下选择： 1，如果内核支持多个storage driver，可以如下考虑优先级：首先考虑特定文件系统的storage driver,即Btrfs和ZFS。否则，可以使用稳定和通用的配置，overlay2,或device mapper（需要手动配置direct-lvm，因为默认的loopback-lvm性能一般）。 2，依据具体的Docker 版本, 操作系统版本做选择（见上图）。 3，依据具体的 backing filesystem做选择（见上图）。 几种典型 storage driver的原理简单讲下AUFS，Overlayfs，Device mapper实现的具体原理。 AUFS 的实现原理AUFS是一种联合文件系统，意思是它将同一个主机下的不同目录堆叠起来(类似于栈)成为一个整体，对外提供统一的视图。AUFS是用联合挂载来做到这一点。 在Docker中，AUFS实现了镜像的分层。AUFS中的分支对应镜像中的层。 aufs中文件的读写:读的时候会先去container layer读，如果没有会继续往下层读取。写的时候也是，如果container layer文件没有，先去image layer复制文件到container layer,然后再写入。因为AUFS工作在文件的层次上，也就是说AUFS对文件的操作需要将整个文件复制到读写层内，哪怕只是文件的一小部分被改变，也需要复制整个文件。 aufs中文件的删除:AUFS通过在最顶层(container layer)生成一个whiteout文件来删除文件。whiteout文件会掩盖下面只读层相应文件的存在，但它事实上没有被删除。 Overlayfs的实现原理OverlayFS与AUFS相似，也是一种联合文件系统(union filesystem)，与AUFS相比，OverlayFS： 设计更简单，被加入Linux3.18版本内核 ，可能更快。 Overlay通过三个概念来实现它的文件系统：一个“下层目录（lower-dir）”，一个“上层目录（upper-dir）”，和一个做为文件系统合并视图的“合并（merged）”目录。受限于只有一个“下层目录”，需要额外的工作来让“下层目录”递归嵌套（下层目录自己又是另外一个overlay的联合），或者按照Docker的实现，将所有位于下层的内容都硬链接到“下层目录”中，这就可能导致inode爆炸式增长（因为有大量的分层内容和硬连接）。 Overlay2基于Linux内核4.0和以后版本中overlay的特性，可以允许有多个下层的目录，解决了一些因为最初驱动的设计而引发的inode耗尽和一些其他问题。不过由于代码库相对还比较年轻，有待时间的检验。 理论情况下，overlay2 和 overlay要比aufs 和 devicemapper性能好，甚至，一些情况下，overlay2要比btrfs好。不过也有一些需要注意的方面： OverlayFS 支持页缓存（page caching）共享。意味着多个使用同一文件的容器可以共享同一页缓存，这使得overlayfs具有很高的内存使用效率。 同aufs一样，第一次写文件时需要复制整个文件，这会带来一些性能开销，在修改大文件时尤其明显。 overlay的inode限制。 Device mapper的实现原理device mapper将所有的镜像和容器存储在它自己的虚拟设备上，这些虚拟设备是一些支持写时复制策略的快照设备。device mapper工作在块层次上而不是文件层次上，这意味着它的写时复制策略不需要拷贝整个文件。device mapper创建镜像的过程如下： 使用device mapper的storge driver创建一个精简配置池；精简配置池由块设备或稀疏文件创建。 接下来创建一个基础设备； 每个镜像和镜像层都是基础设备的快照；这写快照支持写时复制策略，这意味着它们起始都是空的，当有数据写入时才耗费空间。镜像的每一层都是它下面一层的快照，镜像最下面一层是存在于thin pool中的base device的快照。容器是创建容器的镜像的快照。device mapper跟之前的storage driver最大的不同就是它是基于块而不是基于文件，所以对文件的操作实际是对对应文件块的操作，默认每个块的大小为64KB。图展示了容器中的某个进程读取块号为0x44f的数据： device mapper不是最有效使用存储空间的storage driver，启动n个相同的容器就复制了n份文件在内存中，这对内存的影响很大。所以device mapper并不适合容器密度高的场景。 参考文章Docker storage drivers Docker用户指南(4) – 存储驱动选择 深入了解Docker存储驱动 Docker之几种storage-driver比较 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-- Hadoop学习之HDFS]]></title>
    <url>%2F2018%2F01%2F30%2Frecord-for-hdfs%2F</url>
    <content type="text"><![CDATA[概述最近因为项目需要将重心转移到大数据架构这一块，所以将大数据的内容大致过一下，在此记录，内容大部分来自官方文档与博客，主要基于Hadoop 的最新stable版本（2.9）。学习大数据，立马想到的就是Hadoop，Hadoop是一个开源的可依赖，可扩展的分布式计算框架，采用普通PC机集群完成对大量数据的存储，计算。这个项目主要包括四个模块： Hadoop Common: Hadoop的Common utilities模块。 Hadoop Distributed File System (HDFS): 一个支持高可用的分布式文件系统。 Hadoop YARN: 任务调度和资源管理框架。 Hadoop MapReduce: 基于YARN的对大量数据并行处理的计算模型。除了以上四个主要模块之外，Hadoop生态还有很多其他的的系统，以后再慢慢写，这里先写Hadoop生态中最重要的一个组件HDFS。 先看下Hadoop里的服务角色： Hadoop主要的任务部署分为3个部分，分别是：Client机器，主节点和从节点。主节点主要负责Hadoop两个关键功能模块HDFS、Map Reduce的监督。Job Tracker使用Map Reduce进行监控和调度数据的并行处理，namenode则负责HDFS监视和调度。从节点负责了机器运行的绝大部分，担当所有数据储存和指令计算的苦差。每个从节点既扮演着数据节点的角色又充当与他们主节点通信的守护进程。守护进程隶属于Job Tracker，数据节点则归属于名称节点。Client负责把数据加载到集群中，递交给Map Reduce做数据处理工作，并在工作结束后取回或者查看结果。 再看下典型的围绕hadoop 的workflow： HDFS Architecture架构HDFS的整体设计架构就是master/slave，namenode负责元数据的存取，数据管理等功能，datanode负责数据存储。如下： datanode工作原理数据的存储方式与ceph类似，默认都是三副本，先将大数据文件切割成固定大小（比如128M）的block，然后将这些block存放到三个不同的datanode中，namenode会记录对应文件block以及block所在位置。举例一个datanode存储过程:client先做文件切割，并提交存储文件命令给namenode，namenode有一个rack awareness的功能，简单点说就是会将数据存储到不同机架上以避免机架故障（电源故障等），如果是三副本的话，首先client会写入block到某一节点A，然后另一机架中的节点B 会从A复制该数据,再然后同一机架内的另一个节点C会从B复制一份数据。这样一个pipeline既保证了数据的容灾，也能减小数据传输的延迟。 namenode工作原理理解namenode，首先要理解两个文件。一个是 Edits文件，一个是FsImage映像文件，这两个文件就包含整个HDFS集群的元数据，而namenode的最大任务就是维护这两个文件。 Edits文件：NameNode在本地操作系统的文件都会保存在Edits日志文件中。也就是说当文件系统中的任何元数据产生操作时，都会记录在Edits日志文件中。eg：在HDFS上创建一个文件，NameNode就会在Edits中插入一条记录。同样如果修改或者删除等操作，也会在Edits日志文件中新增一条数据。 FsImage映像文件：整个文件系统的名字空间，包括数据块到文件的映射，文件的属性等等，都存储在一个称为FsImage的文件中，这个文件也是放在NameNode所在的文件系统中。（注意到该文件中并没有blockmap,描述数据块Block与DataNode节点之间的对应关系的文件，这是因为每个DataNode已经持有属于自己管理的Block集合，每次blockreport都会将所有DataNode的Block集合汇总后即可构造出完整BlocksMap，所以不用持久化。） 为什么会引入这两个文件呢，因为在HDFS的整个运行期里，所有元数据均在NameNode的内存集中管理，但是由于内存易失特性，一旦出现进程退出、宕机等异常情况，所有元数据都会丢失，为了更好的容错能力，NameNode会周期进行Checkpoint，将其中的一部分元数据（文件系统的目录树Namespace）刷到持久化设备上，即二进制文件FSImage，这样的话即使NameNode出现异常也能从持久化设备上恢复元数据。但是仅周期进行Checkpoint仍然无法保证所有数据的可靠，如前次Checkpoint之后写入的数据依然存在丢失的问题，所以将两次Checkpoint之间对Namespace写操作实时写入EditLog文件，通过这种方式可以保证HDFS元数据的绝对安全可靠。 首先看一下namenode启动时做了哪些操作（下文部分直接引用自该博客）： 接下来看看check point的时候做了哪些操作,这个时候就要引入Secondary NameNode了，NameNode主要是存储文件的metadata，运行时所有数据都保存在内存中，这个的HDFS可存储的文件受限于NameNode的内存。而Secondary NameNode可以看做是NameNode的灾备（并非HA），它会定时与NameNode进行同步，定期的将fsimage映像文件和Edits日志文件进行合并，并将合并后的传入给NameNode，替换其镜像，并清空编辑日志。如果NameNode失效，需要手动的将其设置成namenode主机。checkpoint的时间默认是3600秒（可配置），当Edits日志文件超过最大值时也会进行check point。checkpoint大致如下： NameNode通知Secondary NameNode进行checkpoint。 Secondary NameNode通知NameNode切换edits日志文件，使用一个空的。 Secondary NameNode通过Http获取NmaeNode上的fsimage映像文件和切换前的edits日志文件。 Secondary NameNode在内容中合并fsimage和Edits文件。 Secondary NameNode将合并之后的fsimage文件发送给NameNode。 NameNode用Secondary NameNode 传来的fsImage文件替换原先的fsImage文件 HDFS 使用常用操作HDFS本质是一个文件系统，所以跟Linux 的文件系统使用类似，也是增删改查，不过这里的“改”不是update，而是truncate，也就是文件截断，HDFS 不支持update操作，这与它读多写少的特性相对应。对HDFS的操作可以通过shell，web 界面，libhdfs (C API)或WebHDFS (REST API)来操作。HDFS里的文件跟linux 文件类似，也有own user，group，也有读写执行权限的划分，不过这里的可执行权限只对目录有用，意思是是否有权限对该目录的子目录或文件有可读权限。每一个文件的操作命令都会进行Permission Checks，不通过则fail。HDFS的具体shell命令略，可参考FileSystemShell。 Quotas 设置HDFS支持 administrator对quota的设置，包括： name quota: 指定目录下文件或者目录的数量限制。 space quota: 指定目录下文件的占用空间限制。 Storage Type Quotas：指定目录下Storage Type的限制。 administrator可以通过命令行或其他的方式进行设置。 透明加密透明加密主要是防止application与HDFS之间进行端到端的数据传输时的数据安全，不需要更改user application的任何代码。详细参考TransparentEncryption。 HDFS HA这里的HA主要是指namenode的HA。hadoop文档提供了两种方法，本质其实是一样的，因为namenode主要依靠FsImage和EditLog两个文件管理DataNode的数据，要想保证新的namenode能随时替换，就要保证这两个文件的一致性，FsImage是存储在磁盘上还好说，EditLog在内存里随时变化，就要保证两个namenode的EditLog文件是一样的。两个方案如下： QJM：the Quorum Journal Manager，这种方案是通过JournalNode共享EditLog的数据，使用的是Paxos算法（zookeeper就是使用的这种算法），保证活跃的NameNode与备份的NameNode之间EditLog日志一致,配合zookeeper可以实现自动切换,推荐使用。 NFS：Network File System 或 Conventional Shared Storage，传统共享存储，其实就是在服务器挂载一个网络存储（比如NAS），Active NameNode将EditLog的变化写到NFS，备份NameNode检查到修改就读取过来，是两个NameNode数据一致,缺点是如果namenode或者standby namenode与NFS磁盘之间的网络出了问题，HA即失效。 以上是QJM HA的典型的结构图。集群中共有两个namenode(简称NN)，其中只有一个是active状态，另一个是standby状态。active 的NN负责响应DN(datanode)的请求，为了最快的切换为active状态，standby状态的NN同样也连接到所有的datenode上获取最新的块信息(blockmap)。active NN会把元数据的修改(edit log)发送到多数的journal节点上(2n+1个journal节点，至少写到n+1个上)，standby NN从journal节点上读取edit log，并实时的合并到自己的namespace中。另外standby NN连接所有DN，实时的获取最新的blockmap。这样，一旦active的NN出现故障，standby NN可以立即切换为active NN. 具体配置参考HADOOP(二):HDFS 高可用原理 HDFS 认证与授权为了确保数据安全，认证授权这一块hadoop也下了一番功夫，一般来说，需要经历一下几个阶段： 认证 proxy user service level Authorization 第三方权限控制Ranger Hadoop POSIX ACLs 认证部分有两种方式，simple和kerberos，simple不做任何处理，会由操作系统层获取用户，客户端可以通过设置环境变量HADOOP_USER_NAME来伪装用户。kerberos认证对集群里的所有机器都分发了keytab，使得集群机器进程之间不能随便访问。代理部分：当客户端访问hadoop时，并不想以当前进程用户去调用，上层应用一般有自己一套用户管理体系，所以hadoop提供代理机制，让进程用户可以代理登录用户提交请求。然而，如果对代理用户不加以控制的话，那权限便相对于无限放大，比如代理超级用户：hdfs，yarn等，所以对于进程用户会设置可在哪些主机提交请求和代理哪些用户组成员。权限控制首先经过Service Level Authorization，检测服务级别权限。比如哪些用户可以连接namenode，resourcemanager，属于服务级别的acl控制。参考Hadoop服务层授权控制接着由Ranger进行目录，队列等资源的权限管控, 属于更细粒度的权限控制。如果ranger没有策略控制，则进入原始HDFS文件系统权限或者MR权限控制。 参考文章Welcome to Apache Hadoop HDFS Users Guide 深入理解Hadoop集群和网络 Hadoop2.7.3 HA高可靠性集群搭建 Hadoop之HDFS分布式文件系统NameNode及Secondary NameNode详解 Hadoop 访问管理 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Bigdata</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bigdata-- Centos7 安装CDH5.14记录]]></title>
    <url>%2F2018%2F01%2F26%2Fcentos7-install-cdh5.14%2F</url>
    <content type="text"><![CDATA[概述目标是搭建一个CDH 的测试环境，图方便在网上找了一些中文的搭建博客，结果问题百出，还是回到官网浏览，找到对应的安装文档，花了一点时间搭建，这里记录一下。系统配置如下： centos7.2 minimal,8G内存，100G磁盘(master节点) centos7.2 minimal,4G内存，100G磁盘(node节点) centos7.2 minimal,4G内存，100G磁盘(node节点) centos7.2 minimal,4G内存，100G磁盘(node节点)搭建的cdh为最新的CDH5.14，感觉master内存还是太少，有点吃力。 搭建步骤CDH的搭建有多种方法，一般来说，都是先搭建Cloudera Manager,然后利用Cloudera Manager搭建CDH，如果是测试环境，可以直接利用Cloudera Manager自动化安装，这种方式使用内嵌的PostgreSQL作为metadata等数据的存储，不适于生产环境。生产环境中一般会使用Mysql或其他独立搭建的数据库（当然要做HA），所以我们会先搭建一个Mysql数据库备用。 准备工作在所有节点上都要执行的工作，包括host设置，设置yum repo，无密钥登录，ntp设置，安装jdk,关闭防火墙等等。 修改 hostname，执行命令 hostname NAME,并修改 /etc/hostname文件。 修改/etc/hosts文件 123410.10.10.77 cdh110.10.10.78 cdh210.10.10.79 cdh310.10.10.82 cdh4 关闭防火墙. 123456789$ systemctl stop firewalld$ systemctl disable firewalld``` - 所有节点设置无密钥登录,命令大致如下，将最终的authorized_keys再覆盖回所有节点：```bash$ ssh-keygen -t rsa$ cat id_rsa.pub &gt;&gt; authorized_keys$ chmod 600 authorized_keys$ scp authorized_keys root@cdh2:~/.ssh/ 设置 yum repo. 12$ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 阿里yum源$ curl -o /etc/yum.repos.d/cloudera-manager.repo https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/cloudera-manager.repo # cloudera yum源 ntp设置，安装ntp，编辑crontab定时同步。 1234$ yum install ntpd$ ntpdate time1.aliyun.com$ crontab e30 02 * * * ntpdate time1.aliyun.com 安装jdk. 1yum install oracle-j2sdk1.7 -y 关闭SElinux,修改/etc/selinux/config为disabled，重启生效。 12345678910# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - SELinux is fully disabled.SELINUX=disabled# SELINUXTYPE= type of policy in use. Possible values are:# targeted - Only targeted network daemons are protected.# strict - Full SELinux protection.SELINUXTYPE=targeted master节点安装与配置Mysql 安装mysql. 12345$ wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm$ sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm$ yum update$ sudo yum install mysql-server$ sudo systemctl start mysqld 删除/var/lib/mysql/ib_logfile0 和 /var/lib/mysql/ib_logfile1文件，并配置/etc/my.conf如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[mysqld]transaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:# symbolic-links = 0key_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 550#expire_logs_days = 10#max_binlog_size = 100M#log_bin should be on a disk with enough free space. Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your system#and chown the specified folder to the mysql user.log_bin=/var/lib/mysql/mysql_binary_log# For MySQL version 5.1.8 or later. For older versions, reference MySQL documentation for configuration help.binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidsql_mode=STRICT_ALL_TABLES 安装 MySQL JDBC Driver从该页面下载对应tar包，解压。 123$ tar zxvf mysql-connector-java-5.1.31.tar.gz$ mkdir -p /usr/share/java/$ cp mysql-connector-java-5.1.31/mysql-connector-java-5.1.31-bin.jar /usr/share/java/mysql-connector-java.jar 创建对应的数据库。 12345mysql&gt; create database database DEFAULT CHARACTER SET utf8;Query OK, 1 row affected (0.00 sec)mysql&gt; grant all on database.* TO 'user'@'%' IDENTIFIED BY 'password';Query OK, 0 rows affected (0.00 sec) 数据库的名字，用户名等可以参考除了以下列出的，还需创建oozie的数据库以及用户名密码. Cloudera Manager 安装 在master节点安装Cloudera Manager Server并启动。 12$ yum install cloudera-manager-daemons cloudera-manager-server$ systemctl start cloudera-scm-server 在master和node节点安装Cloudera Manager Agent。修改 /etc/cloudera-scm-agent/config.ini 中的server_host为master的IP。 12$ yum install cloudera-manager-agent cloudera-manager-daemons$ systemctl start cloudera-scm-agent CDH 安装进入 Cloudera Manager的console，http://Server host:7180,登录后便可以进入CDH的安装部署了。 该过程没有截图，且因为web界面看着比较直白不在赘述。 参考文章Installation Path B - Installation Using Cloudera Manager Parcels or Packages Connect Hue to MySQL or MariaDB Troubleshooting Installation and Upgrade Problems 离线部署 Cloudera Manager 5 和 CDH 5.12.1 及使用 CDH 部署 Hadoop 集群服务 Centos 7安装CDH 5.13.0总结 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Ops</tag>
        <tag>Bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ops-- 将Flask APP 打包为rpm包]]></title>
    <url>%2F2018%2F01%2F20%2Fcreate-rpm-for-python-flask-application%2F</url>
    <content type="text"><![CDATA[概述为了实现无人值守安装，需要将我们用Python研发的一个WEB做成一个RPM 包，这里简单记录一下。先简单介绍下需要打包的这个Python应用，是我们为了安装CDH 大数据平台做的一个辅助WEB，使用Flask开发，除了用cherry起的一个WEB 服务外，还有一个celery的worker 进程。 构建思路 首先是该Python应用的依赖包，有两种解决方案，一种是全部打包到一个RPM包里，简单粗暴，不容易起冲突，但是不灵活，文件大，每做一次升级比较麻烦，第二种方案是将该应用的依赖包做成RPM包（其实都有现成的），在SPEC文件中注明依赖包，这样在RPM 安装时会自动安装依赖包，这种方案比较灵活，不过需要花时间去找到所有依赖包的RPM 包并放到YUM REPO中。我们采用的是第二种方案。 因为要起两个服务，所以要写两个启动文件。 Python 的setuptools 有一个 python setup.py bdist_rpm 命令用于构建rpm包，不过还是需要自己定制SPEC文件，所以这里直接使用rpmbuild。 本次构建的系统环境是centos6.9。 具体实践 安装rpmbuild 1$ yum install -y rpm-build 使用普通用户并修改topdir目录 123456$ useradd rpmbuilder$ su - rpmbuilder$ vim ~/.rpmmacros # 修改工作目录 %_topdir /home/rpmbuilder/rpmbuild$ mkdir -pv ~/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125; 简单介绍下工作目录中的几个目录的作用： 准备源码文件，主要包括三个文件，一个是Flask源码文件全部打包压缩成一个tar.gz包，还有两个启动文件。将这三个文件放到SOURCES目录。启动文件示例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475. /etc/rc.d/init.d/functionsrunuser=rootprog=cdhboot-webworker_num=10export C_FORCE_ROOT=Trueexec="/opt/cdhboot/supervisor/server_cherrypy.py" # 我们将所有源码文件安装到/opt/cdhboot/目录，后面会提到。pidfile="/var/run/cdhboot/$prog.pid"start() &#123; [ -f $exec ] || exit 5 echo -n $"Starting $prog: " daemon --user $runuser --pidfile $pidfile "python $exec &amp;&gt;/dev/null &amp; echo \$! &gt; $pidfile" retval=$? echo return $retval&#125;stop() &#123; echo -n $"Stopping $prog: " killproc -p $pidfile $prog retval=$? echo return $retval&#125;restart() &#123; stop start&#125;reload() &#123; restart&#125;force_reload() &#123; restart&#125;rh_status() &#123; status -p $pidfile $prog&#125;rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125;case "$1" in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status condrestart|try-restart) rh_status_q || exit 0 restart ;; *) echo $"Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload&#125;" exit 2esacexit $? 编写SPEC文件，SPEC文件有几个部分组成，也代表着rpm打包时的几个步骤。先看下rpm打包的四个步骤： 对应的SPEC文件示例如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# 第一部分：自定义的变量，通常是包名，版本等信息%define name cdhboot%define version 0.1%define unmangled_version 0.1%define unmangled_version 0.1%define release 1# 第二部分：定义rpm包的信息，三个源文件，依赖包，以及BuildRootSummary: cdh bootName: %&#123;name&#125;Version: %&#123;version&#125;Release: %&#123;release&#125;Source0: %&#123;name&#125;-%&#123;unmangled_version&#125;.tar.gz Source1: cdhboot-workerSource2: cdhboot-webLicense: MITGroup: Development/LibrariesBuildRoot: /root/rpmbuild/ Prefix: %&#123;_prefix&#125;BuildArch: noarchVendor: pekingzcc &lt;pekingzcc@gmail.com&gt;Url: https://github.com/zhangchenchenRequires: python-celery,python-mongoengine,python-prettytable,python-cherrypy,python-argparse,pytz,python-flask,python-flask-login # 依赖包，使用yum安装时会先下载安装依赖包%descriptionWEB TO BOOT CDH# 第三部分：准备阶段，%setup是一个宏命令，解压缩包到cdhboot目录，并cd到该目录下。%prep%setup -n %&#123;name&#125;-%&#123;unmangled_version&#125; -n cdhboot# 第四部分：安装之前的操作，添加一个sysadmin用户%preif [ $1 == 1 ];then /usr/sbin/useradd sysadmin -s /sbin/nologin 2&gt; /dev/nullfi# 第五部分：安装阶段，创建相应目录。并将源文件复制到相应目录。%&#123;__install&#125;是一个宏命令，类似于cp命令。%installmkdir -p %&#123;buildroot&#125;/opt/cdhbootcp -r ./* %&#123;buildroot&#125;/opt/cdhboot/mkdir -p %&#123;buildroot&#125;/var/run/cdhbootmkdir -p %&#123;buildroot&#125;/var/log/cdhboot%&#123;__install&#125; -p -D -m 0755 %&#123;SOURCE1&#125; %&#123;buildroot&#125;/etc/rc.d/init.d/cdhboot-worker%&#123;__install&#125; -p -D -m 0755 %&#123;SOURCE2&#125; %&#123;buildroot&#125;/etc/rc.d/init.d/cdhboot-web# 第六部分：安转之后的操作，加入开机启动服务%postif [ $1 == 1 ];then /sbin/chkconfig --add cdhboot-worker /sbin/chkconfig cdhboot-worker on /sbin/chkconfig --add cdhboot-web /sbin/chkconfig cdhboot-web onfi# 第七部分：卸载之后的操作，删除sysadmin用户，并停止服务 %preunif [ $1 == 0 ];then /usr/sbin/userdel sysadmin 2&gt; /dev/null /etc/init.d/cdhboot-worker stop &gt; /dev/null 2&gt;&amp;1 /etc/init.d/cdhboot-web stop &gt; /dev/null 2&gt;&amp;1fi# 第八部分：构建完成后删除临时构建目录内容%cleanrm -rf $RPM_BUILD_ROOT# 第九部分：文件部分，但凡上文构建过程中出现的文件或目录，这里都要对这些文件或目录注明属性%files %defattr(-,root,root)%attr(0755,root,root) /etc/rc.d/init.d/cdhboot-worker%attr(0755,root,root) /etc/rc.d/init.d/cdhboot-web/opt/var/run/var/log 测试SPEC文件。为了测试SPEC文件，我们可以分阶段的执行构建命令。12345rpmbuild -bp cdhboot.spec 制作到%prep段rpmbuild -bc cdhboot.spec 制作到%build段rpmbuild -bi cdhboot.spec 执行 spec 文件的 "%install" 阶段 (在执行了 %prep 和 %build 阶段之后)。这通常等价于执行了一次 "make install"rpmbuild -bb cdhboot.spec 制作二进制包rpmbuild -ba cdhboot.spec 表示既制作二进制包又制作src格式包,即全过程。 通过分阶段的构建来测试对应部分的编写正确性，同时，也可以去临时目录BUILDROOT 下查看构建的文件，临时目录BUILDROOT在构建阶段相当于安装时机器的根目录。构建完成后再RPMS目录下可以看到构建成功的rpm包。 之后可以对rpm包进行签名，就可以放到yum repo中发布了。 参考文章How to create a rpm for python application 使用rpm-build制作nginx的rpm包 记录自己将Python程序打包成rpm包的过程 rpmbuild spec文件的编写,以及rpm包的打包 How to create an RPM package/zh-cn 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Ops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ops-- CDH大数据平台自动化安装部署]]></title>
    <url>%2F2018%2F01%2F04%2Fauto-install-big-data-platform%2F</url>
    <content type="text"><![CDATA[概述Cloudera 的CDH应该是目前大数据平台安装部署的首选，不过Cloudermanager只是简化了大数据组件的安装部署，其他的比如操作系统的安装，磁盘raid，seed节点配置，免密登录，ntp安装等都是需要手动或脚本完成，所以这里为了安装部署的方便，便把这一系列过程自动化，实现CDH大数据平台的自动化安装。 使用的工具有： KickStart Cobbler SaltStalk CdhBoot(自研的基于saltstalk的web 平台) 安装流程解析大致有以下几个过程： 磁盘raid（可选） seed节点操作系统安装 node节点操作系统安装 cdh准备安装工作 cdh安装 现在就按这几个顺序大致记录下整过程所需做的工作。 磁盘raid规划如下： datenode 系统分区用两块磁盘做raid1。 datenode的数据存储分区用单独一块磁盘做raid0（可选，单独一块磁盘不做raid也是可以的） namenode 系统分区用两块磁盘做raid1。 namenode 数据存储分区做raid5。 磁盘raid这一块因为服务器硬件的不同，做raid方式也不同，博主一直没能找到一个比较通用的自动化方式，所以最终只能手动解决。 seed节点操作系统安装这一部分的工作主要是seed节点操作系统的制作，大致步骤如下： 安装镜像制作软件包（createrepo &amp; mkisofs） 拷贝系统镜像文件到指定目录 编写Kickstart 文件 生成依赖关系和ISO文件 具体操作可以参考基于Kickstart自动化安装CentOS实践 这里注意： 因为是离线环境，所以这里要把所需要安装的所有rpm包全部放到镜像Packages目录里。 ks文件的编写，可以先手动安装一个系统，然后在安装完成的root目录下会生成一个anaconda-ks.cfg，该文件记录了你手动安装时的操作，然后根据这个文件去更改。这里有一个示例Centos-KickStart-Example。 seed节点安装系统后，需要手动配置一下seed节点的IP ，这里也可以写入ks文件，但相对不灵活，所以还是建议单独配置。 node节点操作系统安装该部分就是要用cobbler去推操作系统，需要我们在seed节点安装cobbler，导入镜像，然后配置待安装node节点的mac地址，最后推系统。为了完成自动化，我们是这样做的： 在seed节点安装的ks文件中，我们在post部分加入一个脚本或者salt命令，在seed节点安装并启动CdhBoot。 Cdhboot的web页面配置node mac地址，同时，在后端接收到请求后会调用salt命令进行cobbler的安装部署以及image的导入。 这里解释下为什么不把cobbler的安装配置放在ks文件中，因为cobbler依赖我们配置的网卡IP，如果那个网卡没起来，cobbler就会报错，所以我们把cobbler的部署放在配置ip 后。注意，利用cobbler推送的镜像中也要编写相应的ks文件。 cdh准备安装工作该部分主要是Cdhboot 调用salt api 完成免密登录，ntp安装，yum源设定等等准备工作 cloudera-manager安装同上，这部分也是Cdhboot 调用salt api 完成。 这样，我们的工作大致完成，完成之后，大数据平台的安装步骤如下： 各节点做raid 安装seed节点,配置IP 在Cdhboot 界面完成node操作系统安装，cdh准备工作，cloudera-manager安装。 在cloudera-manager界面完成cdh安装。 参考文章基于Kickstart自动化安装CentOS实践 Linux Kickstart 自动安装 SaltStack事件驱动(4) – event reactor namenode datanode 是否有必要做 raid 解决PXE批量安装Linux系统时kickstart自动识别硬盘名称的问题 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Ops</tag>
        <tag>Cobbler</tag>
        <tag>SaltStalk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BigData-- 大数据常用开源组件一览]]></title>
    <url>%2F2017%2F12%2F25%2Fbig-data-architeucture%2F</url>
    <content type="text"><![CDATA[概述最近有一个项目是跟公司的大数据平台有关，这里梳理一下常用的大数据组件，做个记录，同时，还有一个原因，就是希望用比较简洁的语言让我女朋友能对大数据的整体架构有个大致了解。大数据平台的架构不一而足，市面上的开源工具非常多，所选用的组件也是因人而异，不过都是为了具体业务应用场景而服务，这里选取一些常用的开源组件,试图从大数据处理的整个流程出发，将各个开源组件的功能特点，适用场景等讲清楚，个人观点，难免纰漏。 大数据处理流程在介绍大数据组件之前，先把大数据的整体处理流程梳理一下，因为要想了解整个大数据组件体系，首先得清楚我们要面临的问题，了解了问题才能去找相应的解决方案，如果只是一猛子扎进hadoop 体系里，反而会越看越迷糊。大数据的处理流程大致包括数据收集，数据存储，数据计算与数据分析这四个阶段，其实数据计算与分析算是一个阶段，这里为了以后区分组件的方便将其分开，数据计算可以理解为计算引擎那部分，数据分析就是更上层的BI分析工具等等。接下来就从这四个流程出发，简单介绍下常用工具。 数据收集在大数据还未兴盛之前，我们一般是将数据存储到关系型数据库（Mysql等），或者文本文件（日志文件等）中，这种方法在数据量不大的情况下是可以的，但在数据量达到TB，甚至PB级别，这种情况就不适用了。为了能够存储这种量级的数据，大数据存储组件应运而生，这里先暂时不说大数据存储，先说数据收集。出现了大数据存储组件之后，有一个问题就是要将数据收集到大数据存储组件中，于是，相应的组件也就出现了： 以下是收集组件： Sqoop：Sqoop是一个工具，用来在关系型数据库和Hadoop之间传输数据。你可以使用Sqoop来从RDBMS(MySQL or Oracle)导入数据到Hadoop环境里，或者通过MapReduce转换数据，把数据导回到RDBMS。 Flume： Flume 是Cloudera提供的一个高可用的、高可靠的、分布式的海量日志采集、聚合和传输的系统。Flume支持在日志系统中定制各类数据发送方，用于收集数据。同时，Flume支持对数据进行简单处理，并写入各种数据接受方（可定制）。 Logstash：也是一个应用程序日志、事件的传输、处理、管理和搜索的平台。可以用它来统一对应用程序日志进行收集管理，提供了Web接口用于查询和统计。 除了收集之外，因为巨大的IO压力，我们通常会在收集组件与数据存储组件之间加一层消息队列用于削峰填谷，降低IO压力，常用的组件包括： Kafka 是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模网站中的所有动作流数据，目前已成为大数据系统在异步和分布式消息之间的最佳选择。 RabbitMQ 是一个受欢迎的消息代理系统，通常用于应用程序之间或者程序的不同组件之间通过消息来进行集成。RabbitMQ提供可靠的应用消息发送、易于使用、支持所有主流操作系统、支持大量开发者平台。 ActiveMQ 是Apache出品，号称“最流行的，最强大”的开源消息集成模式服务器。ActiveMQ特点是速度快，支持多种跨语言的客户端和协议，其企业集成模式和许多先进的功能易于使用，是一个完全支持JMS1.1和J2EE 1.4规范的JMS Provider实现。 MQ组件的对比可以参考阿里中间件团队做的压测RocketMQ与kafka对比（18项差异）,Kafka、RabbitMQ、RocketMQ消息中间件的对比 —— 消息发送性能 数据存储数据存储这块分为两个部分，一部分是底层的文件系统，还有一部分就是之上的数据库或数据仓库。 文件系统大数据文件系统其实是大数据平台架构最为基础的组件，其他的组件或多或少都会依赖这个基础组件， 目前应用最为广泛的大数据存储文件系统非Hadoop 的HDFS莫属，除此之外，简单介绍下号称可以取代HDFS的Ceph。 HDFS：HDFS是一个高度容错性（多副本，自恢复）的分布式文件系统，能提供高吞吐量的数据访问，非常适合大规模数据集上的访问，不支持低延迟数据访问，不支持多用户写入、任意修改文件。HDFS是Hadoop 大数据工具栈里最基础有也是最重要的一个组件，基于Google的GFS开发。 Ceph：Ceph是一个符合POSIX、开源的分布式存储系统。最早是加州大学圣克鲁兹分校（USSC）博士生 Sage Weil 博士期间的一项有关存储系统的研究项目，Ceph的主要目标是设计成基于POSIX的没有单点故障的分布式文件系统，使数据能容错和无缝的复制。真正让ceph叱咤风云的是开源云计算解决方案Openstack，Openstack+Ceph的方案已被业界广泛使用。 数据库或数据仓库针对大数据的数据库大部分是NOSQL数据库，这里顺便澄清一下，NOSQL的真正意义是“ not only sql”，并非NOSQL是RMDB的对立面。 Hbase：是一个开源的面向列的非关系型分布式数据库（NoSQL），它参考了谷歌的BigTable建模，实现的编程语言为Java。它是Apache软件基金会的Hadoop项目的一部分，运行于HDFS文件系统之上，为 Hadoop 提供类似于BigTable规模的服务。因此，它可以容错地存储海量稀疏的数据。 MongoDB：一个基于分布式文件存储的数据库，面向文件，旨在为web应用提供可扩展的高性能数据存储解决方案。介于关系数据库和非关系数据库之间的开源产品，是非关系数据库当中功能最丰富、最像关系数据库的产品。 Cassandra：是一个混合型的非关系的数据库，类似于Google的BigTable，由Facebook开发。 Neo4j：一个高性能的，NOSQL图形数据库，它将结构化数据存储在网络上而不是表中。 数据计算简单介绍以下目前比较流行的几种大数据计算框架： MapReduce：最为知名的当属MapReduce，MapReduce属于一种批处理计算框架，借助于HDFS，基于磁盘进行数据计算，MapReduce的容错能力超强，适合处理巨大规模集群（几百上千个节点）下长时间非实时的大计算任务；但其实时性较差。Hadoop系列的Hive,Pig等数据仓库都是基于MapReduce做的。 Spark：是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性,同时保证了高容错性和高可伸缩性,允许用户将Spark部署在大量的廉价硬件之上,形成集群。Spark是MapReduce的替代方案，而且兼容HDFS等分布式存储层，可融入Hadoop的生态系统，以弥补缺失MapReduce的不足。适合迭代计算（常见于machine learning领域，比如PageRank）和交互式计算（data mining领域，比如SQL查询）。 Storm：典型的流计算系统，会对随时进入系统的数据进行计算,近实时处理需求的任务很适合使用流处理模式,适于处理必须对变动或峰值做出响应，并且关注一段时间内变化趋势的数据，类似框架还有 spark streaming。 除了计算框架外，因为是分布式系统，我们还需要对计算资源进行分配调度，以及各种服务间的协调，发现，配置管理等，所以这里又出现了两个重要的组件： Yarn：Hadoop 2.0中推出的非常重要的资源管理框架，负责集群资源管理和调度，MapReduce就是运行在YARN上的离线处理框架。 Zookeeper：是Google的Chubby一个开源的实现，是Apache软件基金会的一个软件项目，他为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。 两者的区别是：Yarn是resource management，解决的问题是怎样提高整个集群的资源利用率。Zookeeper是 Coordination，解决的是集群中各种服务的发现，同步，协调配合以保持整个集群的稳定。 数据分析数据分析的工具就更多了，这里列举一些业界用的比较多的，相对成熟的工具： Hive,Pig：两者都是Hadoop开源的分析工具，将这两个工具放在一起的原因是，他们底层都是基于MapReduce实现的，不过Hive是采用SQL的形式调用，而Pig是用脚本的形式，因为SQL的便利易用，Hive已逐渐取代Pig。 Impala： Impala是Cloudera开发的一款用来进行大数据实时查询分析的开源工具，它能够实现通过SQL风格来操作数据，Impala没有再使用缓慢的 Hive+MapReduce批处理，而是通过使用与商用并行关系数据库中类似的分布式查询引擎（由Query Planner、Query Coordinator和Query Exec Engine三部分组成），可以直接从HDFS或HBase中用SELECT、JOIN和统计函数查询数据，从而大大降低了延迟。 Presto：随着数据越来越多，使用Hive进行一个简单的数据查询可能要花费几分到几小时，显然不能满足交互式查询的需求，Facebook开发人员便开发了Presto，Presto依赖于Hive meta，摒弃了MapReduce方法，通过使用分布式查询，可以快速高效的完成海量数据的查询。 Spark：这里又提了Spark是因为Spark的应用越来越广泛，而且Spark 集成了很多机器学习的框架，可以很方便的调用。 Kylin：Apache Kylin是一个开源的分布式分析引擎，提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc. 开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 整体框架以上是按照数据处理的流程来区分的，这里从网上找到一张由下而上的整体架构： 参考文章Big data architecture - Introduction Exploring Big Data Solutions: When To Use Hadoop vs In-Memory vs MPP Hadoop大数据平台架构与实践 | hadoop概述与安装 大数据领域常用的技术、框架 Hadoop 分布式文件系统 - HDFS 链家网大数据平台建设，平台枢纽——工具链 美团大数据平台架构实践 大数据开源组件图谱 大数据框架对比：Hadoop、Storm、Samza、Spark和Flink 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kuberbetes-- 利用Jenkins在Kubernetes中实践CI/CD]]></title>
    <url>%2F2017%2F12%2F17%2Fachieve-cicd-in-kubernetes-with-jenkins%2F</url>
    <content type="text"><![CDATA[概述本文利用jenkins在k8s中简单实践了一下CI/CD，部分实验内容来自Set Up a CI/CD Pipeline with Kubernetes ，除此外，还试验了一把利用jenkins kubernetes plugin实现动态分配资源构建。 在kubernetes中简单实践jenkins首先简单介绍下jenkins,jenkins是一个java编写的开源的持续集成工具。具体来说，他可以将软件构建，测试，发布等一系列流程自动化，达到一键部署的目的。在进行本实验前，首先要有一个k8s环境，这里不再赘述。 部署jenkins这里存储用的是ceph rbd，所以先创建一个PVC： jenkins-pvc.yaml123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: jenkins-pvc namespace: cicdspec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: ceph-web 部署jenkins: jenkins-deployment.yaml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: jenkins namespace: cicd labels: app: jenkinsspec: strategy: type: Recreate template: metadata: labels: app: jenkins tier: jenkins spec: containers: - image: chadmoon/jenkins-docker-kubectl:latest name: jenkins securityContext: privileged: true ports: - containerPort: 8080 name: jenkins - containerPort: 50000 name: agent protocol: TCP volumeMounts: - name: docker mountPath: /var/run/docker.sock - name: jenkins-persistent-storage mountPath: /root/.jenkins - name: kube-config mountPath: /root/.kube/config - name: image-registry mountPath: /root/.docker volumes: - name: docker hostPath: path: /var/run/docker.sock - name: jenkins-persistent-storage persistentVolumeClaim: claimName: jenkins-pvc - name: kube-config hostPath: path: /root/.kube/config - name: image-registry configMap: name: image-registry-auth 简单解释一下： 该镜像除了安装jenkins，还装了docker cli（与host docker daemon交互），kubectl（与k8s apiserver交互） 容器开了两个端口，一个用于web-ui,一个用于后面实验jenkins kubernetes plugin时与JNLP slave agents 交互。 挂载了四个volume，依次是，一个用于docker cli，一个用于存储jenkins数据，一个用于kubectl与k8s交互验证，最后挂载了一个configmap，与image registry（我们用的harbor）交互验证。 部署jenkins service &amp; ingress: jenkins-service-ingress.yaml1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Servicemetadata: name: jenkins-web-ui namespace: cicd labels: app: jenkinsspec: ports: - port: 80 targetPort: 8080 name: web-ui - port: 50000 targetPort: 50000 name: agent selector: app: jenkins tier: jenkins---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: jenkins-web-ui namespace: cicdspec: rules: - host: jenkins.com http: paths: - backend: serviceName: jenkins-web-ui servicePort: 80 完成上述所有操作后，查看一下对应的pod 是否running。 配置pipeline按照ingress的配置，修改本地host，然后浏览器中输入http://jenkins.com ，就进入到jenkins 的web-ui了。 按照提示创建完用户后，开始进入CICD 的实验环节： 新建一个item，命名并选中pipeline: pipeline 配置如下： 在Git Repository URL部分添加github url,这里用的是我的github-test-kubernetes-ci-cd，是直接fork自kubernetes-ci-cd，并做了一些更改,之后保存就可以了。进入刚创建的item，点击立即构建： 之后就可以看到构建信息了，如果出错也可以查看对应步骤的log。同时，我们的应用也已经部署到k8s中了。 步骤详解接下来看一下点击“立即构建”后发生了什么，点击后，jenkins首先是从github检出项目代码，然后根据检出的项目中根目录下的Jenkinsfile进行项目构建，看下该项目的Jenkinsfile。123456789101112131415161718192021222324252627node &#123; checkout scm env.DOCKER_API_VERSION="1.23" sh "git rev-parse --short HEAD &gt; commit-id" tag = readFile('commit-id').replace("\n", "").replace("\r", "") appName = "hello-kenzan" registryHost = "172.16.21.253:10080/library/" imageName = "$&#123;registryHost&#125;$&#123;appName&#125;:$&#123;tag&#125;" env.BUILDIMG=imageName stage "Build" sh "docker build -t $&#123;imageName&#125; -f applications/hello-kenzan/Dockerfile applications/hello-kenzan" stage "Push" sh "docker push $&#123;imageName&#125;" stage "Deploy" sh "sed 's#127.0.0.1:30400/hello-kenzan:latest#'$BUILDIMG'#' applications/hello-kenzan/k8s/deployment.yaml | kubectl apply -f -" sh "kubectl rollout status deployment/hello-kenzan"&#125; 可以看到，Jenkinsfile定义了三个阶段，第一个阶段是“Build”,这个阶段是根据给定的Dockerfile创建一个镜像，第二个阶段“Push”,把生成的镜像push到我们的镜像仓库中，最后一个阶段是”Deploy”，编辑了一下deployment.yaml模板，然后调用kubectl命令进行部署。看一下“Build”阶段的dockerfile:123456FROM nginx:latestCOPY index.html /usr/share/nginx/html/index.htmlCOPY DockerFileEx.jpg /usr/share/nginx/html/DockerFileEx.jpgEXPOSE 80 就是一个很简单的nginx应用。再看下deployment.yaml：12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: Servicemetadata: name: hello-kenzan labels: app: hello-kenzanspec: ports: - port: 80 targetPort: 80 selector: app: hello-kenzan tier: hello-kenzan type: NodePort---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: hello-kenzan labels: app: hello-kenzanspec: strategy: type: Recreate template: metadata: labels: app: hello-kenzan tier: hello-kenzan spec: containers: - image: 127.0.0.1:30400/hello-kenzan:latest name: hello-kenzan ports: - containerPort: 80 name: hello-kenzan 这里服务发现使用的nodeport，可以改为其他方式（比如ingress）。 接下来，可以试着修改一下index.html，然后push到github中，再构建一下，看看内容是否改变。 利用jenkins kubernetes plugin实现动态分配资源构建在上述实例中，我们利用jenkins实现了一个小应用的CI/CD，这个小应用非常简单，“build”阶段就是直接在本地调用host docker构建的镜像，设想一下，如果这个应用需要编译，需要测试，那么这个时间就长了，而且如果都在本地构建的话，一个人使用还好，如果多个人一起构建，就会造成拥塞。为了解决上述问题，我们可以充分利用k8s的容器编排功能，jenkins接收到任务后，调用k8s api，创造新的 agent pod，将任务分发给这些agent pod，agent pod执行任务，任务完成后将结果汇总给jenkins pod，同时删除完成任务的agent pod。为了实现上述功能，我们需要给jenkins安装一个插件，叫做jenkins kubernetes plugin。 插件安装与配置安装比较简单，直接到jenkins 界面的系统管理，插件管理界面进行安装就可以了。安装好之后，进入系统管理—–&gt;系统设置，最下面有一个“云”，选择“新增一个云”—-&gt;kubernetes。 这里没有配置k8s，因为如果不配置api-server的话，jenkins会默认使用~/.kube/config下的配置，而我们已经在~/.kube/config做过配置了，所以这里就不做了。Jenkins URL我们使用的是集群内的服务地址。再往下看 kubernetes pod template配置： 这个pod tempalte就是之后我们创建 agent使用的模板，镜像使用“jenkins/jnlp-slave:alpine”，配置完成后，点击保存。然后还要配置一下agent与jenkins通信的端口，在系统管理—-&gt;Configure Global Security，指定端口为我们之前设定的5000端口： 简单测试配置完成后做一个简单的测试。 新建一个item，这里选择“构建一个自由风格的软件项目”： 配置时注意在General部分有一个restrict： Label Expression就写之前我们k8s podtemplate 的label。 在构建部分我们写一个简单的测试命令：echo TRUE 点击立即构建，如果成功的话，我们在“管理主机”模块会看到新增了一个主机： 同时，也会在k8s中发现新创建了一个名为jnlp-slave-8bq5m的pod。任务结束后，pod删除，主机消失，在console output 会看到执行结果： 出现问题总结jnlp-slave pod创建失败查看pod日志，发现是连接不上jenkins ，通过修改Configure Global Security的启用安全，TCP port for JNLP agents指定端口解决。 jnlp-slave pod 无法删除因为我们执行构建后，如果 jnlp-slave pod创建失败，它会不断的尝试创建新的pod，并试图连接jenkins，一段时间后，就会创造很多失败的jnlp-slave pod。如果遇到这种情况，需要尽早中断任务并删除失败的pod。在删除某个pod时 ，该pod一直处于termating阶段，kubectl delete无法删除。后来参考Pods stuck at terminated status，使用如下命令解决：1kubectl delete pod NAME --grace-period=0 --force 参考文章Set Up a CI/CD Pipeline with a Jenkins Pod in Kubernetes Achieving CI/CD with Kubernetes 容器时代CI/CD平台中的Kubernetes调度器定制方法 Jenkinsfile使用 安装和设置 kubectl jenkins-kubernetes-plugin 基于Kubernetes 部署 jenkins 并动态分配资源 使用Kubernetes-Jenkins实现CI/CD 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kuberbetes-- kubernetes ingress 实践]]></title>
    <url>%2F2017%2F12%2F15%2Fkubernetes-ingress-intro%2F</url>
    <content type="text"><![CDATA[概述k8s集群暴露内部服务有多种方式： Nodeport Service: 通过host port做端口映射，由kube-proxy实现。 LoadBalancer Service: 利用云平台提供的LoadBalancer服务，也是由kube-proxy实现。 Ingress: k8s1.2版本新增功能，由反向代理负载均衡器，Ingress Controller，ingress配置共同实现。 Port Proxy: 起一个pod专门做端口转发，可以将pod/host port 转发给k8s service，参考Proxy a pod port or host port to a kubernetes Service Service loadbalancer: 在物理机上部署一套Service loadbalancer，参考Bare Metal Service Load Balancers 其中，后两种方法因为相对比较复杂，逐渐被遗弃。目前用的比较多的就是前三种,LoadBalancer Service需要云服务的参与，NodePort 方式在集群内服务数量可控的情况下可以使用，当服务数量增多时需要注意端口管理，防止端口冲突。Ingress方式需要我们自行部署ingress controller服务，本篇文章详述一下Ingress方式。 Ingress 原理在讲Ingres实现原理时，我们可以从NodePort入手，NodePort最大的弊端就是端口多了之后难以管理，我们自然而然的就会想到利用反向代理工具（比如 nginx）只监听host上一个端口，然后再根据请求的域名转发给集群内部服务，这就要求这个nginx能够转发到集群内部，这个简单，我们直接将nginx部署到集群内部就可以了。接下来的问题就是如何配置nginx了，这就要借助k8s中的ingress了，ingress实际上就是一个yaml文件，真正执行配置nginx的是叫做ingress controller的程序，它会调用k8s 的api，获取ingress，然后根据这个yaml文件生成nginx 配置模板，写入nginx。除此之外，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的目的。 贴一张图，图片来自这里 Ingress 实践ingress包括反向代理负载均衡器，Ingress Controller，ingress三部分，通常反向代理负载均衡器与Ingress Controller会部署到同一个pod中，一个负责反向代理，一个负责与k8s交互并更新配置。不过随着微服务的流行，有人将这两个功能合在了一块，就是traefik。这样，大致结构就是这样： 接下来开始部署： 因为有RBAC，所以先部署相应角色：traefik-rbac.yaml12345678910111213141516171819202122232425262728293031323334353637---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerrules: - apiGroups: - "" resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controllersubjects:- kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system 接下来开始部署traefik(可以使用deployment或DaemonSet): traefik-daemonset.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556---apiVersion: v1kind: ServiceAccountmetadata: name: traefik-ingress-controller namespace: kube-system---kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 hostNetwork: true containers: - image: traefik name: traefik-ingress-lb ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8080 securityContext: privileged: true args: - -d - --web - --kubernetes---kind: ServiceapiVersion: v1metadata: name: traefik-ingress-service namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: web - protocol: TCP port: 8080 name: admin type: NodePort 创建一个ingress，将traffik的web ui 暴露出来： traefik-ingress.yaml123456789101112131415161718192021222324252627apiVersion: v1kind: Servicemetadata: name: traefik-web-ui namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - port: 80 targetPort: 8080---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-web-ui namespace: kube-system annotations: kubernetes.io/ingress.class: traefikspec: rules: - host: traefik-ui.minikube http: paths: - backend: serviceName: traefik-web-ui servicePort: 80 Ingress spec 中包含配置一个loadbalancer或proxy server的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前ingress只支持http规则。每条http规则包含以下信息：一个host配置项（比如traefik-ui.minikube，默认是*），path列表（比如：/testpath，默认是:/），每个path都关联一个backend(比如traefik-web-ui:80)。在loadbalancer将流量转发到backend之前，所有的入站请求都要先匹配host和path。如果没有rules，可以指定一个默认backend（例如404 page）。 部署完成，修改本地host，将traefik-ui.minikube 指向host ip。浏览器中输入http://traefik-ui.minikube/dashboard/#/即可： 生产环境中可以利用dns再做一层负载均衡。 如果需要部署 https，可以参考Using Traefik with TLS on Kubernete 最后贴一个trouble shooting 的案例Kubernetes: troubleshooting ingress and services traffic flows 参考文章Ingress Kubernetes Ingress Controller Kubernetes: troubleshooting ingress and services traffic flows Traefik-kubernetes 初试 Kubernetes Nginx Ingress 教程 Kubernetes Ingress解析 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kuberbetes-- kubernetes网络方案总结（二）]]></title>
    <url>%2F2017%2F12%2F14%2Fkubernetes-network-summary-2%2F</url>
    <content type="text"><![CDATA[概述上篇文章对k8s的几种网络方案做了详细对比，本篇文章主要探究下两种典型的网络方案flannel和calico。 flannelflannel是coreos为k8s设计的一个非常简洁的多节点3层容器网络互联方案。 原理flannel 旨在解决不同host上的容器网络互联问题，大致原理是每个 host 分配一个 subnet，容器从此 subnet 中分配 IP，这些 IP 可以在 host 间路由，容器间无需 NAT 和 port mapping 就可以跨主机通信。每个 subnet 都是从一个更大的 IP 池中划分的，flannel 会在每个主机上运行一个叫 flanneld 的 agent，其职责就是从池子中分配 subnet。为了在各个主机间共享信息，flannel 用 etcd（如果是k8s集群会直接调用k8s api）存放网络配置、已分配的 subnet、host 的 IP 等信息。节点间的通信有以下多种backen支持。 VXLAN：推荐配置，利用内核级别的VXLAN来封装host之间传送的包。 host-gw：对于性能有要求的推荐配置，但是不支持云环境。通过在host的路由表中直接创建到其他主机 subnet 的路由条目，从而实现容器跨主机通信。要求所有host在二层互联。 udp：默认模式，通常用于debug，或以上两种条件都不具备。 找了一张有容云博客中的图： 图中backend是用udp作为示例，我们可以换成其他任意两种，原理类似。 实践在docker 中的实践可以参考安装配置 flannel - 每天5分钟玩转 Docker 容器技术（59）系列文章。 在k8s中，如果是使用kubeadm安装k8s集群的话，可以参考kubeadm,其实最主要的就是这个文件，默认是vxlan，博主之前安装就是用的vxlan,就不重复操作了，大致解释下yaml文件中的内容： 创建对应clusterrole,clusterrolebinding。 创建一个flannel service account。 创建一个configmap用作cni和flannel的配置。其中flannel的network 要与之前kubeadm安装k8s时的pod network CIDR符合，默认backend是vxlan，可以改成其他的。 创建flannel daemonset在每台host上起一个pod，每个pod有两个容器，一个跑flanneld 程序，另一个initcontainer部署一些cni配置以便kubelet可以读取。 calico相对于flannel的小而美，calico是一个比较完整的项目，官网的title是“Secure networking for the cloud native era”，可见calico是专为云环境设置，且比较注重安全性，也就是网络隔离，ACL控制等都是可以实现的。列一下官网中的feature: 可扩展，分布式的控制组件 基于可配置policy的网络安全 无需overlay 可以集成所有的云平台（From Kubernetes to OpenStack, AWS to GCE） 经过生产环境认证。 原理calico是一个纯三层的数据中心网络方案，实现类似于flannel host-gw,不过它没有复用docker 的docker0 bridge，而是自己实现的。Calico在每一个计算节点利用Linux Kernel实现了一个高效的vRouter来负责数据转发，而每个vRouter通过BGP协议负责把自己上运行的workload的路由信息像整个Calico网络内传播——小规模部署可以直接互联，大规模下可通过指定的BGP route reflector来完成。Calico基于iptables还提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。对于有IP限制的host，也可以使用calico的IPIP方案（overlay方式）。 calico 的架构： 具体通信流程可以结合实践，参考如何部署 Calico 网络？- 每天5分钟玩转 Docker 容器技术（67） 实践在docker上的实践参考Calico with Docker,嫌麻烦直接看如何部署 Calico 网络？- 每天5分钟玩转 Docker 容器技术（67） 在k8s上的部署可以参考Adding Calico to an Existing Kubernetes Cluster tips: calico 可以通过profile 来实现ACL控制，结合k8s的NetworkPolicy可以实现租户网络隔离,这对公有云还是很有必要的，参考容器编排之Kubernetes多租户网络隔离。 参考文章coreos/flannel calico Kubernetes网络原理及方案 安装配置 flannel - 每天5分钟玩转 Docker 容器技术（59） 最新实践 | 将Docker网络方案进行到底 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kuberbetes-- kubernetes网络方案总结（一）]]></title>
    <url>%2F2017%2F12%2F12%2Fkubernetes-network-summary-1%2F</url>
    <content type="text"><![CDATA[概述kubernetes本身是不提供网络方案的，我们需要通过addon的方式为k8s集群提供网络功能。目前有很多这种第三方的解决方案，从实现原理上来说大致分为两种，一种是Overlay Networking，也就是通过封包解包的方式构造一个隧道，这种方案在IAAS层用的比较多，比如openstack-neutron就使用的ovs，这种方式因为封包解包的动作，所以性能会有所下降，调试也比较复杂（搞openstack的深有体会）。不过部署简单，架构清晰。代表性的方案有flannel(udp/vxlan)，weave，calico(ipip)，openvswitch等。还有一种方案就是通过路由来实现(更改iptables等手段)，这种方式性能损耗少，不过随着集群内容器增加，路由记录也会不断增加，性能随之逐渐下降。这种方案的典型代表是flannel(host-gw)，calico(bgp)，Macvlan（需特殊二层路由器），contiv等。 下面就说下几种网络方案的对比，并在下一篇博客中详细叙述下两种最常见的网络方案：flannel和calico。 几种典型网络方案对比博主没有条件对这几种网络方案进行具体实践对比，从网上搜了一些比较靠谱的测评，罗列如下。 性能对比性能对比，博主找到了两篇相对靠谱的文章，一篇是从hacker news上面找到的，后来发现了中文翻译版，这篇文章主要是对比了Docker host方案，flannel，IPVlan三种方案，这里提一嘴IPVlan，它是 Linux 内核中的一个驱动，有了它，无需使用桥接接口，就可以创建拥有唯一 IP 地址的虚拟网络接口。IPvlan 是一个相对较新的解决方案，现在还没有能完成上述过程的自动化工具。如果集群的机器和容器比较多，部署 IPvlan 的难度不小，不容易设置。直接上这篇文章的结论吧： 总结一下就是私有云环境下flannel + host-gw方案顶呱呱。 第二篇文章是来自有容云团队的，他们覆盖的方案范围比较广，详情可参考Kubernetes网络原理及方案，这里直接贴测试结果： 再次说一下，以上测试仅供参考，性能测试是一个罗生门，具体的网络方案还是得看实际应用场景，是私有云环境，公有云环境，还是混合云环境，有无SDN，节点数量，集群规模等等，最朴素的判断Underlay 网络性能确实优于 Overlay 网络，但是Overlay 较 Underlay 可以支持更多的二层网段，能更好地利用已有网络，以及有避免物理交换机 MAC 表耗尽等优势，所以在方案选型的时候需要综合考虑。 特点对比没想到搜到了一篇学长的博客，翻了一下他的linkedin，立马汗颜，扯远了，文章写的很详细，省的自己整理了，还是直接贴结论，详情去看文章吧。 参考文章Comparison of Networking Solutions for Kubernetes Battlefield: Calico, Flannel, Weave and Docker Overlay Network 一个适合 Kubernetes 的最佳网络互联方案 最新实践 | 将Docker网络方案进行到底 Kubernetes网络原理及方案 Docker容器跨主机通信方案选哪一种？ 一文搞懂各种 Docker 网络 - 每天5分钟玩转 Docker 容器技术（72） 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kuberbetes-- 私有容器云平台实践优化总结]]></title>
    <url>%2F2017%2F12%2F08%2Fsome-tips-about-private-container-cloud%2F</url>
    <content type="text"><![CDATA[概述私有容器云平台一期的项目算是进入尾声，本篇文章算是对该项目做一个简单的总结，以及对基于k8s私有容器云落地的一些想法。 架构整体大致架构如下： 因为没有公有云的加入，算是一个比较简单的架构，基本功能包括容器应用的构建，管理，镜像管理（采用Harbor），权限管理，以及针对集群的监控，日志管理等等。因为是第一版，没有将devops放进来，可能年前会发一个小版本加进来。 主要的开发工作如下： 集群的安装部署，以及对应自动化脚本文件的编写（我们用的是saltstalk），包括k8s，ceph，harbor等等。 定制k8s addon文件，包括calico网络，监控，日志等。 定制k8s yaml模板。 用户 web端的编码，用户对容器云的使用都是通过该web client进行，包括权限管理也是放在这里，可以通过web shell访问容器。 应用迁移 其他 优化结合具体工作谈一下具体优化的地方： docker部分 首先是版本问题，k8s建议是 1.12.6,同时要注意与内核版本的兼容问题。 docker 镜像部分优化：docker镜像大致可以分为三层，底层的系统层（可以使用alpine），中间层是运行环境（java/python等），最上层是应用代码层。一般来说系统层肯定是会复用的，应用层肯定是不会复用的。所以系统层就需要做一下定制，安装一些常用工具即可。中间层的复用就见仁见智了，如果想节省磁盘空间，且应用的运行环境比较统一（比如都是java8），那么可以复用（将jdk直接以volume形式挂载到容器中），如果业务繁多，且技术债务比较重，还是隔离较好。（以笔者的经验，若不是推倒重来，一般还是一个容器一个环境更好。） 依赖库的安装：尽量使用yum等安装（装完后会默认删除） kubernetes 部分安装部署这部分没什么好说的，就是尽量上tls（可以使用cloudera的套件），然后master节点做HA，我们使用kubeadm进行的安装部署，然后在此基础上做的HA 。最近好像使用rancher 安装更简单了，笔者抽时间会尝试一下。 使用 网络部分：k8s网络方案有很多，根据适用场景的不同，选择也不同。我们用的calico，因为对openstack比较友好。目前来看，比较有潜力的开源方案有flannel(host-gw),calico(ipip/bgp)。关于网络这一部分，后续会专门写一篇文章。 存储部分：这部分我们使用了多种存储以应对不同的应用类型。对于一般的有状态服务，我们使用ceph rbd。如果是对io请求敏感的服务，我们会挂载本地host ssd。这个地方需要注意做node label，防止应用挂掉然后schedule到别的host。 QOS部分：QOS部分就使用k8s的resource request/limit来控制。 label使用：随着应用的逐渐增多，各种label层出不穷，而且运维人员不可能对每一个label都熟悉，所以建议对label做一个管理或规范。 addon插件 监控：Prometheus &amp; Grafana，基本够用，参考kubernetes– kubernetes 监控指南（二） 日志：EFK基本够用，如果不放心，中间加一个MQ（kafka）。对于日志收集，我们是在在每个pod中添加一个sidecar container将输出日志都重定向到标准输出，这样就可以用fluentd统一收集。 web ui：除了原生的dashboard（只允许特定的几个运维IP访问）外，我们还做了一个面向developer的界面，用于执行应用的创建，升级等操作。 devops：这部分还没做，年前会出个小版本。 后续的工作以及一些想法 devops部分，目前我们只能算是拿容器当虚拟机来使用。只有上了devops，才算真正发挥容器的作用。后续会对接gitlab代码库，调研devops工具，选择合适的方案。 目前我们云平台的权限管理部分还太粗粒度，只是在web 层面做的一刀切，后续会对接k8s 的RBAC。 之后会对接大数据的一部分业务，而且目前社区spark on kubernetes也在积极开发，后续会持续关注。 支持更多一键式部署应用。 对k8s的一些概念要进行二层封装，从使用者的角度考虑，做到看一眼界面基本知道大致怎样用。 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>private cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes-- kubernetes 的日志解决方案]]></title>
    <url>%2F2017%2F11%2F23%2Fkubernetes-logging-solution%2F</url>
    <content type="text"><![CDATA[概述对于一个分布式平台来说，日志收集处理是一个不可或缺的功能。目前，ELK Stack 已经成为最流行的集中式日志解决方案。本文主要梳理一下ELK的一些理论知识，并针对K8S容器云平台探讨一下集中式日志解决方案的可行性，并做一下简单实践。 ELK StackELK Stack主要包括以下组件： Elasticsearch：分布式搜索和分析引擎，基于 Apache Lucene构建，用于对大容量的数据进行接近实时的存储，搜索和分析。具有高可伸缩，高可靠，易管理等特点。通常用作某些应用的基础搜索引擎，使其具有复杂的搜索功能。 Logstash: 数据收集引擎。支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置。 Kibana：数据分析和可视化平台。通常与 Elasticsearch配合使用，对其中数据进行搜索、分析和以统计图表的方式展示； Filebeat：ELK 协议栈的新成员，一个轻量级开源日志文件数据搜集器，基于 Logstash-Forwarder 源代码开发，是对它的替代（Logstash占用内存太大）。在需要采集日志数据的 server 上安装 Filebeat，并指定日志目录或日志文件后，Filebeat 就能读取数据，迅速发送到 Logstash 进行解析，亦或直接发送到 Elasticsearch 进行集中式存储和分析。 ELK 的架构设计是跟业务息息相关的，如果是数据量比较小，可靠性要求不高，允许数据丢失的情况可以直接布单实例的ELK，大致如下： 日志搜集部分的logstash可以部署在多台机器上，当然，也可以采用其他日志收集工具，比如Filebeat，rsyslog，fluent等。这种架构每个环节都有单点故障的可能，而且没有分流的功能，一旦出现数据量激增的情况可能中间的某个组件就挂了。 生产环境中会在上述架构的基础上增加一些高可用的特性，示例如下： 这里首先注意到的是增加了一个消息队列来削峰填谷，在收集数据完之后，这里还用logstash做了数据过滤，格式转换等数据处理工作（可选），elasticsearch采用集群的方式部署（图中未体现出来）。 Kubernetes Logging Architecture在k8s官网中，对于日志处理的理论部分说的还是挺详细的。总结如下： 在k8s日志收集方案中，大致可以分为三个级别，第一级别是pod中程序产生的应用日志，第二个级别是node级别的系统日志，第三个级别是集群级别的日志收集方案。 首先是pod级别的日志，默认指定程序输出到标准输出，然后就可以通过kubectl logs获取到日志。node级别的日志收集方案，首先要考虑的就是容器中程序产生的日志，这部分日志可以通过容器配置中的log-driver来对日志进行日志管理。其他程序的日志可以指定日志输出路径（比如/var/log）。值得注意的是，这个级别的解决方案需要一个logrotate组件来对日志文件进行管理。常用的log-driver如下： 集群级别的日志解决方案，这种情况下就要使用ELK Stack了，同时还要考虑容器漂移问题。对于日志收集部分，有三种日志收集方案： 使用节点日志agent:也就是在node级别进行日志收集。一般使用DaemonSet部署在每个node中。这种方式优点是耗费资源少，因为只需部署在节点，且对应用无侵入。缺点是只适合容器内应用日志必须都是标准输出。 使用sidecar container作为容器日志agent：也就是在pod中跟随应用容器起一个日志处理容器，有两种形式：一种是直接将应用容器的日志收集并输出到标准输出（叫做Streaming sidecar container），如下图： 还有一种是将应用容器日志直接输出到日志收集后端，也就是每一个pod中都起一个日志收集agent（比如logstash或fluebtd）。如下图：这种方式的优点是可以收集多种形式的日志(比如文件，socket等)，缺点是耗费资源较多，每个pod都要起一个日志收集容器，相对来说，Streaming sidecar container的形式比较折中，既能收集多种形式的容器，耗费资源也没有太多，因为起的日志处理容器仅仅是将多种形式的日志输出到标准输出而已。 在应用容器中直接将日志推到存储后端。 EFK 实践接下来是具体实践，以k8s项目的addon中的EFK为例，因为版本不同，我这边的k8s是1.7.3,而github中的是1.8,所以yaml文件做了一些更改，主要是api-version的改变。 首先说明一下该实践的大体架构：每个节点以daemonset的形式跑一个fluentd，收集节点日志，收集的数据存储到ES中，最终通过Kibana可视化。这种部署方式只能收集容器应用日志输出到标准输出。而且，因为没有对ES加验证，且存储方式不是持久存储，所以不能在生产环境中使用。 部署Elasticsearch在部署ES之前，首先看一下docker 的log-driver配置，修改为json-file,默认的可能是journald。fluentd要读取/var/log/containers/目录下的log日志，这些日志是从/var/lib/docker/containers/${CONTAINER_ID}/${CONTAINER_ID}-json.log链接过来的，如果log-driver是journald，就会读取不到:123vim /etc/sysconfig/dockerOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'........ 部署es-statefulset：es-statefulset.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112apiVersion: v1kind: ServiceAccountmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilerules:- apiGroups: - "" resources: - "services" - "namespaces" - "endpoints" verbs: - "get"---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: kube-system name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilesubjects:- kind: ServiceAccount name: elasticsearch-logging namespace: kube-system apiGroup: ""roleRef: kind: ClusterRole name: elasticsearch-logging apiGroup: ""---# Elasticsearch deployment itselfapiVersion: apps/v1beta1kind: StatefulSetmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: serviceName: elasticsearch-logging replicas: 2 selector: matchLabels: k8s-app: elasticsearch-logging version: v5.6.4 template: metadata: labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: "true" spec: serviceAccountName: elasticsearch-logging containers: - image: registry.cn-qingdao.aliyuncs.com/zhangchen-aisino/elasticsearch:v5.6.4 name: elasticsearch-logging resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: elasticsearch-logging mountPath: /data env: - name: "NAMESPACE" valueFrom: fieldRef: fieldPath: metadata.namespace # Elasticsearch requires vm.max_map_count to be at least 262144. # If your OS already sets up this number to a higher value, feel free # to remove this init container. volumes: - name: elasticsearch-logging emptyDir: &#123;&#125; # Elasticsearch requires vm.max_map_count to be at least 262144. # If your OS already sets up this number to a higher value, feel free # to remove this init container. initContainers: - image: alpine:3.6 command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"] name: elasticsearch-logging-init securityContext: privileged: true 部署es-service:es-service.yaml1234567891011121314151617apiVersion: v1kind: Servicemetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "Elasticsearch"spec: ports: - port: 9200 protocol: TCP targetPort: db selector: k8s-app: elasticsearch-logging 部署Fluentd部署fluentd配置文件：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255kind: ConfigMapapiVersion: v1data: containers.input.conf: |- &lt;source&gt; type tail path /var/log/containers/*.log pos_file /var/log/es-containers.log.pos time_format %Y-%m-%dT%H:%M:%S.%NZ tag kubernetes.* read_from_head true format multi_format &lt;pattern&gt; format json time_key time time_format %Y-%m-%dT%H:%M:%S.%NZ &lt;/pattern&gt; &lt;pattern&gt; format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z &lt;/pattern&gt; &lt;/source&gt; system.input.conf: |- # Example: # 2015-12-21 23:17:22,066 [salt.state ][INFO ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081 &lt;source&gt; type tail format /^(?&lt;time&gt;[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?&lt;severity&gt;[^ \]]*) *\] (?&lt;message&gt;.*)$/ time_format %Y-%m-%d %H:%M:%S path /var/log/salt/minion pos_file /var/log/es-salt.pos tag salt &lt;/source&gt; # Example: # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script &lt;source&gt; type tail format syslog path /var/log/startupscript.log pos_file /var/log/es-startupscript.log.pos tag startupscript &lt;/source&gt; # Examples: # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json" # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404 &lt;source&gt; type tail format /^time="(?&lt;time&gt;[^)]*)" level=(?&lt;severity&gt;[^ ]*) msg="(?&lt;message&gt;[^"]*)"( err="(?&lt;error&gt;[^"]*)")?( statusCode=($&lt;status_code&gt;\d+))?/ path /var/log/docker.log pos_file /var/log/es-docker.log.pos tag docker &lt;/source&gt; # Example: # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal &lt;source&gt; type tail # Not parsing this, because it doesn't have anything particularly useful to # parse out of it (like severities). format none path /var/log/etcd.log pos_file /var/log/es-etcd.log.pos tag etcd &lt;/source&gt; # Multi-line parsing is required for all the kube logs because very large log # statements, such as those that include entire object bodies, get split into # multiple lines by glog. # Example: # I0204 07:32:30.020537 3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537] &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/kubelet.log pos_file /var/log/es-kubelet.log.pos tag kubelet &lt;/source&gt; # Example: # I1118 21:26:53.975789 6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/kube-proxy.log pos_file /var/log/es-kube-proxy.log.pos tag kube-proxy &lt;/source&gt; # Example: # I0204 07:00:19.604280 5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266] &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/kube-apiserver.log pos_file /var/log/es-kube-apiserver.log.pos tag kube-apiserver &lt;/source&gt; # Example: # I0204 06:55:31.872680 5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/kube-controller-manager.log pos_file /var/log/es-kube-controller-manager.log.pos tag kube-controller-manager &lt;/source&gt; # Example: # W0204 06:49:18.239674 7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312] &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/kube-scheduler.log pos_file /var/log/es-kube-scheduler.log.pos tag kube-scheduler &lt;/source&gt; # Example: # I1104 10:36:20.242766 5 rescheduler.go:73] Running Rescheduler &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/rescheduler.log pos_file /var/log/es-rescheduler.log.pos tag rescheduler &lt;/source&gt; # Example: # I0603 15:31:05.793605 6 cluster_manager.go:230] Reading config from path /etc/gce.conf &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/glbc.log pos_file /var/log/es-glbc.log.pos tag glbc &lt;/source&gt; # Example: # I0603 15:31:05.793605 6 cluster_manager.go:230] Reading config from path /etc/gce.conf &lt;source&gt; type tail format multiline multiline_flush_interval 5s format_firstline /^\w\d&#123;4&#125;/ format1 /^(?&lt;severity&gt;\w)(?&lt;time&gt;\d&#123;4&#125; [^\s]*)\s+(?&lt;pid&gt;\d+)\s+(?&lt;source&gt;[^ \]]+)\] (?&lt;message&gt;.*)/ time_format %m%d %H:%M:%S.%N path /var/log/cluster-autoscaler.log pos_file /var/log/es-cluster-autoscaler.log.pos tag cluster-autoscaler &lt;/source&gt; # Logs from systemd-journal for interesting services. &lt;source&gt; type systemd filters [&#123; "_SYSTEMD_UNIT": "docker.service" &#125;] pos_file /var/log/gcp-journald-docker.pos read_from_head true tag docker &lt;/source&gt; &lt;source&gt; type systemd filters [&#123; "_SYSTEMD_UNIT": "kubelet.service" &#125;] pos_file /var/log/gcp-journald-kubelet.pos read_from_head true tag kubelet &lt;/source&gt; &lt;source&gt; type systemd filters [&#123; "_SYSTEMD_UNIT": "node-problem-detector.service" &#125;] pos_file /var/log/gcp-journald-node-problem-detector.pos read_from_head true tag node-problem-detector &lt;/source&gt; forward.input.conf: |- # Takes the messages sent over TCP &lt;source&gt; type forward &lt;/source&gt; monitoring.conf: |- # Prometheus Exporter Plugin # input plugin that exports metrics &lt;source&gt; @type prometheus &lt;/source&gt; &lt;source&gt; @type monitor_agent &lt;/source&gt; # input plugin that collects metrics from MonitorAgent &lt;source&gt; @type prometheus_monitor &lt;labels&gt; host $&#123;hostname&#125; &lt;/labels&gt; &lt;/source&gt; # input plugin that collects metrics for output plugin &lt;source&gt; @type prometheus_output_monitor &lt;labels&gt; host $&#123;hostname&#125; &lt;/labels&gt; &lt;/source&gt; # input plugin that collects metrics for in_tail plugin &lt;source&gt; @type prometheus_tail_monitor &lt;labels&gt; host $&#123;hostname&#125; &lt;/labels&gt; &lt;/source&gt; output.conf: |- # Enriches records with Kubernetes metadata &lt;filter kubernetes.**&gt; type kubernetes_metadata &lt;/filter&gt; &lt;match **&gt; type elasticsearch log_level info include_tag_key true host elasticsearch-logging port 9200 logstash_format true # Set the chunk limits. buffer_chunk_limit 2M buffer_queue_limit 8 flush_interval 5s # Never wait longer than 5 minutes between retries. max_retry_wait 30 # Disable the limit on the number of retries (retry forever). disable_retry_limit # Use multiple threads for processing. num_threads 2 &lt;/match&gt;metadata: name: fluentd-es-config-v0.1.1 namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile 在部署fluentd-daemonset之前，先要给k8s node 添加label，beta.kubernetes.io/fluentd-ds-ready: “true”，因为fluentd-daemonset是根据这个label进行node selector。可以通过kubectl label 命令添加:1kubectl label node/node1 beta.kubernetes.io/fluentd-ds-ready:="true" 然后部署fluentd-daemonsetfluentd-es-ds.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111apiVersion: v1kind: ServiceAccountmetadata: name: fluentd-es namespace: kube-system labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilerules:- apiGroups: - "" resources: - "namespaces" - "pods" verbs: - "get" - "watch" - "list"---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilesubjects:- kind: ServiceAccount name: fluentd-es namespace: kube-system apiGroup: ""roleRef: kind: ClusterRole name: fluentd-es apiGroup: ""---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: fluentd-es-v2.0.2 namespace: kube-system labels: k8s-app: fluentd-es version: v2.0.2 kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: template: metadata: labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" version: v2.0.2 # This annotation ensures that fluentd does not get evicted if the node # supports critical pod annotation based priority scheme. # Note that this does not guarantee admission on the nodes (#40573). annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: fluentd-es containers: - name: fluentd-es image: registry.cn-qingdao.aliyuncs.com/zhangchen-aisino/fluentd-elasticsearch:v2.0.2 env: - name: FLUENTD_ARGS value: --no-supervisor -q resources: limits: memory: 500Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: libsystemddir mountPath: /host/lib readOnly: true - name: config-volume mountPath: /etc/fluent/config.d nodeSelector: beta.kubernetes.io/fluentd-ds-ready: "true" terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers # It is needed to copy systemd library to decompress journals - name: libsystemddir hostPath: path: /usr/lib64 - name: config-volume configMap: name: fluentd-es-config-v0.1.1 部署 Kibana部署kibana-deployment：kibana-deployment.yaml1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: apps/v1beta1kind: Deploymentmetadata: name: kibana-logging namespace: kube-system labels: k8s-app: kibana-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: replicas: 1 selector: matchLabels: k8s-app: kibana-logging template: metadata: labels: k8s-app: kibana-logging spec: containers: - name: kibana-logging image: docker.elastic.co/kibana/kibana:5.6.4 resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch-logging:9200 - name: SERVER_BASEPATH value: /api/v1/proxy/namespaces/kube-system/services/kibana-logging - name: XPACK_MONITORING_ENABLED value: "false" - name: XPACK_SECURITY_ENABLED value: "false" ports: - containerPort: 5601 name: ui protocol: TCP 部署kibana-service：kibana-service.yaml1234567891011121314151617apiVersion: v1kind: Servicemetadata: name: kibana-logging namespace: kube-system labels: k8s-app: kibana-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "Kibana"spec: ports: - port: 5601 protocol: TCP targetPort: ui selector: k8s-app: kibana-logging 等到所有 pod都running之后，执行kubectl proxy 命令：1kubectl proxy --address='172.16.21.250' --port=8086 --accept-hosts='^*$' 登录http://172.16.21.250:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging/app/kibana 即进入kibana的界面，在doscovery中 创建index,便可以看到ES中的日志数据了。 问题汇总 kibana使用Nodeport之后，本以为可以直接使用Nodeport连接，但是会报404 status 错误，在搜索之后，大概明白一点，如果启动参数中添加了server.basePath，那么一般是需要在前端做一个反向代理来重定向。在kibana的yaml文件中删除SERVER_BASEPATH该环境变量后，可以正常访问。 之后，尝试将ES的数据存储放到ceph中，yaml文件老写不对，最终尝试成功，文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119apiVersion: v1kind: ServiceAccountmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilerules:- apiGroups: - "" resources: - "services" - "namespaces" - "endpoints" verbs: - "get"---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: kube-system name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilesubjects:- kind: ServiceAccount name: elasticsearch-logging namespace: kube-system apiGroup: ""roleRef: kind: ClusterRole name: elasticsearch-logging apiGroup: ""---# Elasticsearch deployment itselfapiVersion: apps/v1beta1kind: StatefulSetmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: serviceName: elasticsearch-logging replicas: 2 selector: matchLabels: k8s-app: elasticsearch-logging version: v5.6.4 template: metadata: labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: "true" spec: serviceAccountName: elasticsearch-logging containers: - image: registry.cn-qingdao.aliyuncs.com/zhangchen-aisino/elasticsearch:v5.6.4 name: elasticsearch-logging resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: elasticsearch-logging mountPath: /data env: - name: "NAMESPACE" valueFrom: fieldRef: fieldPath: metadata.namespace #volumes: #- name: elasticsearch-logging #emptyDir: &#123;&#125; # Elasticsearch requires vm.max_map_count to be at least 262144. # If your OS already sets up this number to a higher value, feel free # to remove this init container. initContainers: - image: alpine:3.6 command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"] name: elasticsearch-logging-init securityContext: privileged: true volumeClaimTemplates: - metadata: name: elasticsearch-logging annotations: volume.beta.kubernetes.io/storage-class: "ceph-web" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 50Gi 参考文章Logging Architecture Elasticsearch Add-On 应用日志收集 从ELK到EFK ELK实战一:架构的选择 Docker日志收集最佳实践 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes-- kubernetes 使用ceph rbd作为持久存储]]></title>
    <url>%2F2017%2F11%2F17%2Fkubernetes-integrate-with-ceph%2F</url>
    <content type="text"><![CDATA[kubernetes 中的存储方案对于有状态服务，存储是一个至关重要的问题。k8s提供了非常丰富的组件来支持存储，这里大致列一下： volume: 就是直接挂载在pod上的组件，k8s中所有的其他存储组件都是通过volume来跟pod直接联系的。volume有个type属性，type决定了挂载的存储是什么，常见的比如：emptyDir，hostPath，nfs，rbd，以及下文要说的persistentVolumeClaim等。跟docker里面的volume概念不同的是，docker里的volume的生命周期是跟docker紧紧绑在一起的。这里根据type的不同，生命周期也不同，比如emptyDir类型的就是跟docker一样，pod挂掉，对应的volume也就消失了，而其他类型的都是永久存储。详细介绍可以参考Volumes Persistent Volumes：顾名思义，这个组件就是用来支持永久存储的，Persistent Volumes组件会抽象后端存储的提供者（也就是上文中volume中的type）和消费者（即具体哪个pod使用）。该组件提供了PersistentVolume和PersistentVolumeClaim两个概念来抽象上述两者。一个PersistentVolume（简称PV）就是后端存储提供的一块存储空间，具体到ceph rbd中就是一个image，一个PersistentVolumeClaim（简称PVC）可以看做是用户对PV的请求，PVC会跟某个PV绑定，然后某个具体pod会在volume 中挂载PVC,就挂载了对应的PV。关于更多详细信息比如PV,PVC的生命周期，dockerfile 格式等信息参考Persistent Volumes Dynamic Volume Provisioning: 动态volume发现，比如上面的Persistent Volumes,我们必须先要创建一个存储块，比如一个ceph中的image，然后将该image绑定PV，才能使用。这种静态的绑定模式太僵硬，每次申请存储都要向存储提供者索要一份存储快。Dynamic Volume Provisioning就是解决这个问题的。它引入了StorageClass这个概念，StorageClass抽象了存储提供者，只需在PVC中指定StorageClass，然后说明要多大的存储就可以了，存储提供者会根据需求动态创建所需存储快。甚至于，我们可以指定一个默认StorageClass，这样，只需创建PVC就可以了。 kubernetes 与ceph 整合首先要有一个k8s集群，一个ceph集群，k8s集群的搭建与ceph集群的搭建不再赘述。 预备工作 在每个k8s node中安装ceph-common 1yum install -y ceph-common 将ceph配置文件ceph.conf,ceph admin的认证文件ceph.client.admin.keyring复制到k8s node的/etc/ceph/ 目录下。 配置 ceph secret1grep key /etc/ceph/ceph.client.admin.keyring |awk '&#123;printf "%s", $NF&#125;'|base64 获取base64加密的key：QVFCZmdTcFRBQUFBQUJBQWNXTmtsMEFtK1ZkTXVYU21nQ0FmMFE9PQ==利用该key创建ceph-secret,这里我们单独创建了一个test-ceph namespace,所有操作都在该namespace下。ceph-secret.yaml12345678apiVersion: v1kind: Secretmetadata: name: ceph-secret namespace: test-cephtype: "kubernetes.io/rbd" data: key: QVFCZmdTcFRBQUFBQUJBQWNXTmtsMEFtK1ZkTXVYU21nQ0FmMFE9PQ== Persistent Volumes 测试首先在ceph 中创建一个2G 的image，这里为了方便直接在rbd pool中创建。 1rbd create test-image -s 2G --image-feature layering 查看新建image信息：12345678[root@seed galera-cluster]# rbd info test-imagerbd image 'test-image': size 2048 MB in 512 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.5ed8238e1f29 format: 2 features: layering flags: 注：这里有个ceph的坑，在jewel版本下默认format是2，开启了rbd的一些属性，而这些属性有的内核版本是不支持的，会导致map不到device的情况，可以在创建时指定feature（我们就是这样做的）,也可以在ceph配置文件中关闭这些新属性：rbd_default_features = 2。参考rbd无法map(rbd feature disable)。 创建PV,需要指定ceph mon节点地址，以及对应的pool，image等： test.pv.yml123456789101112131415161718192021apiVersion: v1kind: PersistentVolumemetadata: name: test-pv namespace: test-cephspec: capacity: storage: 2Gi accessModes: - ReadWriteOnce rbd: monitors: - 172.16.21.250:6789 - 172.16.21.251:6789 - 172.16.21.252:6789 pool: rbd image: test-image user: admin secretRef: name: ceph-secret persistentVolumeReclaimPolicy: Recycle 创建PVC： test.pvc.yml1234567891011kind: PersistentVolumeClaimapiVersion: v1metadata: name: test-pvc namespace: test-cephspec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 查看PV,PVC，如果状态是bound那么两者绑定成功。 创建一个pod验证： 12345678910111213141516171819202122232425apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-dm namespace: test-cephspec: replicas: 1 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: ceph-rbd-volume mountPath: "/usr/share/nginx/html" volumes: - name: ceph-rbd-volume persistentVolumeClaim: claimName: test-pvc 进入该容器，利用dh -h 命令验证是否挂载成功。 Dynamic Volume Provisioning测试创建一个storageclass： 1234567891011121314apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: ceph-storage namespace: test-cephprovisioner: ceph.com/rbdparameters: monitors: 172.16.21.250:6789,172.16.21.251:6789,172.16.21.252:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: test-ceph pool: rbd userId: admin userSecretName: ceph-secret 创建一个PVC，指定storageclass: 123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: ceph-claim-dynamic namespace: test-cephspec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: ceph-storage 注：这里又趟到了一个大坑，如果这样就直接创建pod挂载的话会报错如下：1Error creating rbd image: executable file not found in $PATH 这是因为我们的k8s集群是使用kubeadm创建的，k8s的几个服务也是跑在集群静态pod中，而kube-controller-manager组件会调用rbd的api，但是因为它的pod中没有安装rbd，所以会报错，如果是直接安装在物理机中，因为我们已经安装了ceph-common，所以不会出现这个问题。我们在该issue下找到了解决方案。如下： 创建一个 rbd-provisioner ： 12345678910111213141516apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: rbd-provisioner namespace: kube-systemspec: replicas: 1 template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: "quay.io/external_storage/rbd-provisioner:v0.1.0" serviceAccountName: persistent-volume-binder 这样就可以直接用PVC了，创建 pod测试下： 12345678910111213141516171819202122232425apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-test-dynamic namespace: test-cephspec: replicas: 1 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: ceph-rbd-volume mountPath: "/usr/share/nginx/html" volumes: - name: ceph-rbd-dynamic-volume persistentVolumeClaim: claimName: ceph-claim-dynamic 实战：创建一个mysql-galera集群在进入具体实战之前，先介绍一下k8s针对有状态服务推出的一个组件，statefulset(1.5之前叫做petset),statefulset与deployment,replicasets是一个级别的。不过Deployments和ReplicaSets是为无状态服务而设计。statefulset则是为了解决有状态服务的问题。它的应用场景如下： 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现。 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现。 有序收缩，有序删除（即从N-1到0）。 由应用场景可知，statefuleset特别适合mqsql,redis等数据库集群。相应的，一个statefuleset有以下三个部分： 用于定义网络标志（DNS domain）的Headless Service 用于创建PersistentVolumes的volumeClaimTemplates 定义具体应用的StatefulSet 以下是从github上找到的一个示例dockerfile: 首先创建headless service： galera-service.yaml1234567891011121314151617apiVersion: v1kind: Servicemetadata: annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: "true" name: galera namespace: galera labels: app: mysqlspec: ports: - port: 3306 name: mysql # *.galear.default.svc.cluster.local clusterIP: None selector: app: mysql 创建statefulset，这里存储直接用的storageclass指定ceph rbd,镜像下载需要科学上网。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: mysql namespace: galeraspec: serviceName: "galera" replicas: 3 template: metadata: labels: app: mysql spec: initContainers: - name: install image: gcr.io/google_containers/galera-install:0.1 imagePullPolicy: Always args: - "--work-dir=/work-dir" volumeMounts: - name: workdir mountPath: "/work-dir" - name: config mountPath: "/etc/mysql" - name: bootstrap image: debian:jessie command: - "/work-dir/peer-finder" args: - -on-start="/work-dir/on-start.sh" - "-service=galera" env: - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace volumeMounts: - name: workdir mountPath: "/work-dir" - name: config mountPath: "/etc/mysql" containers: - name: mysql image: gcr.io/google_containers/mysql-galera:e2e ports: - containerPort: 3306 name: mysql - containerPort: 4444 name: sst - containerPort: 4567 name: replication - containerPort: 4568 name: ist args: - --defaults-file=/etc/mysql/my-galera.cnf - --user=root readinessProbe: # TODO: If docker exec is buggy just use gcr.io/google_containers/mysql-healthz:1.0 exec: command: - sh - -c - "mysql -u root -e 'show databases;'" initialDelaySeconds: 15 timeoutSeconds: 5 successThreshold: 2 volumeMounts: - name: datadir mountPath: /var/lib/ - name: config mountPath: /etc/mysql volumes: - name: config emptyDir: &#123;&#125; - name: workdir emptyDir: &#123;&#125; volumeClaimTemplates: - metadata: name: datadir annotations: volume.beta.kubernetes.io/storage-class: "ceph-web" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 1Gi 以上涉及到三个镜像，前两个镜像在initContainers 下，表示这两个镜像创建的是Init Container，顾名思义，就是完成一些初始化的工作。这些 Init Container 按照定义的顺序依次执行，只有所有的Init Container 执行完后，主容器才启动。由于一个Pod里的存储卷是共享的，所以 Init Container 里产生的数据可以被主容器使用到。这里两个initcontainer主要完成以下两个工作：安装mysql-galera等组件,生成配置文件等。 最终结果如下： 12345[root@seed galera-cluster]# kubectl get pods -n galeraNAME READY STATUS RESTARTS AGEmysql-0 1/1 Running 0 47mmysql-1 1/1 Running 0 24mmysql-2 1/1 Running 0 2m 参考文章kubernetes Ceph RBD Dynamic Provisioning and Storage Classes in Kubernetes 使用Ceph RBD为Kubernetes集群提供存储卷 使用Ceph做持久化存储创建MySQL集群 Error creating rbd image: executable file not found in $PATH MySQL on Docker: Running Galera Cluster on Kubernetes StatefulSet 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>ceph</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes-- kubernetes 监控指南（二）]]></title>
    <url>%2F2017%2F11%2F09%2Fkubernetes-monitoring-guide-2%2F</url>
    <content type="text"><![CDATA[之前一篇博客介绍了k8s的官方监控方案，相对比较简单，因为缺乏通用性，所以一般生产环境还是用另一种方案，也就是本篇博客要介绍的，Prometheus &amp; Grafana. Prometheus &amp; GrafanaPrometheus介绍Prometheus 是一个开源的系统监控预警工具，由Soundcloud开发并开源，目前是一个独立的项目，已经有很多公司采用，主要特点有以下几个： 多维度数据模型 灵活的类SQL查询语句 不依赖分布式存储，单个自主的服务器节点。 通过基于HTTP的pull方式采集时序数据。 对于push型的数据可以通过中间网关来暂时存储支持 通过服务发现或者静态配置来发现数据搜集对象。 支持多种多样的图表和界面展示，比如Grafana等。 不过Prometheus也不是万能的，他不适于一些实时要求比较高的监控项目，且因为是单机存储，存储量有限，局限了存储时间。prometheus集成了leveldb，一个能高效插入数据的数据库，在ssd盘下io占用比较高，可能会有大量数据堆积内存，内存占用率大（可以进行适当配置）。 Prometheus的整体架构如下： 由上图可以看出Prometheus的几个重要组件： Prometheus Server：主要负责数据采集，存储，提供PromQL查询语言的支持。 Push Gateway: 支持临时性Job主动推送数据的中间网关。 Jobs/Exporter: 数据采集组件,从目标处搜集数据，并将其转化为Prometheus支持的格式,然后等待Prometheus Server对数据的拉取。 Alertmanager: 警告管理器，用来进行报警。 Prometheus的工作原理大致是这样的：Prometheus Server定时去拉取暴露出metric数据的http接口（一般由exporter实现），该接口可通过配置文件、Zookeeper、Consul等方式配置。拉取到数据后，会进行一定规则的数据清洗，然后存储到时间序列数据库中，可以通过PromQL和其他API，比如grafana可视化地展示收集的数据。对于主动推送型的数据，可以先放到PushGateway中，然后等待Server拉取。 Prometheus安装部署有一些Prometheus的安装项目已经将所有的yaml文件写好了，比如prometheus-operator,为了便于理解，本文还是一步步部署。 创建 一个独立的namesapcemonitoring-namespace.yaml1234apiVersion: v1kind: Namespacemetadata: name: monitoring 创建RBAC相关配置rbac.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kube-state-metricsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metricssubjects:- kind: ServiceAccount name: kube-state-metrics namespace: monitoring---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: kube-state-metricsrules:- apiGroups: [""] resources: - nodes - pods - services - resourcequotas - replicationcontrollers - limitranges verbs: ["list", "watch"]- apiGroups: ["extensions"] resources: - daemonsets - deployments - replicasets verbs: ["list", "watch"]---apiVersion: v1kind: ServiceAccountmetadata: name: kube-state-metrics namespace: monitoring---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus-k8s namespace: monitoring---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [""] resources: - nodes - services - endpoints - pods verbs: ["get", "list", "watch"]- apiGroups: [""] resources: - configmaps verbs: ["get"]- nonResourceURLs: ["/metrics"] verbs: ["get"]---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus-k8s namespace: monitoring 创建node-exporter用于收集节点信息exporter-daemonset.yaml1234567891011121314151617181920212223242526apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: prometheus-node-exporter namespace: monitoring labels: app: prometheus component: node-exporterspec: template: metadata: name: prometheus-node-exporter labels: app: prometheus component: node-exporter spec: containers: - image: prom/node-exporter:v0.14.0 name: prometheus-node-exporter ports: - name: prom-node-exp #^ must be an IANA_SVC_NAME (at most 15 characters, ..) containerPort: 9100 hostPort: 9100 hostNetwork: true hostPID: true 创建对应的service：exporter-service.yaml1234567891011121314151617181920apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' name: prometheus-node-exporter namespace: monitoring labels: app: prometheus component: node-exporterspec: clusterIP: None ports: - name: prometheus-node-exporter port: 9100 protocol: TCP selector: app: prometheus component: node-exporter type: ClusterIP 通过以下命令查看是否创建成功： 1kubectl -n monitoring get daemonset,svc 创建kube-state-metrics用于收集k8s的metric信息。 kube-state-metrics-deployment.yaml123456789101112131415161718apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-state-metrics namespace: monitoringspec: replicas: 2 template: metadata: labels: app: kube-state-metrics spec: serviceAccountName: kube-state-metrics containers: - name: kube-state-metrics image: gcr.io/google_containers/kube-state-metrics:v0.5.0 ports: - containerPort: 8080 创建对应的service:kube-state-metrics-deployment-service.yaml12345678910111213141516apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' name: kube-state-metrics namespace: monitoring labels: app: kube-state-metricsspec: ports: - name: kube-state-metrics port: 8080 protocol: TCP selector: app: kube-state-metrics 查看对应pod创建成功后，继续下一步，如果出现错误，多半是image pull不成功，可以科学上网pull下来。 创建node-directory-size-metrics主要用于读取节点目录，获取磁盘使用metric数据。node-directory-size-metrics-deployment.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: node-directory-size-metrics namespace: monitoring annotations: description: | This `DaemonSet` provides metrics in Prometheus format about disk usage on the nodes. The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now. The other container `caddy` just hands out the contents of that file on request via `http` on `/metrics` at port `9102` which are the defaults for Prometheus. These are scheduled on every node in the Kubernetes cluster. To choose directories from the node to check, just mount them on the `read-du` container below `/mnt`.spec: template: metadata: labels: app: node-directory-size-metrics annotations: prometheus.io/scrape: 'true' prometheus.io/port: '9102' description: | This `Pod` provides metrics in Prometheus format about disk usage on the node. The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now. The other container `caddy` just hands out the contents of that file on request on `/metrics` at port `9102` which are the defaults for Prometheus. This `Pod` is scheduled on every node in the Kubernetes cluster. To choose directories from the node to check just mount them on `read-du` below `/mnt`. spec: containers: - name: read-du image: giantswarm/tiny-tools imagePullPolicy: Always # FIXME threshold via env var # The command: - fish - --command - | touch /tmp/metrics-temp while true for directory in (du --bytes --separate-dirs --threshold=100M /mnt) echo $directory | read size path echo "node_directory_size_bytes&#123;path=\"$path\"&#125; $size" \ &gt;&gt; /tmp/metrics-temp end mv /tmp/metrics-temp /tmp/metrics sleep 300 end volumeMounts: - name: host-fs-var mountPath: /mnt/var readOnly: true - name: metrics mountPath: /tmp - name: caddy image: dockermuenster/caddy:0.9.3 command: - "caddy" - "-port=9102" - "-root=/var/www" ports: - containerPort: 9102 volumeMounts: - name: metrics mountPath: /var/www volumes: - name: host-fs-var hostPath: path: /var - name: metrics emptyDir: medium: Memory 创建prometheus 配置文件创建prometheus的主要配置文件。prometheus-config-map.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111apiVersion: v1data: prometheus.yaml: | global: scrape_interval: 10s scrape_timeout: 10s evaluation_interval: 10s rule_files: - "/etc/prometheus-rules/*.rules" scrape_configs: # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L37 - job_name: 'kubernetes-nodes' tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: '(.*):10250' replacement: '$&#123;1&#125;:10255' target_label: __address__ # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L79 - job_name: 'kubernetes-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: (.+)(?::\d+);(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L119 - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L156 - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: (.+):(?:\d+);(\d+) replacement: $&#123;1&#125;:$&#123;2&#125; target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - source_labels: [__meta_kubernetes_pod_container_port_number] action: keep regex: 9\d&#123;3&#125;kind: ConfigMapmetadata: creationTimestamp: null name: prometheus-core namespace: monitoring 创建 prometheus-rulesprometheus-rules.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071apiVersion: v1data: cpu-usage.rules: | ALERT NodeCPUUsage IF (100 - (avg by (instance) (irate(node_cpu&#123;name="node-exporter",mode="idle"&#125;[5m])) * 100)) &gt; 75 FOR 2m LABELS &#123; severity="page" &#125; ANNOTATIONS &#123; SUMMARY = "&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected", DESCRIPTION = "&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 75% (current value is: &#123;&#123; $value &#125;&#125;)" &#125; instance-availability.rules: | ALERT InstanceDown IF up == 0 FOR 1m LABELS &#123; severity = "page" &#125; ANNOTATIONS &#123; summary = "Instance &#123;&#123; $labels.instance &#125;&#125; down", description = "&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; has been down for more than 1 minute.", &#125; low-disk-space.rules: | ALERT NodeLowRootDisk IF ((node_filesystem_size&#123;mountpoint="/root-disk"&#125; - node_filesystem_free&#123;mountpoint="/root-disk"&#125; ) / node_filesystem_size&#123;mountpoint="/root-disk"&#125; * 100) &gt; 75 FOR 2m LABELS &#123; severity="page" &#125; ANNOTATIONS &#123; SUMMARY = "&#123;&#123;$labels.instance&#125;&#125;: Low root disk space", DESCRIPTION = "&#123;&#123;$labels.instance&#125;&#125;: Root disk usage is above 75% (current value is: &#123;&#123; $value &#125;&#125;)" &#125; ALERT NodeLowDataDisk IF ((node_filesystem_size&#123;mountpoint="/data-disk"&#125; - node_filesystem_free&#123;mountpoint="/data-disk"&#125; ) / node_filesystem_size&#123;mountpoint="/data-disk"&#125; * 100) &gt; 75 FOR 2m LABELS &#123; severity="page" &#125; ANNOTATIONS &#123; SUMMARY = "&#123;&#123;$labels.instance&#125;&#125;: Low data disk space", DESCRIPTION = "&#123;&#123;$labels.instance&#125;&#125;: Data disk usage is above 75% (current value is: &#123;&#123; $value &#125;&#125;)" &#125; mem-usage.rules: | ALERT NodeSwapUsage IF (((node_memory_SwapTotal-node_memory_SwapFree)/node_memory_SwapTotal)*100) &gt; 75 FOR 2m LABELS &#123; severity="page" &#125; ANNOTATIONS &#123; SUMMARY = "&#123;&#123;$labels.instance&#125;&#125;: Swap usage detected", DESCRIPTION = "&#123;&#123;$labels.instance&#125;&#125;: Swap usage usage is above 75% (current value is: &#123;&#123; $value &#125;&#125;)" &#125; ALERT NodeMemoryUsage IF (((node_memory_MemTotal-node_memory_MemFree-node_memory_Cached)/(node_memory_MemTotal)*100)) &gt; 75 FOR 2m LABELS &#123; severity="page" &#125; ANNOTATIONS &#123; SUMMARY = "&#123;&#123;$labels.instance&#125;&#125;: High memory usage detected", DESCRIPTION = "&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 75% (current value is: &#123;&#123; $value &#125;&#125;)" &#125;kind: ConfigMapmetadata: creationTimestamp: null name: prometheus-rules namespace: monitoring 创建prometheus-core即创建prometheus-server服务。 prometheus-core-deployment.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prometheus-core namespace: monitoring labels: app: prometheus component: corespec: replicas: 1 template: metadata: name: prometheus-main labels: app: prometheus component: core spec: serviceAccountName: prometheus-k8s containers: - name: prometheus image: prom/prometheus:v1.7.0 args: - '-storage.local.retention=12h' - '-storage.local.memory-chunks=500000' - '-config.file=/etc/prometheus/prometheus.yaml' - '-alertmanager.url=http://alertmanager:9093/' ports: - name: webui containerPort: 9090 resources: requests: cpu: 500m memory: 500M limits: cpu: 500m memory: 500M volumeMounts: - name: config-volume mountPath: /etc/prometheus - name: rules-volume mountPath: /etc/prometheus-rules volumes: - name: config-volume configMap: name: prometheus-core - name: rules-volume configMap: name: prometheus-rules 创建对应service：prometheus-service.yaml12345678910111213141516171819apiVersion: v1kind: Servicemetadata: name: prometheus namespace: monitoring labels: app: prometheus component: core annotations: prometheus.io/scrape: 'true'spec: type: NodePort ports: - port: 9090 protocol: TCP name: webui selector: app: prometheus component: core 如果以上pod都运行正常，那么prometheus大致安装完成了，接下来查看prometheus service对应的nodeport，然后就可以通过浏览器访问该service了。 点击“status”中的target，可以看到所有的数据来源状态： Grafana安装部署Grafana的具体安装部署过程就不再赘述了，比较简单，之前的博客中有详细教程，这里只说一下与prometueus对接的配置。其实也比较简单，就是一个添加数据源的操作。进入grafana界面，点击datasource，进入添加datasource的界面。 然后添加prometueus暴露出的cluster-ip:port即可，也可以用service名称代替，不过要加上namespace，比如：http://prometheus.monitoring:9090 。测试成功即可。 alertmanager 安装部署以上步骤完成后，还需要一个报警机制，alertmanager作为一个prometueus的独立模块实现了报警机制，安装如下： 创建alertmanager 配置文件本示例中只使用了邮箱报警（注意邮箱的配置），也可以使用slack报警配置，如果考虑其他比如短信或微信报警可以与onealert进行集成。alert-manager-configmap.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263kind: ConfigMapapiVersion: v1metadata: name: alertmanagerdata: config.yml: |- global: # ResolveTimeout is the time after which an alert is declared resolved # if it has not been updated. resolve_timeout: 5m # The smarthost and SMTP sender used for mail notifications. smtp_smarthost: 'smtp.gmail.com:587' smtp_from: 'foo@bar.com' smtp_auth_username: 'foo@bar.com' smtp_auth_password: 'barfoo' # The API URL to use for Slack notifications. #slack_api_url: 'https://hooks.slack.com/services/abc123' # # The auth token for Hipchat. # hipchat_auth_token: '1234556789' # # # Alternative host for Hipchat. # hipchat_url: 'https://hipchat.foobar.org/' # # The directory from which notification templates are read. templates: - '/etc/alertmanager-templates/*.tmpl' # The root route on which each incoming alert enters. route: # The labels by which incoming alerts are grouped together. For example, # multiple alerts coming in for cluster=A and alertname=LatencyHigh would # be batched into a single group. group_by: ['alertname', 'cluster', 'service'] # When a new group of alerts is created by an incoming alert, wait at # least 'group_wait' to send the initial notification. # This way ensures that you get multiple alerts for the same group that start # firing shortly after another are batched together on the first # notification. group_wait: 30s # When the first notification was sent, wait 'group_interval' to send a batch # of new alerts that started firing for that group. group_interval: 5m # If an alert has successfully been sent, wait 'repeat_interval' to # resend them. #repeat_interval: 1m repeat_interval: 15m # A default receiver # If an alert isn't caught by a route, send it to default. receiver: default # All the above attributes are inherited by all child routes and can # overwritten on each. # The child route trees. routes: # Send severity=slack alerts to slack. - match: severity: slack receiver: slack_alert - match: severity: email receiver: slack_alert # receiver: email_alert receivers: - name: 'default' email_configs: - to: 'foo@bar.com' 创建报警格式配置文件alertmanager-templates.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177apiVersion: v1data: default.tmpl: | &#123;&#123; define "__alertmanager" &#125;&#125;AlertManager&#123;&#123; end &#125;&#125; &#123;&#123; define "__alertmanagerURL" &#125;&#125;&#123;&#123; .ExternalURL &#125;&#125;/#/alerts?receiver=&#123;&#123; .Receiver &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "__subject" &#125;&#125;[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status "firing" &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;] &#123;&#123; .GroupLabels.SortedPairs.Values | join " " &#125;&#125; &#123;&#123; if gt (len .CommonLabels) (len .GroupLabels) &#125;&#125;(&#123;&#123; with .CommonLabels.Remove .GroupLabels.Names &#125;&#125;&#123;&#123; .Values | join " " &#125;&#125;&#123;&#123; end &#125;&#125;)&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "__description" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "__text_alert_list" &#125;&#125;&#123;&#123; range . &#125;&#125;Labels: &#123;&#123; range .Labels.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125; &#123;&#123; end &#125;&#125;Annotations: &#123;&#123; range .Annotations.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125; &#123;&#123; end &#125;&#125;Source: &#123;&#123; .GeneratorURL &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.title" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.username" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.fallback" &#125;&#125;&#123;&#123; template "slack.default.title" . &#125;&#125; | &#123;&#123; template "slack.default.titlelink" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.pretext" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.titlelink" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.iconemoji" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.iconurl" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.text" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "hipchat.default.from" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "hipchat.default.message" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.description" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.client" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.clientURL" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.instances" &#125;&#125;&#123;&#123; template "__text_alert_list" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "opsgenie.default.message" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "opsgenie.default.description" &#125;&#125;&#123;&#123; .CommonAnnotations.SortedPairs.Values | join " " &#125;&#125; &#123;&#123; if gt (len .Alerts.Firing) 0 -&#125;&#125; Alerts Firing: &#123;&#123; template "__text_alert_list" .Alerts.Firing &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123; if gt (len .Alerts.Resolved) 0 -&#125;&#125; Alerts Resolved: &#123;&#123; template "__text_alert_list" .Alerts.Resolved &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123; define "opsgenie.default.source" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "victorops.default.message" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125; | &#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "victorops.default.from" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "email.default.subject" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "email.default.html" &#125;&#125; &lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt; &lt;!-- Style and HTML derived from https://github.com/mailgun/transactional-email-templates The MIT License (MIT) Copyright (c) 2014 Mailgun Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --&gt; &lt;html xmlns="http://www.w3.org/1999/xhtml" xmlns="http://www.w3.org/1999/xhtml" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;head style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;meta name="viewport" content="width=device-width" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;title style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;&#123;&#123; template "__subject" . &#125;&#125;&lt;/title&gt; &lt;/head&gt; &lt;body itemscope="" itemtype="http://schema.org/EmailMessage" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; -webkit-font-smoothing: antialiased; -webkit-text-size-adjust: none; height: 100%; line-height: 1.6em; width: 100% !important; background-color: #f6f6f6; margin: 0; padding: 0;" bgcolor="#f6f6f6"&gt; &lt;table style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; width: 100%; background-color: #f6f6f6; margin: 0;" bgcolor="#f6f6f6"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0;" valign="top"&gt;&lt;/td&gt; &lt;td width="600" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; display: block !important; max-width: 600px !important; clear: both !important; width: 100% !important; margin: 0 auto; padding: 0;" valign="top"&gt; &lt;div style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; max-width: 600px; display: block; margin: 0 auto; padding: 0;"&gt; &lt;table width="100%" cellpadding="0" cellspacing="0" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; border-radius: 3px; background-color: #fff; margin: 0; border: 1px solid #e9e9e9;" bgcolor="#fff"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 16px; vertical-align: top; color: #fff; font-weight: 500; text-align: center; border-radius: 3px 3px 0 0; background-color: #E6522C; margin: 0; padding: 20px;" align="center" bgcolor="#E6522C" valign="top"&gt; &#123;&#123; .Alerts | len &#125;&#125; alert&#123;&#123; if gt (len .Alerts) 1 &#125;&#125;s&#123;&#123; end &#125;&#125; for &#123;&#123; range .GroupLabels.SortedPairs &#125;&#125; &#123;&#123; .Name &#125;&#125;=&#123;&#123; .Value &#125;&#125; &#123;&#123; end &#125;&#125; &lt;/td&gt; &lt;/tr&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 10px;" valign="top"&gt; &lt;table width="100%" cellpadding="0" cellspacing="0" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;a href="&#123;&#123; template "__alertmanagerURL" . &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; color: #FFF; text-decoration: none; line-height: 2em; font-weight: bold; text-align: center; cursor: pointer; display: inline-block; border-radius: 5px; text-transform: capitalize; background-color: #348eda; margin: 0; border-color: #348eda; border-style: solid; border-width: 10px 20px;"&gt;View in &#123;&#123; template "__alertmanager" . &#125;&#125;&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; if gt (len .Alerts.Firing) 0 &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;[&#123;&#123; .Alerts.Firing | len &#125;&#125;] Firing&lt;/strong&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &#123;&#123; range .Alerts.Firing &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Labels&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Annotations) 0 &#125;&#125;&lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Annotations&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; range .Annotations.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &lt;a href="&#123;&#123; .GeneratorURL &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; color: #348eda; text-decoration: underline; margin: 0;"&gt;Source&lt;/a&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Alerts.Resolved) 0 &#125;&#125; &#123;&#123; if gt (len .Alerts.Firing) 0 &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;hr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;[&#123;&#123; .Alerts.Resolved | len &#125;&#125;] Resolved&lt;/strong&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &#123;&#123; range .Alerts.Resolved &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Labels&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Annotations) 0 &#125;&#125;&lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Annotations&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; range .Annotations.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &lt;a href="&#123;&#123; .GeneratorURL &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; color: #348eda; text-decoration: underline; margin: 0;"&gt;Source&lt;/a&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &lt;/table&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;div style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; width: 100%; clear: both; color: #999; margin: 0; padding: 20px;"&gt; &lt;table width="100%" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 12px; vertical-align: top; text-align: center; color: #999; margin: 0; padding: 0 0 20px;" align="center" valign="top"&gt;&lt;a href="&#123;&#123; .ExternalURL &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 12px; color: #999; text-decoration: underline; margin: 0;"&gt;Sent by &#123;&#123; template "__alertmanager" . &#125;&#125;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0;" valign="top"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; &#123;&#123; end &#125;&#125; &#123;&#123; define "pushover.default.title" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pushover.default.message" &#125;&#125;&#123;&#123; .CommonAnnotations.SortedPairs.Values | join " " &#125;&#125; &#123;&#123; if gt (len .Alerts.Firing) 0 &#125;&#125; Alerts Firing: &#123;&#123; template "__text_alert_list" .Alerts.Firing &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Alerts.Resolved) 0 &#125;&#125; Alerts Resolved: &#123;&#123; template "__text_alert_list" .Alerts.Resolved &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123; define "pushover.default.url" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; slack.tmpl: | &#123;&#123; define "slack.devops.text" &#125;&#125; &#123;&#123;range .Alerts&#125;&#125;&#123;&#123;.Annotations.DESCRIPTION&#125;&#125; &#123;&#123;end&#125;&#125; &#123;&#123; end &#125;&#125;kind: ConfigMapmetadata: creationTimestamp: null name: alertmanager-templates namespace: monitoring 创建 alertmanager 应用1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: alertmanager namespace: monitoringspec: replicas: 1 selector: matchLabels: app: alertmanager template: metadata: name: alertmanager labels: app: alertmanager spec: containers: - name: alertmanager image: quay.io/prometheus/alertmanager:v0.7.1 args: - '-config.file=/etc/alertmanager/config.yml' - '-storage.path=/alertmanager' ports: - name: alertmanager containerPort: 9093 volumeMounts: - name: config-volume mountPath: /etc/alertmanager - name: templates-volume mountPath: /etc/alertmanager-templates - name: alertmanager mountPath: /alertmanager volumes: - name: config-volume configMap: name: alertmanager - name: templates-volume configMap: name: alertmanager-templates - name: alertmanager emptyDir: &#123;&#125; 创建 alertmanager service12345678910111213141516171819apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' prometheus.io/path: '/metrics' labels: name: alertmanager name: alertmanager namespace: monitoringspec: selector: app: alertmanager type: NodePort ports: - name: alertmanager protocol: TCP port: 9093 targetPort: 9093 创建完成后，可以通过alertmanager service的nodeport进入alertmanager 的管理界面，里面有一些设置选项，如下： 具体设置可以参考Prometheus监控 - Alertmanager报警模块。可以通过stop 某个节点的kubelet程序的方式测试一下报警模块是否运行正常。 这样，所有的工作基本完成，接下来就是根据自己的需求进行监控的配置调整，比如grafana面板的管理，报警的具体要求，阈值等。 ——- 更新 ——2017-11-23—–增加ceph监控我司容器云平台后端存储使用的ceph,花了一点时间将ceph的监控转到prometheus上，并找了一个不错的grafana模板，最终效果（实验环境）如下： 简单叙述下具体过程： 部署 ceph-exporterceph-exporter是digitalocean开源的一个获取ceph metric的工具，可以直接在某一台物理机上部署，也可以pod形式部署。我们这里使用pod形式部署，yaml文件如下： ceph-exporter-deployment.yaml1234567891011121314151617181920212223apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ceph-exporter namespace: monitoringspec: replicas: 1 template: metadata: labels: app: ceph-exporter spec: containers: - name: ceph-exporter image: digitalocean/ceph_exporter imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /etc/ceph name: ceph-conf volumes: - hostPath: path: /etc/ceph name: ceph-conf 创建service： ceph-exporter-svc.yaml12345678910111213141516171819---apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: app: ceph-exporter k8s-app: ceph-exporter name: ceph-exporter namespace: monitoringspec: ports: - name: ceph-exporter port: 9128 protocol: TCP targetPort: 9128 selector: app: ceph-exporter 注意：service配置文件中prometheus.io/scrape: ‘true’ ，表示该service可以被prometheus 抓取到。 部署完成后，到prometheus 的target页面查看是否能被prometheus 抓取。 导入 grafana 模板grafana 模板可以直接在官网搜索。这里的模板使用的这个，直接导入对应json文件，注意要将”datasource”改为自己的datasource name,不然会报错。 参考文章kubernetes-prometheus prometheus overview Prometheus入门 Kubernetes 1.6 部署prometheus和grafana 基于Prometheus做多维度的容器监控 用K8S+Prometheus四步搭建Ceph监控 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ops-- ntp设置记录]]></title>
    <url>%2F2017%2F11%2F03%2Fntp-set-record%2F</url>
    <content type="text"><![CDATA[概述简单记录下ntp 的配置过程。有三台机器，系统都是Centos7,如下： 172.16.21.250,作为ntpclient从公网阿里的ntpserver获取时间，同时作为ntpserver为内网的client提供时间服务。 172.16.21.251，作为内网ntpclient。 172.16.21.252，作为内网ntpclient ntp server部署配置 安装rpm包：yum install ntp -y 建议关闭firewalld,如果开启了iptables，要放开udp 的123端口。添加 1-AINPUT -p udp -m state --state NEW -m udp --dport 123 -j ACCEPT 若开启了chronyd服务，关闭并关闭开机自启,chronyd为centos7默认时钟同步工具，会出现冲突。 1systemctl stop chronyd &amp; systemctl disable chronyd 修改配置文件。主要是两个地方，一个是连接它的客户端限制，一个是它连接的服务端设置。 12345678910111213# Permit all access over the loopback interface. This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 172.16.21.0 mask 255.255.255.0 nomodify # 设定客户端的IP限制，且没有更改服务端时间的权限restrict 127.0.0.1 # 这里添加本机host，因为执行ntpq -p 需要loopback.........# 添加阿里的ntpserverserver ntp1.aliyun.com preferserver ntp2.aliyun.comserver ntp3.aliyun.comserver ntp4.aliyun.comserver ntp5.aliyun.com.......... 开启ntpd，并设为开机自启。 1systemctl start ntpd &amp; systemctl enable ntpd 查看服务端是否正常工作： 1234567$ ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*time5.aliyun.co 10.137.38.86 2 u 5 128 377 3.265 1.952 0.763+120.25.115.19 10.137.38.86 2 u 121 128 377 36.172 2.273 0.583-120.25.115.20 10.137.38.86 2 u 126 128 377 38.508 6.644 0.581+time4.aliyun.co 10.137.38.86 2 u 16 128 377 37.158 1.693 0.582 ntp client部署配置 安装rpm包：yum install ntp -y 测试是否安装成功,并获取当前时间，通过hwclock–r写入bios时间 123$ ntpdate 172.16.21.2503 Nov 10:32:13 ntpdate[8008]: adjust time server 172.16.21.250 offset -0.076117 sec$ date; hwclock -r 加入crontab，设置自动同步（每隔三个小时更新一次时间） 123$ crontab -e 0 */1 * * * /usr/sbin/ntpdate 172.16.21.250;/sbin/hwclock -w &gt;/dev/null 2&gt;&amp;1$ crontab –l0 */1 * * * /usr/sbin/ntpdate 172.16.21.250;/sbin/hwclock -w &gt;/dev/null 2&gt;&amp;1 如果时区不对，可以这样修改：12$ timedatectl set-timezone Asia/Shanghai # 修改为北京时间$ timedatectl status #查看是否修改成功 参考文章Centos7.1 for NTP服务器配置 ntpq fails with “timed out, nothing received” and “Request timed out” errors 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Ops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes-- kubernetes 监控指南（一）]]></title>
    <url>%2F2017%2F10%2F29%2Fkubernetes-monitoring-guide-1%2F</url>
    <content type="text"><![CDATA[关于k8s的监控，官方文档有一个系列Kubernetes: a monitoring guide,涵盖了大部分内容，如果觉得费时可以直接参阅这篇文章,本文主要是偏重Heapster &amp; InfluxDB &amp; Grafana 方案的实践。 Heapster &amp; InfluxDB &amp; GrafanaHeapster &amp; InfluxDB &amp; Grafana的组合是kubernetes监控的官方推荐组合，官方dashboard的概览中就用到了这三个工具来展示监控数据。这三个组合中， Heapster 作为metric数据的聚合和处理 InfluxDB是一个时间序列数据库，用来存储Heapster传过来的metric数据 Grafana是一个dashboard的可视化展现工具。 Heapster 介绍Heapster会收集集群中的node，namespace，pod等级别的metric信息，对这些数据聚合之后存储到指定的后端存储系统中。Heapster是通过访问node上的kubelet的API来获取metric数据，而kubelet中聚合了cAdvisor这个工具采集当前节点的所有容器的性能数据。目前Heapster支持的后端数据库包括memory、InfluxDB、BigQuery、 Google Cloud Monitoring 和 Google Cloud Logging等。Heapster收集到的metric数据可以通过restAPI访问，主要是CPU和内存数据，包括集群级别，node级别，namespace级别，pod级别，容器级别的metric数据。 Grafanna 介绍Grafanna是一个比较知名的dashboard可视化工具,可以将时序数据通过检索展现成图标或曲线等形式，通过插件机制支持多种后端数据源，比如InfluxDB, Graphite, Elasticsearch, Prometheus等。 InfluxDB 介绍InfluxDB是基于LevelDB ，为了优化写请求比较多的情况而实现的一种时间序列数据库，每一条数据都带有时间戳属性，主要用于实时数据采集，时间跟踪记录等。 如下图，便是kubernetes监控的总体架构图： 安装部署 首先要有一个k8s集群，然后clone heapster的yaml文件。 1git clone https://github.com/kubernetes/heapster.git 根据yaml文件创建对应的服务，yaml文件有3个，分别对应Heapster &amp; InfluxDB &amp; Grafana，查看yaml文件，其实就是创建了一系列serviceAccount，DeployMent，Service。注：如果是生产环境，注意修改InfluxDB的存储由暂时性volume改为持久volume（将emptyDir: {}换成挂载磁盘或其他存储方式）。 12cd ./heapster/deploy/kube-config/influxdb kubectl create -f ./* 创建完成后，查看对应的pod是否起来了，如果没有起来在排错。对于kubeadm部署的集群，默认是采用tls双向认证，且采用RBAC的授权机制，所以还需要创建rolebinding. 1kubectl create -f ./heapster/deploy/kube-config/rbac/heapster-rbac.yaml 配置参数解析看下heapster的yaml文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546apiVersion: v1kind: ServiceAccountmetadata: name: heapster namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: heapster namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: heapster spec: serviceAccountName: heapster containers: - name: heapster image: gcr.io/google_containers/heapster-amd64:v1.4.0 imagePullPolicy: IfNotPresent command: - /heapster - --source=kubernetes:https://kubernetes.default - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086---apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: Heapster name: heapster namespace: kube-systemspec: ports: - port: 80 targetPort: 8082 selector: k8s-app: heapster 可以看到创建了一个serviceAccount，用于与api-server通信，创建了一个Deployment，拉取相关镜像，可以看到执行heapster命令时，有两个参数，一个source指定metric的来源，https://kubernetes.default就是默认的k8s集群访问地址，可以通过kubectl get service获得对应IP。sink指定要存储的数据地址，这里就是influxDB的地址。最后创建对应的service。其他服务类似。 服务调用与实践这三个服务都有对应的restAPI可以调用，执行kubectl cluster-info 可以获取对应的地址。 可以看出都是走的api-server的端口，应该是api-server调的这三个服务。 如果直接在浏览器里输入路径，会出现认证不通过错误：“User “system:anonymous” cannot proxy services in the namespace “kube-system”.”。这时因为rbac的权限认证导致，可以在浏览器中导入admin证书，以admin的角色进行访问。也可以通过kubectl proxy，在本地访问，也可以设置api-server的insecure-port，通过不安全端口进行访问，因为不安全端口不会进行认证和授权。 通过 insecure-port 访问 heapster 修改api-server 的yaml文件，添加insecure-port,insecure-bind-address 1234567891011121314151617metadata: creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-systemspec: containers: - command: - kube-apiserver - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key .................. - --insecure-port=6444 # 添加不安全端口 - --insecure-bind-address=0.0.0.0 # 绑定不安全端口地址 .................. 由不安全端口构造url,如下示例(关于如何构造url参考使用Heapster获取kubernetes集群对象的metric数据)： 12345678910111213141516171819202122curl http://172.16.21.250:6444/api/v1/proxy/namespaces/kube-system/services/heapster/api/v1/model/namespaces/kube-system/metrics/memory/usage&#123; "metrics": [ &#123; "timestamp": "2017-11-01T15:18:00Z", "value": 1124192256 &#125;, &#123; "timestamp": "2017-11-01T15:19:00Z", "value": 1120612352 &#125;, &#123; "timestamp": "2017-11-01T15:20:00Z", "value": 1123553280 &#125;, &#123; "timestamp": "2017-11-01T15:21:00Z", "value": 1121275904 &#125; ], "latestTimestamp": "2017-11-01T15:21:00Z" &#125; 访问 Grafana 界面Grafana也提供rest api，url 的构建同上，可以参考HTTP API Reference。除此之外，grafana还有一个UI可以访问，在Grafana的yaml文件中添加type：NodePort,然后就可以通过ip:NodePort访问该UI。 通过该UI界面，设置查询SQL，就可以定制自己需要的图表，参考How to Utilize the “Heapster + InfluxDB + Grafana” Stack in Kubernetes for Monitoring Pods 以上就是Heapster &amp; InfluxDB &amp; Grafana 实现k8s监控的内容，这套方案的优点就是部署简单，与k8s结合的很好，缺点也比较明显，heapster是专为k8s设计的，没有通用性，且没有alert机制（当然可以通过设置grafana，但是需要自己做镜像）。除了这一套方案外 ，还有一种广泛使用的方案：prometheus + Grafana，这套方案相对比较通用，除了k8s还可以对接其他系统（比如运行在k8s里面的数据库集群等），且有报警模块alertmanager，缺点就是配置相对比较麻烦，不过为了以后的方便，还是建议采用这种方案。 参考文章Kubernets监控 Heapster+InfluxDB+Grafana Kubernetes: a monitoring guide Monitoring Kubernetes How to Utilize the “Heapster + InfluxDB + Grafana” Stack in Kubernetes for Monitoring Pods 使用Heapster获取kubernetes集群对象的metric数据 UBERNETES 集群监控方案研究 使用Prometheus完成Kubernetes集群监控 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes-- kubernetes scheduler设计以及resource QOS]]></title>
    <url>%2F2017%2F10%2F26%2Fkubernetes-qos%2F</url>
    <content type="text"><![CDATA[kubernetes scheduler 设计kubernetes最主要的功能是容器编排，而容器编排中第一个想到的便是新建一个容器时，需要将该容器调度到哪一个节点上，其实跟openstack中的nova-scheduler很像。看下我们设计sheduler时需要满足的需求： 从用户角度，需要满足指定节点，与指定pod亲和/反亲和（例如同为cpu密集型的服务不要放在同一节点），服务分散 从集群的角度，需要满足资源平衡，利用率高等 看下kubernetes中大致的调度过程，图片来自Kubernetes中的资源管理 ： 再看下调度器的具体调度算法，大致分了两类，predicate和prioritizer： predicate:一系列过滤函数将不满足的node排除 prioritizer: 对通过的节点按照一系列的算法进行优先级排序，最后选择优先级最高的。 简单介绍下常用的predicate 的几个算法： PodFitsResources：节点上剩余的资源是否大于 pod 请求的资源 PodFitsHost：如果 pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配 PodFitsHostPorts：节点上已经使用的 port 是否和 pod 申请的 port 冲突 PodSelectorMatches：过滤掉和 pod 指定的 label 不匹配的节点 NoDiskConflict：已经 mount 的 volume 和 pod 指定的 volume 不冲突，除非它们都是只读 prioritizer的常用算法： LeastRequestedPriority：将Pod部署到剩余CPU和内存最多的Node上，这里的request就是kubernetes的QOS中定义的request。 BalancedResourceAllocation：节点上 CPU 和 Memory 使用率越接近，权重越高。这个应该和上面的一起使用，不应该单独使用 ImageLocalityPriority：倾向于已经有要使用镜像的节点，镜像总大小值越大，权重越高 NodeLabelPriority ：检查Node是否存在Pod需要的Label，有则10分，没有则0分 Spreading：同一个Service下的Pod尽可能的分散在集群中。Node上运行的通Service下的Pod数目越少，分数越高。 Anti-Affinity：与Spreading类似，但是分散在拥有特定Label的所有Node中，包括Node Affinity/Anti-Affinity和Pod Affinity/Anti-Affinity。 此外，1.6之后还有Taints and Tolerations等scheduler策略，当然，也可以定制，然后创建pod时指定schedulername,参考Advanced Scheduling in Kubernetes。 kubernetes resource Quality of Service(QOS)kubernetes中的QOS实现是通过request和limit两个概念,主要包括两个具体的指标：CPU和内存。request可以理解为pod的下限，即最少分配给该pod的资源，limit则相应的是分配资源的上限。对于每一个资源，container可以指定具体的资源需求（requests）和限制（limits），requests申请范围是0到node节点的最大配置，而limits申请范围是requests到无限，即0 &lt;= requests &lt;=Node Allocatable, requests &lt;= limits &lt;= Infinity。对于CPU，如果pod中服务使用CPU超过设置的limits，pod不会被kill掉但会被限制。如果没有设置limits，pod可以使用全部空闲的cpu资源。对于内存，当一个pod使用内存超过了设置的limits，pod中container的进程会被kernel因OOM kill掉。当container因为OOM被kill掉时，系统倾向于在其原所在的机器上重启该container或本机或其他重新创建一个pod。在Kubernetes中，pod的QoS级别分为以下三种：Guaranteed, Burstable与 Best-Effort。 Guaranteed满足条件：pod中所有容器都必须统一设置limits，并且设置参数都一致，如果有一个容器要设置requests，那么所有容器都要设置，并设置参数同limits一致。如果一个容器只指明limit而未设定request，则request的值等于limit值。 Guaranteed举例1：容器只指明了limits而未指明requests。 1234567891011containers:name: fooresources: limits: cpu: 10m memory: 1Giname: barresources: limits: cpu: 100m memory: 100Mi Guaranteed举例2：requests与limit均指定且值相等。123456789101112131415161718containers:name: fooresources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Giname: barresources: limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi Best-Effort满足条件：Pod中所有容器的所有Resource的request和limit都没有赋值。 示例：12345containers: name: foo resources: name: bar resources: Burstable满足条件：pod中只要有一个容器的requests和limits的设置不相同，满足“0&lt;request&lt;limit&lt;∞” 。 示例,Container bar没有指定resources： 1234567891011containers: name: foo resources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Gi name: bar 3种QoS优先级从有低到高（从左向右）：Best-Effort pods -&gt; Burstable pods -&gt; Guaranteed pods 也就是说，QoS pods被kill掉场景与顺序如下： Best-Effort 类型的pods：系统用完了全部内存时，该类型pods会最先被kill掉。 Burstable类型pods：系统用完了全部内存，且没有Best-Effort container可以被kill时，该类型pods会被kill掉。 Guaranteed pods：系统用完了全部内存、且没有Burstable与Best-Effort container可以被kill，该类型的pods会被kill掉。注：如果pod进程因使用超过预先设定的limites而非Node资源紧张情况，系统倾向于在其原所在的机器上重启该container或本机或其他重新创建一个pod。 具体到k8s的配置中，大致要配置两个地方，一个是namesapce中所有容器的cpu/内存的总和request和limit，另一个是namespace中默认的容器cpu/内存的request，limit值(在容器创建时没有指定request，limit时使用默认值)。这两类资源一个叫做ResourceQuota，一个叫做LimitRange ，参考示例： 12345678910111213apiVersion: v1kind: LimitRangemetadata: name: mem-limit-rangespec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container 以上为一个LimitRange的yaml文件，指定namespace并创建：1kubectl create -f limitrange-defaults.yaml --namespace=default 当在创建pod的时候，如果没有指定具体的request和limit，就会按照默认值。 再创建一个ResourceQuota：1234567891011121314apiVersion: v1kind: ResourceQuotametadata: name: mem-cpu-demospec: hard: requests.cpu: "1" requests.memory: 1Gi limits.cpu: "2" limits.memory: 2Gi pods: "10" persistentvolumeclaims: "1" services.loadbalancers: "2" services.nodeports: "10" 同样，指定namespace并创建，1kubectl create -f resourcequota-default.yaml --namespace=default 由配置文件可以看出，除了指定cpu/内存的request和limit，还可以指定pod数量以及其他一些api resource的数量。更多示例参考Manage Memory, CPU, and API Resources 参考文章Kubernetes中的资源管理 kubernetes 简介：调度器和调度算法 How does the Kubernetes scheduler work Kubernetes计算资源管理–requests和limits Kubernetes之服务质量保证（QoS） Configure Quality of Service for Pods resource-qos.md Manage Memory, CPU, and API Resources 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask-- SQLAlchemy 与 flask-sqlalchemy记录]]></title>
    <url>%2F2017%2F10%2F24%2FSQLAlchemy-%26%26-flask-sqlalchemy%2F</url>
    <content type="text"><![CDATA[SQLAchemy在flask开发中，涉及到数据库的交互，一般都会想到SQLAchemy。SQLAchemy是一个python的SQL工具集，它实现了ORM,类似于Java web开发中的hibernete。 flask-sqlachemy参考文章OpenNebula vs. OpenStack: User Needs vs. Vendor Driven Comparing OpenNebula and OpenStack: Two Different Views on the Cloud OpenNebula 4.14 Hands-on Tutorial 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack-- OpenStack 与OpenNebula的前世今生]]></title>
    <url>%2F2017%2F09%2F12%2FopenNebula-vs-openstack%2F</url>
    <content type="text"><![CDATA[引出本篇博文说实话感觉滞后太多，因为关于云计算开源框架的争论在前几年还甚嚣尘上，自从openstack风卷残云之势席卷整个云计算圈之后，之前的一些老前辈貌似都默默退出了，这也包括本文的一个主角OpenNebula。其实OpenNebula一直都在，只不过OpenStack的名气太大，以至于其他框架都被无意间忽略了。不过随着OpenStack项目的飞速膨胀，复杂度与可维护性也随之疯涨，OpenNebula这个一直秉承“Simplicity,Flexibility”的框架也得到了不少人的拥趸，本文就主要讲讲这个本应在几年前讨论的话题：OpenStack 与OpenNebula，孰优孰劣。 OpenStack VS OpenNebula定位的区别Openstack从一开始就是打算跟AWS正面刚，所以一直借鉴（抄袭）AWS,可以认为是走公有云（提供基础设施）路线的，而OpenNebula则是走企业云（数据中心虚拟化）路线的，是打算跟vmware干仗的。但随着各个厂商的站队，OpenStack突然红了，但是红了也是有代价的，OpenStack的发展方向开始被Foundation控制，而这个Foundation是由各大厂商（以red hat为首）控制的，所以OpenStack其实是面向这些巨头们的，当然其中不乏各大公司的博弈，君不见早期launchpad上多个顽疾似的bug一直无人问津，而各大厂商的适配driver却早已开发完善。OpenNebula一直有一个核心开发组织掌握着开发的整体方向，相对于OpenStack,”铜臭味”没那么重，但也因为没有巨头的摇旗呐喊，所以开发进度不是很快。所以，按照OpenNebula开发者的话说：Openstack是服务于foundation的，而OpenNebula才是真正服务于用户的。 整体架构的区别这个区别对于技术人员来说就更感兴趣了。因为刚开始的定位不同，自然架构也是天壤之别，先上两张图直观感受下： 一张图就可以大致了解到，如今的OpenStack俨然是一个庞然大物，不过好在各个子项目分工明确，划分成层之后如下图（图比较老了，有些项目已经毕业了）： 再看下具体部署情况： openstack的典型部署架构： opennebula的典型部署架构： 可以看出，在计算节点openstack要部署很多agent(包括nova-compute)，而opennebula只要保证可以SSH连接以及有hypervisor就可以，是一种无侵入式的设计。 支持服务的区别网上一张截图（时效性不保证）： 总结Openstack因为各大厂商力推的原因，走的是大而全的路线，你能想到的云计算服务，openstack基本都有对应的子项目，但也造成了架构复杂，学习路线陡峭，后期维护成本高等现象。OpenNebula走的是小而美的路线，无侵入式设计，架构简单，容易上手，但是因为保证无侵入式，所以要大量调用shell，动用的编程语言包括C++,ruby，shell等，项目代码相对混乱，代码可读性不高。综上所述，如果是建一个求稳定性，且物理环境一致，规模相对较大的私有云，Openstack是首选，而如果计算节点系统不一致，特别是计算节点有公有云的情况(也就是搭建混合云)，可以考虑OpenNebula。 参考文章OpenNebula vs. OpenStack: User Needs vs. Vendor Driven Comparing OpenNebula and OpenStack: Two Different Views on the Cloud OpenNebula 4.14 Hands-on Tutorial 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes-- 手动安装高可用k8s1.6问题汇总]]></title>
    <url>%2F2017%2F08%2F22%2Finstall-k8s.1.6.3-ha-version%2F</url>
    <content type="text"><![CDATA[kubernetes HA 整体架构k8s的HA相对于openstack的HA要简单很多，主要包括以下三各方面： etcd的HA:创建HA集群,如果还不放心，可以使用分布式存储系统 apiserver(无状态服务)的HA:双活模式，前面加一个load balance controller manager 和 scheduler的HA:主备模式。 博主是按照Kubernetes 1.6 高可用集群部署这篇博客一步步安装下来的，基于Kubernetes二进制包手动部署一个高可用的Kubernetes 1.6集群，将启用ApiServer的TLS双向认证和RBAC授权等安全机制，当然，也可以利用kubeadm创建一个k8s cluster后再扩展成高可用的，参考一步步打造基于Kubeadm的高可用Kubernetes集群。安装过程不再赘述，参考上述博文，主要讲下期间遇到的问题以及解决方法。 etcd高可用集群部署在该过程中碰到的一个坑是，在创建tls密钥和证书的过程中。用的工具是cfssl,创建etcd证书签名请求配置文件的时候需要指定node节点的IP，当时希望可以外网访问，就使用的floating-ip,在etcd的systemd unit文件中自然也就使用了floating-ip，然而etcd却一直起不来，查看日志报错： listen tcp 172.16.21.55:2380: bind: cannot assign requested address. 监听端口失败，恍然大悟，openstack中的floating-ip是不会在虚拟机上创建一个新网卡的，而是通过l3-agent的转发实现。将floating-ip更改为内网IP 后问题解决。 Kubernetes各组件TLS证书和密钥除了etcd是使用tls双向认证外，这里apiserver也启用了双向认证，这样的话，凡是与apiserver通信的组件都要创建对应的证书与密钥,所有的需要创建的组件包括： kube-apiserver kubernetes-admin：RBAC相关，该证书拥有访问kube-apiserver的所有权限。 kube-controller-manager kube-scheduler kubelet:每个节点都要配置 kube-proxy：每个节点都要配置 Kubernetes Master集群部署我们这里用openstack创建了一个load balance用于apiserver的高可用，如果是在裸机上，也有多种方案，比如，可以使用haproxy+keepalived的方案实现，也可以直接在各节点用nginx做反向代理，参考kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明。 Kubernetes Node节点部署在部署Pod Network插件flannel碰到一个问题，执行完create命令后，pod一直处于containercreating状态，describe发现报错信息为：error syncing pod，没有多余的报错信息，而且容器压根就没有创建起来。挣扎了许久，最后直接在/var/log/messaging中暴力搜索了一下syncing pod，终于找到有用的信息：12.....Aug 22 13:12:33 host-10-10-10-52 dockerd: time="2017-08-22T13:12:33.841538881+08:00" level=error msg="Handler for GET /v1.24/images/gcr.io/google_containers/pause-amd64:3.0/json returned error: No such image: gcr.io/google_containers/pause-amd64:3.0" 通过翻墙将该镜像pull到本地后，一切解决。关于pause container的作用，直接引用What is the role of ‘pause’ container? The pause container is a container which holds the network namespace for the pod. It does nothing ‘useful’. (It’s actually just a little bit of assembly that goes to sleep and never wakes up)This means that your ‘apache’ container can die, and come back to life, and all of the network setup will still be there. Normally if the last process in a network namespace dies the namespace would be destroyed and creating a new apache container would require creating all new network setup. With pause, you’ll always have that one last thing in the namespace. 参考文章Building High-Availability Clusters Kubernetes 1.6 高可用集群部署 ssl 双向认证和单向认证原理 kubernetes(k8s) 1.7.3 calico网络和Master ha安装说明 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes-- 漫谈kubernetes 中的认证 & 授权 & 准入机制]]></title>
    <url>%2F2017%2F08%2F17%2Fkubernetes-authentication-authorization-admission-control%2F</url>
    <content type="text"><![CDATA[概览首先需要了解这三种机制的区别：简单来说，认证(Authenticating)是对客户端的认证，通俗点就是用户名密码验证，授权(Authorization)是对资源的授权，k8s中的资源无非是容器，最终其实就是容器的计算，网络，存储资源，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。准入(Admission Control)机制是一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。在k8s中，这三种机制如下图： k8s的整体架构也是一个微服务的架构，所有的请求都是通过一个GateWay，也就是kube-apiserver这个组件（对外提供REST服务），由图中可以看出，k8s中客户端有两类，一种是普通用户，一种是集群内的Pod，这两种客户端的认证机制略有不同，后文会详述。但无论是哪一种，都需要依次经过认证，授权，准入这三个机制。 kubernetes 中的认证机制需要注意的是，kubernetes虽然提供了多种认证机制，但并没有提供user 实体信息的存储，也就是说，账户体系需要我们自己去做维护。当然，也可以接入第三方账户体系（如谷歌账户），也可以使用开源的keystone去做整合。kubernetes 支持多种认证机制，可以配置成多个认证体制共存，这样，只要有一个认证通过，这个request就认证通过了。下面介绍下官网列举的几种常见认证机制： X509 Client Certs也叫作双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。默认在kubeadm创建的集群中是enabled的，可以在master node上查看kube-apiserver的pod配置文件： 1234567891011121314151617181920212223# cat /etc/kubernetes/manifests/kube-apiserver.json.................containers": [ &#123; "name": "kube-apiserver", "image": "gcr.io/google_containers/kube-apiserver-amd64:v1.5.2", "command": [ "kube-apiserver", "--insecure-bind-address=127.0.0.1", "--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota", "--service-cluster-ip-range=10.96.0.0/12", "--service-account-key-file=/etc/kubernetes/pki/apiserver-key.pem", "--client-ca-file=/etc/kubernetes/pki/ca.pem", "--tls-cert-file=/etc/kubernetes/pki/apiserver.pem", "--tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem", "--token-auth-file=/etc/kubernetes/pki/tokens.csv", "--secure-port=6443", "--allow-privileged", "--advertise-address=192.168.61.100", "--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname", "--anonymous-auth=false", "--etcd-servers=http://127.0.0.1:2379" ], 相关的三个启动参数： client-ca-file: 指定CA根证书文件为/etc/kubernetes/pki/ca.pem，内置CA公钥用于验证某证书是否是CA签发的证书 tls-private-key-file: 指定ApiServer私钥文件为/etc/kubernetes/pki/apiserver-key.pem tls-cert-file：指定ApiServer证书文件为/etc/kubernetes/pki/apiserver.pem 只要有这三个启动参数，就说明开启了https的认证方式，这时，如果在集群外访问 https://masterIP:6443/api 会提示Unauthorized，只有在客户端配置相关认证才可以访问,客户端的认证证书生成与操作可以参考Creating Certificates。证书的生成是kubeadm使用openssl自动生成的，如果是手动配置双向认证，相对比较麻烦，主要配置流程如下： 生成根证书、API Server服务端证书、服务端私钥、各个组件所用的客户端证书和客户端私钥。 修改 Kubernetes 各个服务进程的启动参数，启用双向认证模式. 详细配置可以参考Kubernetes集群安全配置案例 注意，在启动参数中还有一个参数：–insecure-bind-address=127.0.0.1，这个参数主要用与master node上的其他核心组件，比如kube-scheduler，kube-controller-manager通过masterIP:8080与APIserver直接通信，而不用通过双向认证。这一点可以从他们的启动参数–master=127.0.0.1:8080看出。 Static Token File静态token文件认证，同样，在kubeadm创建的集群中也是默认enabled的，比如，上面的apiserver启动参数中，我们可以看到有参数 ：–token-auth-file=/etc/kubernetes/pki/tokens.csv ，这个静态token文件的格式为 token,user,uid,”group1,group2,group3”，如下示例： 12# cat /etc/kubernetes/pki/tokens.csv7db2f1c02d721320,kubeadm-node-csr,0615e0ac-7d70-11e7-ad94-fa163eb9dfdd,system:kubelet-bootstrap 客户端请求的时候需要在http header中加入：”Authorization: Bearer THETOKEN”，如下实例： 1curl -k --header "Authorization: Bearer 7db2f1c02d721320" https://192.168.21.34:6443/api 或者使用brctl: 1234kubectl --server=https://192.168.21.34:6443 \--token=7db2f1c02d721320 \--insecure-skip-tls-verify=true \cluster-info 注意，如果该静态token文件更改的话，需要重启apiserver。 Bootstrap Tokensbootstrap token认证目前处于alpha阶段，目前主要是kubeadm创建k8s集群时使用。使用这种认证方式，k8s会动态的管理一种type为bootstrap token的token，这些token作为secret放在kube-system namespace中。controller-manager中的tokencleaner controller会在bootstrap token 过期时进行删除。使用这种认证方式，apiserver的启动参数中需要有–experimental-bootstrap-token-auth，Controller Manager的启动参数中有–controllers=*,tokencleaner 类似参数。 Static Password File比较简单，kubeadm默认没有开启，生产环境也不建议使用。apiserver启动参数指定–basic_auth_file=/etc/kubernetes/basic_auth。然后在指定的文件中加入用户名密码等就可以了，文件格式为password,user,uid,”group1,group2,group3”。 Service Account TokensService Account Token 是一种比较特殊的认证机制，适用于上文中提到的pod内部服务需要访问apiserver的认证情况，默认enabled。还是看上文中apiserver 的启动配置参数有–service-account-key-file=/etc/kubernetes/pki/apiserver-key.pem，如果没有指明文件，默认使用–tls-private-key-file的值，即API Server的私钥。service accout本身是作为一种资源在k8s集群中，我们可以通过命令行获取： 1234567[root@k8s-master pki]# kubectl get serviceaccount --all-namespacesNAMESPACE NAME SECRETS AGEdefault default 1 7dfor-test default 1 3dkube-system default 1 7dkube-system weave-net 1 7dsock-shop default 1 7d 可以看到k8s集群为所有的namespace创建了一个默认的service account，利用命令describe会发现service account只是关联了一个secret作为token，也就是service-account-token。 123456789101112131415161718192021222324252627282930[root@k8s-master pki]# kubectl describe serviceaccount/default -n kube-systemName: defaultNamespace: kube-systemLabels: &lt;none&gt;Image pull secrets: &lt;none&gt;Mountable secrets: default-token-nbldrTokens: default-token-nbldr[root@k8s-master pki]# kubectl get secret default-token-nbldr -o yaml -n kube-systemapiVersion: v1data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFM0 ........................略.................... namespace: a3ViZS1zeXN0ZW0= token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0..................................kind: Secretmetadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 67aae699-7d70-11e7-a8a9-fa163eb9dfdd creationTimestamp: 2017-08-10T02:05:38Z name: default-token-nbldr namespace: kube-system resourceVersion: "88" selfLink: /api/v1/namespaces/kube-system/secrets/default-token-nbldr uid: 67b20b73-7d70-11e7-a8a9-fa163eb9dfddtype: kubernetes.io/service-account-token 可以看到service-account-token的secret资源包含的数据有三部分： ca.crt，这是API Server的CA公钥证书，用于Pod中的Process对API Server的服务端数字证书进行校验时使用的； namespace，这是Secret所在namespace的值的base64编码：# echo -n “kube-system”|base64 =&gt; “a3ViZS1zeXN0ZW0=” token：该token就是由service-account-key-file的值签署(sign)生成。 这种认证方式主要由k8s集群自己管理，用户用到的情况比较少。我们创建一个pod时，默认就会将该namespace对应的默认service account token mount到Pod中，所以无需我们操作便可以直接与apiserver通信，相关示例参考在Kubernetes Pod中使用Service Account访问API Server，当然也可以指定多个service account token,参考Configure Service Accounts for Pods。 OpenID Connect Tokens类似 OAuth2的认证方式，大致认证过程如下： 除了以上几种认证方式外，还有几种比如Webhook Token Authentication，Keystone Password等，详情见官网。 kubernetes 中的授权机制k8s中的授权策略也支持开启多个授权插件，只要一个验证通过即可。k8s授权处理主要是根据以下请求属性： user, group, extra API、请求方法（如get、post、update、patch和delete）和请求路径（如/api） 请求资源和子资源 Namespace API Group 目前k8s支持的授权模式主要有以下几种： Node Authorization ABAC Authorization RBAC Authorization Webhook Authorization Node Authorization1.7+版本才release的一种授权机制，通过配合NodeRestriction control准入控制插件来限制kubelet访问node，endpoint、pod、service以及secret、configmap、PV和PVC等相关的资源。配置方式为：–authorization-mode=Node,RBAC –admission-control=…,NodeRestriction,… ABAC AuthorizationABAC(Attribute-based access control),使用这种模式需要配置参数：–authorization-mode=ABAC –authorization-policy-file=SOME_FILENAME。这种模式的实现相对比较生硬，就是在master node保存一份policy文件，指定不用用户（或用户组）对不同资源的访问权限,当修改该文件后，需要重启apiserver,跟openstack 的ABAC类似。policy文件的格式如下： 123456789101112131415161718192021222324252627282930313233# Alice can do anything to all resources:&#123; "apiVersion": "abac.authorization.kubernetes.io/v1beta1", "kind": "Policy", "spec": &#123; "user": "alice", "namespace": "*", "resource": "*", "apiGroup": "*" &#125;&#125;# Kubelet can read any pods:&#123; "apiVersion": "abac.authorization.kubernetes.io/v1beta1", "kind": "Policy", "spec": &#123; "user": "kubelet", "namespace": "*", "resource": "pods", "readonly": true &#125;&#125;# Kubelet can read and write events:&#123; "apiVersion": "abac.authorization.kubernetes.io/v1beta1", "kind": "Policy", "spec": &#123; "user": "kubelet", "namespace": "*", "resource": "events" &#125;&#125; 使用这种模式需要配置参数：–authorization-mode=ABAC –authorization-policy-file=SOME_FILENAME RBAC AuthorizationRBAC（Role-Based Access Control）依然处于Beta阶段，通过启动参数–authorization-mode=RBAC，使用kubeadm安装k8s默认会enabled。RBAC API定义了四个资源对象用于描述RBAC中用户和资源之间的连接权限： Role ClusterRole RoleBinding ClusterRoleBinding Role是定义在某个Namespace下的资源，在这个具体的Namespace下使用。 ClusterRole与Role相似，只是ClusterRole是整个集群范围内使用的。RoleBinding把Role绑定到账户主体Subject，让Subject继承Role所在namespace下的权限。 ClusterRoleBinding把ClusterRole绑定到Subject，让Subject集成ClusterRole在整个集群中的权限。 我们可以通过kubectl命令获取对应的Role相关资源进行增删改查：1234567kubectl get roles --all-namespaceskubectl get ClusterRoleskubectl get rolebinding --all-namespaceskubectl get clusterrolebinding API Server已经创建一系列ClusterRole和ClusterRoleBinding。这些资源对象中名称以system:开头的，表示这个资源对象属于Kubernetes系统基础设施。 也就说RBAC默认的集群角色已经完成足够的覆盖，让集群可以完全在 RBAC的管理下运行。 修改这些资源对象可能会引起未知的后果，例如对于system:node这个ClusterRole定义了kubelet进程的权限，如果这个角色被修改，可能导致kubelet无法工作。 Webhook Authorization用户在外部提供 HTTPS 授权服务，然后配置 apiserver 调用该服务去进行授权。apiserver配置参数：–authorization-webhook-config-file=SOME_FILENAME配置文件的格式跟kubeconfig的格式类似，具体参考官方文档 kubernetes 中的准入机制Kubernetes的Admission Control实际上是一个准入控制器(Admission Controller)插件列表，发送到APIServer的请求都需要经过这个列表中的每个准入控制器插件的检查，如果某一个控制器插件准入失败，就准入失败。控制器插件如下： AlwaysAdmit：允许所有请求通过 AlwaysPullImages：在启动容器之前总是去下载镜像，相当于每当容器启动前做一次用于是否有权使用该容器镜像的检查 AlwaysDeny：禁止所有请求通过，用于测试 DenyEscalatingExec：拒绝exec和attach命令到有升级特权的Pod的终端用户访问。如果集中包含升级特权的容器，而要限制终端用户在这些容器中执行命令的能力，推荐使用此插件 ImagePolicyWebhook ServiceAccount：这个插件实现了serviceAccounts等等自动化，如果使用ServiceAccount对象，强烈推荐使用这个插件 SecurityContextDeny：将Pod定义中定义了的SecurityContext选项全部失效。SecurityContext包含在容器中定义了操作系统级别的安全选型如fsGroup，selinux等选项 ResourceQuota：用于namespace上的配额管理，它会观察进入的请求，确保在namespace上的配额不超标。推荐将这个插件放到准入控制器列表的最后一个。ResourceQuota准入控制器既可以限制某个namespace中创建资源的数量，又可以限制某个namespace中被Pod请求的资源总量。ResourceQuota准入控制器和ResourceQuota资源对象一起可以实现资源配额管理。 LimitRanger：用于Pod和容器上的配额管理，它会观察进入的请求，确保Pod和容器上的配额不会超标。准入控制器LimitRanger和资源对象LimitRange一起实现资源限制管理 NamespaceLifecycle：当一个请求是在一个不存在的namespace下创建资源对象时，该请求会被拒绝。当删除一个namespace时，将会删除该namespace下的所有资源对象 DefaultStorageClass DefaultTolerationSeconds PodSecurityPolicy 当Kubernetes版本&gt;=1.6.0，官方建议使用这些插件：–admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds当Kubernetes版本&gt;=1.4.0，官方建议使用这些插件：–admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota以上是标准的准入插件，如果是自己定制的话，k8s1.7版 出了两个alpha features, Initializers 和 External Admission Webhooks，详情可以参考Dynamic Admission Control. 参考文章Authenticating authorization Kubernetes集群安全：Api Server认证 Kubernetes集群安全：准入控制Admission Control Kubernetes 1.6新特性学习：RBAC授权 在Kubernetes Pod中使用Service Account访问API Server kubernetes安全控制认证与授权(二) 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernete-- 利用kubeadm 搭建一个kubernate集群]]></title>
    <url>%2F2017%2F08%2F14%2Finstall-kubernete-by-kubreadm%2F</url>
    <content type="text"><![CDATA[目标 利用 kubeadm 搭建一个四节点的k8s测试集群 利用harbor搭建一个单节点的私有镜像仓库 k8s集群与私有镜像仓库整合 部署dashboard 前期准备准备以下5个节点，一个为k8s的master节点，3个为node节点，最后一个作为私有仓库镜像，系统为centos7.2： 注：k8s的安装方式有很多，kubeadm安装方式是独立节点安装的官方推荐方式，简单可重复，但不适用于生产环境，因为没有做HA，不过可以在安装完之后继续优化做HA，参考一步步打造基于Kubeadm的高可用Kubernetes集群,后续会跟进这一块。 kubernete 集群安装k8s所有节点需要执行的操作所有节点都要安装以下组件： 1234docker：容器运行时，被Kubernetes依赖kubelet：Kubernetes核心组件，运行在集群中的所有节点上，用来启动容器和podskubectl：命令行工具，k8s客户端，用来控制集群，只需要安装到kube-master上,当然，也可以安装到其他节点，然后配置指定master。kubeadm：集群安装工具 首先，安装docker,k8s官方建议版本为1.12，1.13以及17.03+版本还没有测试。所以这里也安装1.12版本。 12345678910111213141516tee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOFsetenforce 0yum update -y yum install -y docker-engine-1.12.6 docker-engine-selinux-1.12.6systemctl enable docker &amp;&amp; systemctl start docker 注：这里有个小坑，就是k8s dashboard在某些版本RH内核下会启动失败，参考issue。 接下来，安装kubectl, kubelet, kubeadm以及一些依赖包。 先把依赖包装上：1yum install -y ebtables socat kubectl 的安装比较简单，参考Install and Set Up kubectl,可以直接下载可执行文件然后添加权限，扔到master节点的/usr/local/bin/目录下即可，注意版本要与k8s版本匹配(注：也可以直接在下文同其他三个组件一起rpm包安装)。 12345curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectlchmod +x ./kubectlsudo mv ./kubectl /usr/local/bin/kubectl 因为kubelet, kubeadm的rpm安装包在gce上，需要翻墙。 如果服务器可以翻墙，可以直接通过yum命令安装：12345678910111213cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet kubeadmsystemctl enable kubelet &amp;&amp; systemctl start kubelet 如果不能翻墙，只能先下载下来，然后安装，需要安装的rpm包url地址可以在这个网页中找到：1curl https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/primary.xml 我们这里只需要安装三个rpm包，kubeadm, kubelet以及kubernetes-cni，可以直接搜索上面的网页然后找到合适版本的rpm包。我们这里安装最新版本1.7.3,对应的地址如下： 12345https://packages.cloud.google.com/yum/pool/f7ec56b0f36a81c0f91bcf26e05f23088082b468b77dac576dc505444dd8cd48-kubeadm-1.7.3-1.x86_64.rpmhttps://packages.cloud.google.com/yum/pool/28b76e6e1c2ec397a9b6111045316a0943da73dd5602ee8e53752cdca62409e6-kubelet-1.7.3-1.x86_64.rpmhttps://packages.cloud.google.com/yum/pool/e7a4403227dd24036f3b0615663a371c4e07a95be5fee53505e647fd8ae58aa6-kubernetes-cni-0.5.1-0.x86_64.rpm 将这三个rpm包打包上传到四个节点上，并安装： 123456789101112tar xzvf /tmp/kubernetes-el7-x86_64.tar.gzkubernetes-el7-x86_64/kubernetes-el7-x86_64/567600102f687e0f27bd1fd3d8211ec1cb12e71742221526bb4e14a412f4fdb5-kubernetes-cni-0.5.0.1-0.07a8a2.x86_64.rpmkubernetes-el7-x86_64/5612db97409141d7fd839e734d9ad3864dcc16a630b2a91c312589a0a0d960d0-kubeadm-1.6.0-0.alpha.0.2074.a092d8e0f95f52.x86_64.rpmkubernetes-el7-x86_64/8a299eb1db946b2bdf01c5d5c58ef959e7a9d9a0dd706e570028ebb14d48c42e-kubelet-1.5.1-0.x86_64.rpmkubernetes-el7-x86_64/93af9d0fbd67365fa5bf3f85e3d36060138a62ab77e133e35f6cadc1fdc15299-kubectl-1.5.1-0.x86_64.rpmcd kubernetes-el7-x86_64/rpm -ivh *systemctl enable kubelet &amp;&amp; systemctl start kubelet 接下来开始基于Kubeadm 创建k8s集群，不过在开始之前，我们先准备下需要用到的镜像，因为kubeadm创建的k8s集群中的kub-api, kube-scheduler, kube-proxy, kube-controller-manager,etcd等服务都是直接拉取镜像跑在k8s集群中，为了避免安装过程中下载镜像浪费太多时间，这里先把镜像下载好。各个版本需要下载的镜像版本也不一样。参考如下： 我们直接用的最新版1.7.3，如果服务器可以翻墙，直接拉取镜像： 1234images=(kube-proxy-amd64:v1.7.3 kube-scheduler-amd64:v1.7.3 kube-controller-manager-amd64:v1.7.3 kube-apiserver-amd64:v1.7.3 etcd-amd64:3.0.17 k8s-dns-sidecar-amd64:1.14.4 pause-amd64:3.0 k8s-dns-kube-dns-amd64:1.14.4 k8s-dns-dnsmasq-nanny-amd64:1.14.4)for imageName in $&#123;images[@]&#125; ; do docker pull gcr.io/google_containers/$imageNamedone 如果不能翻墙，可以先翻墙下载下来，然后push到dockerhub上，再pull下来,注意pull下来之后，还是要更改tag为gcr.io/google_containers/$imageName形式。 master 节点安装在master节点执行： 1kubeadm init 执行完成后，会输出一个token，node节点安装时会用到。这里有一个小坑：该过程一直卡在“[apiclient] Created API client, waiting for the control plane to become ready” ，可以去message里找相关log，一般是两种情况导致，一种是用了proxy，一种是cgroup-driver配置错误，我这边有一次是因为下载的镜像不对，kubeadm默认应该是安转最新版本，比如kubeadm1.6.x会安装1.6.9的相关组件（api-server-1.6.9.controller-manager-1.6.9等），而kubeadm1.7.x会默认安装1.7.x里面的最高版本（此时是1.7.4），所以要下载合适版本的镜像。 node 节点安装在各node节点执行： 1kubeadm join --token=976234.e91451d4305bc282 172.16.21.53 全部执行完成后，在master节点验证： 123456[root@k8s-master ~# kubectl get nodesNAME STATUS AGEk8s-master.novalocal Ready,master 4dk8s-node1.novalocal Ready 4dk8s-node2.novalocal Ready 4dk8s-node3.novalocal Ready 4d 部署pod网络这里选择Weave Net。 1[root@k8s-master ~]# kubectl apply -f https://git.io/weave-kube 等待一段时间，利用下列命令查看部署情况。 1[root@k8s-master ~]# kubectl get pods --all-namespaces 部署sock-shop微服务demo12[root@k8s-master ~]# kubectl create namespace sock-shop[root@k8s-master ~]# kubectl apply -n sock-shop -f "https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true" 查看服务部署情况：1[root@k8s-master ~]# kubectl get pods -n sock-shop 访问172.16.21.34:30001验证。 Harbor 安装比较简单，参考harbor doc。 注意：docker 默认连接镜像使用https，而harbor默认安装是走的http，所以需要修改/etc/docker/daemon.json，添加 1234&#123; "registry-mirrors": ["&lt;your accelerate address&gt;"], "insecure-registries": ["172.16.21.44"]&#125; k8s 添加私有镜像官方给出了三种解决方案： 在node节点配置私有镜像的认证登录文件，其实相当于在node本地执行docker login后，在/.docker目录下生成的一个config.json文件。 12345678# cat ~/.docker/config.json&#123; "auths": &#123; "registry.cn-hangzhou.aliyuncs.com/xxxx/rbd-rest-api": &#123; "auth": "xxxxyyyyzzzz" #一个base64编码结果，不太安全 &#125; &#125;&#125; 这种方法比较繁琐，而且不安全，不推荐。 利用kubectl创建docker-registry的secret 123kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAILkubectl get secret --all-namespaces #查看创建的secret 在写dockerfile的时候指定imagePullSecrets即可，示例如下： 1234567891011121314151617181920# cat ./deployment-with-secret.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-deployment-from-harbor namespace: kube-publicspec: replicas: 3 template: metadata: labels: app: nginx-for-test spec: containers: - name: nginx-for-test image: 172.16.21.253:10080/aisino-lib/docker.io/nginx:latest ports: - containerPort: 80 imagePullSecrets: - name: harbor-k8s-secret 通过secret yaml文件创建pull image所用的secret,其实跟上述方法类似，不过是用yaml文件创建的secret. 123456789# cat myregistrykey.yamlapiVersion: v1kind: Secretmetadata: name: myregistrykey namespace: awesomeappsdata: .dockerconfigjson: &#123;base64 -w 0 ~/.docker/config.json&#125;type: kubernetes.io/dockerconfigjson 其中，dockerconfigjson后面的数据就是docker login后生成的config.json文件的base64编码输出（-w 0让base64输出在单行上，避免折行） 1kubectl create -f myregistrykey.yaml secret使用方式与第二种方式一样，不过kubectl和yaml创建的两个secret的类型略有不同，前者是kubernetes.io/dockercfg，后者是kubernetes.io/dockerconfigjson。 部署dashboard由README 文件可知，有两种部署方式，如果是没有安装RBAC权限控制的，可以执行 1kubectl create -f https://git.io/kube-dashboard 如果有RBAC的，可以执行： 1kubectl create -f https://git.io/kube-dashboard-no-rbac kubeadm安装方式自从1.6+版之后自动安装RBAC，所以需要选择第二种。如果权限问题依旧（注：一般是报错serviceaccount:kube-system:default” cannot list statefulsets.apps in the namespace “default”.）可以根据该issue,添加一个权限。 123456789101112131415# cat dashboard-rbac.ymlkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: dashboard-adminroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects:- kind: ServiceAccount name: default namespace: kube-system# kubectl create -f dashboard-rbac.yml 注：如果想外部可以直接访问dashboard，需要修改下yaml文件，将最后的service配置修改为nodePort,示例如下： 1234567891011121314151617.......................---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - nodePort: 30002 port: 80 targetPort: 9090 selector: k8s-app: kubernetes-dashboard 这样便可以直接http://NODEIP:30002访问。关于port，nodePort, targetPort,可以参考kubernetes中port、target port、node port的对比分析，以及kube-proxy代理 部署Heapster 监控与统计Heapster是一个容器集群监控和性能分析工具，天然支持Kubernetes和CoreOS。这里使用influxDB作为Heapster的后端存储部署，参考安装文档.首先下载对应版本的相关yaml文件： 1wget https://github.com/kubernetes/heapster/archive/v1.3.0.tar.gz 解压并直接部署即可： 1234tar -zxvf v1.3.0.tar.gzcd heapster-1.3.0/deploy/kube-config/influxdbkubectl create -f ./* 该过程会pull相关镜像，同样，可以先翻墙pull下来再push到私有镜像仓库再使用。最终完成后，所有pods都running,可以看到dashboard的界面多了仪表盘。 参考文章k8s-doc-Installing kubeadm CentOS 7 安装Kubernetes 1.5.3 集群(本地安装) Kubernetes从Private Registry中拉取容器镜像的方法 k8s-doc-Images 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack-- openstack 中的资源分离策略]]></title>
    <url>%2F2017%2F08%2F08%2Fopenstack-resource-segeration%2F</url>
    <content type="text"><![CDATA[为什么做资源的分离openstack 作为一个云计算框架，需要统筹计算，存储，网络等资源，本身就已经够复杂了。如果是在一个非常大的环境中，节点众多，且复杂多样，这个时候就有必要根据这些节点的差异做一些逻辑上的分离，以达到不同资源的区分，这样，做水平扩展的时候也可以根据这个逻辑分区针对性的进行扩展。 openstack 中的资源分离策略openstack 的资源分离策略一定程度上借鉴了AWS的策略，整体来说，如下： Infrastructure segregation主要是理解Regions, cells, Host aggregates, Availability zones这几个概念。 Regions:借鉴自AWS，更像是一个地理上的概念（比如北京的数据中心可以作为一个region,南京的数据中心作为另一个region），每个region有自己独立的endpoint，regions之间完全隔离，不同regions之间可以共享keystone/dashboard。openstack默认新建一个region，即RegionOne，如果还想建立第二个Region，可以利用 keystone endpoint­create 命令添加。 Cells:cell主要是为了解决openstack 的扩展性以及规模瓶颈而引进的概念。当openstack达到一定规模后，依赖比较强的database以及AMQP便成为整个系统的瓶颈，引入cell后，每个cell有自己独立的database和AMQP。公司云平台只是一个小云，没有引入，所以不去深究。cell目前是有v1，v2两版，实现方式差异比较大，感兴趣可以参考Nova Cells V2如何帮助OpenStack集群突破性能瓶颈？。 Host aggregates &amp;&amp; Availability zones：这两个概念有共通点，且要相互配合使用，都用来表示一组节点的集合，简单说，AZ(Availability zone)是一个面向用户的概念，(这里只讨论nova 范畴的AZ,cinder,neutron也有对应的AZ概念),AZ一般依据地址，网络部署或电力配置划分，可以是一个独立的机房，或者一个独立供电的机架等，用户在创建instance的时候可以指定AZ，从而使instance创建在指定的AZ中，而host aggregate是一个面向管理员的概念，主要用来给nova-scheduler调度使用，比如根据某一属性（例如含有固态硬盘）划分一个host aggregate，把所有含有固态硬盘的host都放到该host aggregate中，nova-scheduler调度时指定相关属性就可以调度到对应host aggregate中的host。 如上图，有两个地理隔离的Region，四个AZ，以及若干个根据不同属性区分的host aggregate。host aggregate 创建的时候可以指定AZ（如果AZ没有就会自动创建一个AZ），一个host可以属于多个host aggregate，但只能属于一个AZ。如下为一个示例： 1234nova aggregate­create storage­optimized storage­optimized-AZ # 创建一个host aggregatenova aggregate­set­metadata $aggregateID fast­storage=true # 设置 aggregate 的metadatenova aggregate­add­host $aggregateID host­1 #添加host到aggregatenova flavor­key $flavorID set fast­storage=true #添加flavor的 条件 Workload segregation负载这一块的策略是基于server-group来做的，相对比较简单。注意，这里的server-group不再是host的集合，而是instance的集合。比如，我要创建3个instance，因为这3个instance都比较吃内存，所以想要这3个instance在不同的host上，就可以这样做： 1234nova server-group-create --policy anti-affinity group-1 # 创建server-group，注意policy指定策略是不在同一节点nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst1 #创建instance1 nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst2 #创建instance2 nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst3 #创建instance3 在nova配置文件中指定scheduler策略，有一个针对server-group的filter叫做ServerGroupAntiAffinityFilter。除了anti-affinity，还有 affinity策略，就是尽量让同一server-group的instance在同一个host上。 参考文章理解openstack中region、cell、availability zone、host aggregate 概念 openstack运维实战系列(十二)之nova aggregate资源分组 DIVIDE AND CONQUER:RESOURCE SEGREGATION IN THE OPENSTACKCLOUD 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes-- 关于ETCD && 服务发现的一些记录]]></title>
    <url>%2F2017%2F08%2F05%2Fintro-to-etcd%2F</url>
    <content type="text"><![CDATA[ETCD简介ETCD是一个可靠的键值对分布式存储系统，适用场景是用来存储那些写少读多，结构简单，但又比较重要的数据，这里的比较重要是指要保证数据的高可用，一致性。第一个想到的自然就是配置数据的存储与管理。当然，也可以存储其他满足上述要求的数据，比如在k8s中，ETCD会用来存储k8s各items(pods, rc, rs, deployment等)的状态。随着微服务架构的日益火热，ETCD也常作为服务发现的组件来使用。ETCD 有以下几个特性： 使用简单：友好的API(grpc) 安全：支持TLS双向通信 快速：支持10,000 写/秒 可靠： 采用raft协议作分布式选举 ETCD 安装因为是go写的，直接下载二进制文件执行即可，如果想要尝试最新版的，可以去下载源码，自己build,参考Download and build。如果是构造一个cluster，也比较简单，参考Demo 简单记录下常用的启动参数： name：节点名称，默认为 default，在集群中应该保持唯一，一般使用 hostname data-dir：服务运行数据保存的路径，默认为 ${name}.etcd snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘 heartbeat-interval：leader 多久发送一次心跳到 followers。默认值是 100ms eletion-timeout：重新投票的超时时间，如果 follower在该时间间隔没有收到心跳包，会触发重新投票，默认为 1000 ms listen-peer-urls:和同伴通信的地址，比如 http://ip:2380 ，如果有多个，使用逗号分隔。需要所有节点都能够访问， 所以不要使用 localhost！ listen-client-urls: 对外提供服务的地址：比如 http://ip:2379,http://127.0.0.1:2379，客户端会连接到这里和 etcd 交互 advertise-client-urls: 对外公告的该节点客户端监听地址，这个值会告诉集群中其他节点 initial-advertise-peer-urls: 该节点同伴监听地址，这个值会告诉集群中其他节点 initial-cluster: 集群中所有节点的信息，格式为 node1=http://ip1:2380,node2=http://ip2:2380,...... 注意：这里的 node1是节点的–name指定的名字；后面的 ip1:2380是 –initial-advertise-peer-urls指定的值。 initial-cluster-state：新建集群的时候，这个值为 new，假如已经存在的集群，这个值为 existing。 initial-cluster-token：创建集群的 token，这个值每个集群保持唯一。这样的话，如果你要重新创建集群，即使配置和之前一样，也会再次生成新的集群和节点uuid；否则会导致多个集群之间的冲突，造成未知的错误。 当然，还有比如tls设置，使用etcd discovery/dns discovery 进行etcd的启动等，配置详情见官网。 ETCD 使用一般通过两种方式，rest api或者通过命令行（本质也是rest api）,下面简单介绍下这两种方式。在介绍前，先要弄懂几个概念： member： 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node上的其它应用程序提供服务。 Cluster： Cluster 由多个 member 组成。每个 member 中的 node 遵循 raft共识协议来复制日志。Cluster 接收来自 member的提案消息，将其提交并存储于本地磁盘。 Peer： 同一 Cluster 中的其它 member。 命令行方式示例1234567891011121314151617181920212223242526272829$ etcdctl --endpoints=$ENDPOINTS put foo "Hello World!" # 写入操作$ etcdctl --endpoints=$ENDPOINTS get foo # 读取操作$ etcdctl --endpoints=$ENDPOINTS --write-out="json" get foo # 以json的方式输出$ etcdctl --endpoints=$ENDPOINTS get web --prefix # 获取所有前缀是web 的key的value$ etcdctl --endpoints=$ENDPOINTS del key # 删除$ etcdctl --endpoints=$ENDPOINTS txn --interactive # 将多个命令封装在一个事务中compares:value("user1") = "bad" success requests (get, put, delete):del user1 failure requests (get, put, delete):put user1 good$ etcdctl --endpoints=$ENDPOINTS watch stock1 # 监视某个值$ etcdctl --endpoints=$ENDPOINTS lease grant 300 #设定租约# lease 2be7547fbc6a5afa granted with TTL(300s)$ etcdctl --endpoints=$ENDPOINTS put sample value --lease=2be7547fbc6a5afa # 绑定租约$ etcdctl --endpoints=$ENDPOINTS get sample # 租约期限内可以获取值$ etcdctl --endpoints=$ENDPOINTS lease keep-alive 2be7547fbc6a5afa # 维持租约$ etcdctl --endpoints=$ENDPOINTS lease revoke 2be7547fbc6a5afa # 撤销租约# or after 300 seconds$ etcdctl --endpoints=$ENDPOINTS get sample # 撤销租约或者300s后获取不到值$ etcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status # 集群状态$ etcdctl --endpoints=$ENDPOINTS endpoint health$ etcdctl --endpoints=$ENDPOINTS snapshot save my.db # 快照 rest api 示例12345678910111213141516171819202122232425262728293031323334353637383940414243curl http://127.0.0.1:2379/v2/keys/message # 获取某个key的value&#123; "action": "get", "node": &#123; "createdIndex": 2, "key": "/message", "modifiedIndex": 2, "value": "Hello world" &#125;&#125;curl http://127.0.0.1:2379/v2/keys/message -XPUT -d value="Hello etcd" # 更改 &#123; "action": "set", "node": &#123; "createdIndex": 3, "key": "/message", "modifiedIndex": 3, "value": "Hello etcd" &#125;, "prevNode": &#123; "createdIndex": 2, "key": "/message", "value": "Hello world", "modifiedIndex": 2 &#125;&#125;curl http://127.0.0.1:2379/v2/keys/message -XDELETE #删除 &#123; "action": "delete", "node": &#123; "createdIndex": 3, "key": "/message", "modifiedIndex": 4 &#125;, "prevNode": &#123; "key": "/message", "value": "Hello etcd", "modifiedIndex": 3, "createdIndex": 3 &#125;&#125; 架构 &amp; 原理介绍etcd主要分为四个部分: HTTP Server： 用于处理用户发送的API请求以及其它etcd节点的同步与心跳信息请求。 Store：用于处理etcd支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是etcd对用户提供的大多数API功能的具体实现。 Raft：Raft强一致性算法的具体实现，是etcd的核心。 WAL：Write Ahead Log（预写式日志），是etcd的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd就通过WAL进行持久化存储。WAL中，所有的数据提交前都会事先记录日志。Snapshot是为了防止数据过多而进行的状态快照；Entry表示存储的具体日志内容。通常，一个用户的请求发送过来，会经由HTTP Server转发给Store进行具体的事务处理，如果涉及到节点的修改，则交给Raft模块进行状态的变更、日志的记录，然后再同步给别的etcd节点以确认数据提交，最后进行数据的提交，再次同步。 关于集群状态机的转变可以参考CoreOS 实战：剖析 etcd 服务发现工具对比引自服务发现：Zookeeper vs etcd vs Consul： 所有这些工具都是基于相似的原则和架构，它们在节点上运行，需要仲裁来运行，并且都是强一致性的，都提供某种形式的键/值对存储。Zookeeper是其中最老态龙钟的一个，使用年限显示出了其复杂性、资源利用和尽力达成的目标，它是为了与我们评估的其他工具所处的不同时代而设计的（即使它不是老得太多）。etcd、Registrator和Confd是一个非常简单但非常强大的组合，可以解决大部分问题，如果不是全部满足服务发现需要的话。它还展示了我们可以通过组合非常简单和特定的工具来获得强大的服务发现能力，它们中的每一个都执行一个非常具体的任务，通过精心设计的API进行通讯，具备相对自治工作的能力，从架构和功能途径方面都是微服务方式。Consul的不同之处在于无需第三方工具就可以原生支持多数据中心和健康检查，这并不意味着使用第三方工具不好。实际上，在这篇博客里我们通过选择那些表现更佳同时不会引入不必要的功能的的工具，尽力组合不同的工具。使用正确的工具可以获得最好的结果。如果工具引入了工作不需要的特性，那么工作效率反而会下降，另一方面，如果工具没有提供工作所需要的特性也是没有用的。Consul很好地权衡了权重，用尽量少的东西很好的达成了目标。Consul使用gossip来传播集群信息的方式，使其比etcd更易于搭建，特别是对于大的数据中心。将存储数据作为服务的能力使其比etcd仅仅只有健/值对存储的特性更加完整、更有用（即使Consul也有该选项）。虽然我们可以在etcd中通过插入多个键来达成相同的目标，Consul的服务实现了一个更紧凑的结果，通常只需要一次查询就可以获得与服务相关的所有数据。除此之外，Registrator很好地实现了Consul的两个协议，使其合二为一，特别是添加Consul-template到了拼图中。Consul的Web UI更是锦上添花般地提供了服务和健康检查的可视化途径。 参考文章etcd 使用入门 etcd API Etcd Tutorial- The Ultimate Reliable Key Value Storage for Networks 服务发现比较:Consul vs Zookeeper vs Etcd vs Eureka CoreOS 实战：剖析 etcd 深入学习Etcd etcd：从应用场景到实现原理的全方位解读 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>ETCD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-- 关于Harbor 的一些记录]]></title>
    <url>%2F2017%2F08%2F03%2Fintro-to-harbor%2F</url>
    <content type="text"><![CDATA[Harbor 简介Harbor 是 Vmware China研发开源的企业级私有容器Registry,基于docker官方的解决方案Distribution。目前来说，已经成为企业级私有容器仓库的首选（主要是可供选择的本来就不多，可能是因为镜像管理不像容器编排那样是必争之地）。相对于Docker Distribution，Harbor添加了安全，认证，管理等功能，性能以及安全性都得到提升，主要 features如下： 基于角色的访问控制：用户和镜像仓库通过project来组织管理，一个用户对同一project的不同镜像仓库会有不同处理权限。 基于策略的镜像复制：多个registry实例可以实现镜像同步复制，对于load-balancing,高可用，多数据中心，混合云的情景非常有用。 支持 LDAP/AD 镜像删除 &amp; 垃圾回收 镜像的认证 友好的UI 日志审计：所有的操作都是可追踪的 RESTful API 易部署 Harbor 架构与原理Harbor 整体架构Harbor 是以容器的方式运行，以docker-compose的规范形式组织各个组件，并通过docker-compose工具进行启停。Harbor共有五个组件，分别如下： Proxy:Harbor服务的所有请求都由该服务接受并转发，其实就是一个前置的反向代理Nginx，类似于微服务概念中的API-gateway. Registry: 即docker 官方的Registry镜像生成的容器示例，真正负责存贮镜像的地方，处理docker pull/push。针对不同的用户对不同的镜像操作权限不同，registry强制每个请求必须含有一个token以验证权限，如果没有token，会返回一个token服务地址。 Core Services: 主要提供UI,webhook(设置在registry上以获取镜像状态)，token服务。 Database:数据库服务Mysql，存储用户，权限，审计日志，镜像信息等。 Log Collector: 日志收集，跑一个Rsylogd服务。 架构图如下： 这五个容器之间通过docker-link相连，即通过容器名字互相访问，暴露Proxy服务的端口给终端用户访问。 Harbor 工作原理以docker login 与 docker push为例讲解： 客户端 输入docker login 之后，流程如下图： (a) 首先，这个登录请求会被Proxy容器接收到，根据预先设置的匹配规则，该请求会被转发给后端Registry容器。(b) Registry接收到请求后，解析请求，因为配置了基于token的认证，所以会查找token，发现请求没有token 后，返回错误代码401以及token服务的地址URL。(c) Docker客户端接收到错误请求后，转而向token服务地址发送请求，并根据HTTP协议的BasicAuthentication 规范，将用户名密码组合并编码，放在请求头部(header)。(d) 同样，该请求会先发到Proxy容器，继而转发给ui/token的容器,该容器接受请求，将请求头解码，获取到用户名密码。(e) ui/token的容器获取到用户名密码后，通过查询数据库进行比对验证(如果是LDAP 的认证方式,就是与LDAP服务进行校验)，比对成功后，返回成功的状态码，并用密钥生成token，一并发送给Docker客户端。 客户端 登陆成功后，输入docker push xxxxxx 之后，流程如下图（便于说明省略Docker client与Proxy之间通信）： (a) 同样，首先与Registery通信，返回一个token服务的地址URL.(b) Docker客户端会与token服务通信，指明要申请一个push image操作的token。(c) token服务访问数据库验证当前用户是否有该操作的权限，如果有，会将image信息以及push操作进行编码，用私钥签名，生成token返回给Docker客户端。(d) Docker客户端再次与Registry通信，不过这次会将token放到请求header中，Registry收到请求后利用公钥解码并核对，核对成功，便可以开始push 操作了。 Harbor 安装比较简单： 准备：python&gt;=2.7, docker&gt;=1.10, docker-compose&gt;=1.6.0 下载离线安装包 修改配置文件 harbor.cfg 执行脚本install.sh 参考Installation and Configuration Guide Harbor 高可用对于Harbor高可用方案，目前并没有最佳实践，不过我看issues上有不少相关内容，可以参考Harbor HA feature design proposal/discussion。其实，私有云相对来说对镜像的请求并非高频，在做HA的时候还是结合实际情况，切勿为了HA而HA,还要综合考量成本，安全等因素。 这部分内容暂且留个坑，以后再写。 参考文章VMware Harbor：基于 Docker Distribution 的企业级 Registry 服务 基于Harbor和CephFS搭建高可用Private Registr vmware/harbor Docker 企业级私有镜像仓库 Harbor 部署 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Harbor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack-- 最近遇到的几个Openstack问题小结]]></title>
    <url>%2F2017%2F07%2F20%2Ffew-problems-met-in-openstack%2F</url>
    <content type="text"><![CDATA[win7虚拟机CPU核数不变问题问题描述：虚拟机镜像为win7系统，配置为4cpu的时候，发现设备管理器显示为4cpu，但是任务管理器智能识别2cpu，如下图。 问题解决：刚开始以为是win7系统引导项中限制到2核，更改后问题没有解决，更换镜像后问题依旧，后经过搜索得出答案:首先需要了解kvm的cpu虚拟化原理，可以参考KVM 介绍（2）：CPU 和内存虚拟化,简单来说，一个KVM虚拟机就是一个Linux qemu-kvm进程，内存就是该进程的地址空间的一部分，虚拟机的vcpu作为线程运行在该进程的上下文，逻辑关系如下： 我们用kvm命令创建虚拟机时有几个概念，1$ kvm -m 2048 -smp 4,sockets=4,cores=1,threads=1 -drive file=win7_x64_pure 其中，smp 指定为4 ，默认后面什么都不加的话，相当于是sockets=4,cores=1,threads=1，简单解释下socket是cpu的物理单位，core是每个cpu中的物理内核，thread是利用超线程技术实现的一个core虚拟化出的逻辑cpu个数。也就是说，客户机操作系统看到的cpu核数是上述三个数值的乘积，至于使用多socket，还是多core，可以参考vCPU configuration. Performance impact between virtual sockets and virtual cores?。回到之前的问题，openstack默认的max-sockets是4，也就是说只要socket没有特殊指定，且小于等于4，那么逻辑核数就是socket的个数。比如，创建一个4核的kvm虚拟机，那么openstack默认对应kvm命令为kvm -smp 4,sockets=4,cores=1,threads=1。而在某些客户机操作系统会限制物理 CPU （这里即socket）的数目,比如win7操作系统限制2个win，Windows Server 2008 R2 Standard Edition 限制4个。这种情况下，我们就需要更改cpu topology,比如win7,为了实现4核，可以利用以下命令：1kvm -m 2048 -smp 4,sockets=2,cores=1,threads=2 -drive file=win7_x64_pure 更多cpu topology 的知识，参考VirtDriverGuestCPUTopology。回到openstack的话，我们可以给镜像添加合适的 max-sockets来解决此类问题，在win 7的镜像里加上一个属性, hw_max_sockets=2即可。 虚拟机调整配额失败问题描述：如题，对已经启动的虚拟机调整配额，没有反应。问题解决：找到对应计算节点，查看nova-compute日志发现错误原因，错误日志如下： 无法ssh到目标节点，查阅相关资料后，得出resize命令需要设置nova用户在节点之间的passwordless authentication，解决如下： sudo -u nova ssh-keygen # 生成nova的密钥 ssh-copy-id nova@ #复制公钥到目的节点 tips:复制公钥的过程中如果报错“This account is currently not available”，可能是因为用户nova的shell禁止登录了，修改/etc/passwd 中nova对应的shell登录部分由“/sbin /nologin”改成“/bin/bash”。 ceph 节点资源耗尽问题描述：公司有个生产环境的云平台，因为前期的滥用，导致存储资源耗费非常严重，已经出现2个计算节点near full，这会导致出现一系列问题，比如虚拟机卡顿，无法访问等问题。解决方案： 删除不用的虚拟机，镜像等垃圾文件，如果出现无法删除的情况，直接利用rbd命令尝试下，如果还是无法删除，应该是对应osd达到full状态而拒绝客户端操作命令了，只能换个方法。 如果条件允许的话，最好是增加OSD节点，然后再平衡就OK了，不过再平衡的时间太长，对于线上业务是无法接受的。 通过rewight命令手动修改对应osd wight值，比如，对于处于near full/full状态的osd，我们减小其对应的weight值，适当增大有较多剩余空间的osd的weight值。调整的过程中不要一下全部更改，需要调整一下，看下效果，避免因为改动过大出现大范围的再平衡导致时间过长。 在osd 再平衡期间，增加mon-osd-full-ratio/mon osd nearfull ratio值（未验证） Failed to allocate the network(s), not rescheduling.创建虚拟机的时候报如标题所述错误，查看详细日志，可以看出是Virtual Interface creation failed. 自然想到可能是Virtual Interface 创建失败导致创建虚拟机失败，查看neutron-server日志，发现如下信息：12017-08-09 11:21:08.499 154120 WARNING neutron.notifiers.nova [-] Nova returned NotFound for event: [&#123;'tag': u'a653baaf-828d-4641-aabb-1a82c5163889', 'name': 'network-vif-deleted', 'server_uuid': u'4fdf7471-bece-4d93- a044-a4052284c69b'&#125;] 也就是说nova没有接受到network-vif-deleted的event，查看具体出错代码： 12345678910111213141516def _create_domain_and_network(self, context, xml, instance, network_info, disk_info, block_device_info=None, power_on=True, reboot=False, vifs_already_plugged=False): ............................................................... except eventlet.timeout.Timeout: # We never heard from Neutron LOG.warn(_LW('Timeout waiting for vif plugging callback for ' 'instance %(uuid)s'), &#123;'uuid': instance.uuid&#125;, instance=instance) if CONF.vif_plugging_is_fatal: #关键在这行 if guest: guest.poweroff() self.cleanup(context, instance, network_info=network_info, block_device_info=block_device_info) raise exception.VirtualInterfaceCreateException() 可以看到在nova-compute调用_create_domain_and_network函数的时候，会一直等待vif 的创建eventlet（由openvswitch-agent创建并返回evetlet），等待timeout时间之后，如果配置文件中vif_plugging_is_fatal=True,就会创建失败并回滚，如果vif_plugging_is_fatal=False就会略过。明白原理后就简单了，只需修改nova配置文件中vif_plugging_is_fatal=False就可以了，至于为什么nova没有收到eventlet，还有待深入。这一部分可以参考nova network-vif-plugged事件分析1. 注：排错过程有个小插曲，我修改完配置文件后再重启nova-compute还是没效果，果断打断点调试，结果发现断点直接略过了，百思不得其解，折腾半天后，发现是有一个残留的nova-compute的进程一直在跑，kill掉之后，再启动就可以了。 参考文章KVM 介绍（2）：CPU 和内存虚拟化 Openstack Windows7 CPU核数显示不一致 vCPU configuration. Performance impact between virtual sockets and virtual cores? Host key verification failed on VM Resize HANDLING A FULL CEPH FILESYSTEM Ceph集群磁盘没有剩余空间的解决方法 nova network-vif-plugged事件分析1 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack-- nova-network 网络实现一窥]]></title>
    <url>%2F2017%2F06%2F30%2Ftangle-with-nova-network%2F</url>
    <content type="text"><![CDATA[引出nova-network 是Neutron（或Quantum）之前，openstack 的网络管理项目，随着Neutron项目的愈发成熟，nova-network 也随之逐渐被弃用。不过，其实 nova-network 还是非常好用的，相对neutron 来说，简单轻便，也经受过生产环境的验证。而且，可以通过简单配置将虚拟机二层网络与真实物理网络打通，实现利用fixed-ip就可以直接访问虚拟机的单网络环境。本文主要总结下 nova-network 的网络管理以及实现原理，并与neutron 做一下比较。 nova-network 网络类型nova-network 支持两种网络类型（确切地说是三种，这里将Flat与FlatDHCP看做一类），分别是Flat/FlatDHCP 和 Vlan类型。 FlatManager and FlatDHCPManager顾名思义，flat即扁平化，也就是说所有的虚拟机都在一个大的二层网络空间内，这种网络模式一般生产环境中很少用，多用于POC阶段。关于flat networking，我在openstack 上找到一篇wiki，讲解的比较清楚。可惜wiki 上只有讲解flat的，没有其他网络类型。 这里借上文中的一张图简单说下flat这种网络类型，以多节点，多网卡为例： 由图看出，计算节点内都有一个网桥br100与 物理网卡eth0相连，计算节点内的虚拟机通过该网桥与控制节点的对应物理网卡连接实现通信。虚拟机实现南北通信大致如下图： flatDHCP 其实就是在计算节点再运行一个dnsmasp来提供DHCP服务，就不需要借助外部的DHCP服务，两者对比大致如下： flat networking : flatDHCP networking: VlanManagerflat networking 有一个很大的缺点就是所有虚拟机都在一个二层网络，没有租户间的网络隔离，不够灵活，且广播风暴的影响太大，这种情况下，vlan 就出现了。 vlan 网络或给每个租户创建一个（或多个）二层网络，且这些二层网络相互隔离。不过这种网络类型需要支持vlan tag的交换机才可以使用。 nova-network 网络实现原理简单介绍下各自的实现原理 Flat/FlatDHCP网络实现原理Flat类型的基本上上文已经讲了，主要看下flatDHCP 的实现： 跟flat类似，不过在计算节点的网桥上会有一个dnsmasq进程监听并实现该节点的虚拟机IP动态分配。还有一点要注意的是，不同计算节点的虚拟机内网关也不同，比如，上图中，vm_1与vm_2的网关都是10.10.0.1，但在右边计算节点的vm_3 和vm_4的网关却是10.10.0.4。 Vlan实现原理一图胜千言： 上面这幅图就是vlan模式部署最为广泛的场景，即multi-node模式，也就是说不光是网络节点需要运行nova-network服务，计算节点也要运行nova-network（还需要运行nova-api以提供metadata服务），这样就避免出现SPOF,也可以达到分流的作用（跟neutron 的DVR很类似，估计DVR就是借鉴了这里）。需要注意的是每个vlan网桥都是租户独占的，且会创建vlan接口如vlan102,依据802.1q协议打vlanid，与网关eth0连接。Dnsmasq监听网桥网关，负责fixedip的分配。switch port设定为chunk mode。eth0负责vm之间的数据通信，另一网卡如eth1负责外网访问。上图只是显示了一个网卡，如下图是一个多网卡的情况： nova-network vs neutron之前总结过一篇关于neutron 实现二三层网络的总结,就功能性来说，nova-network 只支持两种网络类型，neutron增加了overlay network 的支持，如vxlan/gre, 突破了vlan id个数限制从而支持更多数目的二层网络。而且nova-network 即使是不同租户间，也不允许使用相同的网段，因为租户网络是通过ip+vlan的形式来区分，neutron 就完全可以。但也不是nova-network 就一文不取，相比于neutron网络，虽说没有neutron那么多的功能插件，仅有bridge，但是其稳定性已得到大多数用户的验证，对于小规模的私有云(1千台虚机的规模)，nova-network是可以考虑的。 参考文章OpenStack Networking – FlatManager and FlatDHCPManager Openstack Networking for Scalability and Multi-tenancy with VlanManager Network service - easy version - nova-network Networking with nova-network openstack 网络架构 nova-network + neutron UnderstandingFlatNetworking 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>nova</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Devops-- 借助virtualbox 初次实践 cobbler]]></title>
    <url>%2F2017%2F06%2F26%2Fcobbler-test-with-virtualbox%2F</url>
    <content type="text"><![CDATA[Cobbler 简介Cobbler 是由Python 语言开发，实现在网络安装环境中快速的安装linux 系统。在没有出现Cobbler之前，我们通常是通过使用Kickstart 来进行自动化安装操作系统，但是kickstart 并未实现完全的自动化，我们依然需要安装配置各种服务（DHCP,TFTP,HTTP等），编写kickstart 脚本（即ks.cfg文件）等等。Cobbler可以看做是在kickstart项目上的一层封装，融合了DHCP,TFTP，PXE等一系列服务，提供了CLI和web两种管理形式，使得网络安装操作系统更加方便，同时也提供了API接口方便二次开发。 Cobbler 原理概述Cobbler 是典型的CS架构，我们只需要在服务端做好相应的安装配置，客户端设置PXE网络启动即可。整体的运作流程如下，图片来自Cobbler自动化安装配置实践: Cobbler 实践本次试验借助 VirtualBox 实现： 准备工作 下载 centos7-minimal 镜像，并使用该镜像开启一个虚拟机guest#1 注意网络配置 两个网卡，一个是bridge模式，便于利用xshell等工具连接，一个是设为内部网络。 安装完毕后，设置第二个网卡的静态IP，不要与第一个网卡的网络重复，之后会用第二个网卡所在的局域网进行网络安装。 Cobbler 安装参照 官网Red Hat Entperise Linux 注：安装需要用到 epel源，因为是Centos，只需 yum install epel-release 即可。 Cobbler 配置参考官网Cobbler Quickstart Guide 注： 利用cobbler check 查看配置错误，有些地方并非必须修改。 server 与 next-server设置为第二块网卡的ip Cobbler 各目录说明配置文件目录：/etc/cobbler /etc/cobbler/settings：cobbler 主配置文件 /etc/cobbler/iso/：iso 模板配置文件 /etc/cobbler/pxe：pxe 模板文件 /etc/cobbler/power：电源的配置文件 /etc/cobbler/users.conf：Web 服务授权配置文件 /etc/cobbler/users.digest：用于 web 访问的用户名密码配置文件 /etc/cobbler/dhcp.template：DHCP 服务的配置模板 /etc/cobbler/dnsmasq.template：DNS 服务的配置模板 /etc/cobbler/tftpd.template：tftp 服务的配置模板 /etc/cobbler/modules.conf：Cobbler 模块配置文件 数据目录：/var/lib/cobbler /var/lib/cobbler/config/：用于存放 distros、systems、profiles 等信息配置文件 /var/lib/cobbler/triggers：用于存放用户定义的 cobbler 命令 /var/lib/cobbler/kickstarts/：默认存放 kickstart 文件 /var/lib/cobbler/loaders：存放各种引导程序 镜像数据目录：/var/www/cobbler /var/www/cobbler/ks_mirror/：导入的发行版系统的所有数据 /var/www/cobbler/images/：导入发行版的 Kernel 和 initrd 镜像用于远程网络启动（ks_mirror 下对应发行版 系统的软链） /var/www/cobbler/repo_mirror/：repo 仓库存储目录 日志目录：/var/log/cobbler/ /var/log/cobbler/install.log：客户端系统安装日志 /var/log/cobbler/cobbler.log：cobbler日志 Cobbler 使用 利用lrzsz命令上传 centos7-minimal 镜像至虚拟机guest#1. 本地挂载镜像 1mount -o loop ./CentOS-7-x86_64-Minimal-1611.iso /mnt 导入镜像 1cobbler import --name=centos7 --arch=x86_64 --path=/mnt 执行 cobbler sync命令 virtualbox 开启另一个虚拟机，一个网卡，设置为内部网络，adaptor type 为PC-Net III （为了PXE boot）。设置启动方式为网络启动优先。 Cobbler 常用命令行1234567$ cobbler list #列出相关cobber元素(distros和profile)$ cobbler check #检查cobbler配置(一般会提示需要进行怎样的配置)$ cobbler report #列出cobbler的详细信息$ cobbler distro #查看导入的相关系统发行版信息$ cobbler profile #查看cobbler创建的相关pofile信息$ cobbler sync #同步cobbler相关配置(最好每次执行完配置后都进行修改)$ cobbler reposync #同步repo源 遇到的问题 TFTP Timed out : guest#1的TFTP Server 没有启动造成。 failed to start switch root：dracut cant locate /dev/root 如果是某台机器需要重装系统的话，需要在该机器上安装koan，利用koan命令：koan –replace-self –server=10.45.249.102 –profile=rhel6.6-64-x86_64 进项镜像的拉取以及重装 参考文章Cobbler doc cobbler wiki) Cobbler自动化安装配置实践 自动化运维工具Cobbler Testing out cobbler with virtuabox 主机自动化部署之cobbler总结 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>devops</tag>
        <tag>ops</tag>
        <tag>cobbler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- neutron 分布式虚拟路由（DVR）]]></title>
    <url>%2F2017%2F06%2F20%2Fneutron-dvr%2F</url>
    <content type="text"><![CDATA[引出公司有个测试云平台的虚拟机密码初始化出现了问题，根据之前的经验可能跟这篇问题一样,没想到花费了将近一天的时间才将问题解决，发现最终是跟neutron 分布式虚拟路由有关，这里记录一下。 问题是这样的：nova boot的时候传入 user-data（user-data中含有需要修改的密码），instance起来的时候会执行cloud-init，然后会使用EC2的datasource方式（即“169.254.169.254”）去获取user-data，然而发现密码没有更改，查看日志发现,发现获取user-data时，报错如下 Calling ‘http://169.254.169.254/2009-04-04/meta-data/instance-id&#39; failed [0/120s]: bad status code [500] 初步断定就是instance metadata 的某个环节网络不通。 问题发现 第一步想到的就是重启下相关服务，将neutron-metada-agent,neutron-l3-agent,nova-api重启后发现问题依旧。 查看相关日志，没有找到有关报错信息，而且奇怪的是网络节点对应的neutron-ns-metadata-proxy-xxxxxxxxx.log都没有处理请求的日志，说明请求可能没有走到这里。 只能按照请求的处理步骤一步步排查，首先确定虚拟机所在的网络是有路由器相连的，所以metadata请求是走的路由器的namespace，相关知识参考openstack– openstack instance metadata 服务机制探索,网络节点对应路由器的namespace中一切正常，9697 端口的重定向，以及监听9697端口的neutron-metadata-ns-proxy进程也正常。没办法，在该路由器的namespace中对应端口抓包试试，虚拟机中发送169.254.169.254，却一点反应没有，断定请求压根就没有走到该路由器的namespace。只能返回去虚拟机实例中抓包试试，使用traceroute命令发现请求是发到对应网关的，而对应网关就应该在路由器的namespace中啊，到这里基本就蒙逼了。 在经过一番徒劳无功的试错后，终于找到了一点线索。我在查看neutron agent状态的时候，发现neutron-l3-agent不只是在网络节点，计算节点也存在neutron-l3-agent，立马想到neutron-l3-agent会在本地计算节点也创建虚拟路由器，所以获取metadata的请求就发送到本地计算节点的路由器 namespace，果然，本地计算节点也有一个路由器，抓包发现确实是发送到了这里，而且该节点对应网络的neutron-ns-metadata-proxy-xxxxxxxxx.log发现了 socket.error报错信息，说明neutron-ns-metadata-proxy通过 unix domian socket 转发给 neutron-metadata-agent的时候出现了问题。本能的想是不是配置错误导致的，查看l3-agent的配置文件时发现有个配置项：agent_mode = dvr ，赶紧搜了一下，终于找到了进入的正确姿势。 neutron 分布式虚拟路由（DVR）整体架构neutron 的DVR 是属于openstack HA的一部分，主要解决虚拟路由器的HA（即neutron-l3-agent的HA）。之前写过一篇记录openstack的HA,但没有仔细研究过neutron DVR。 在没有使用dvr时，我们的整体架构大致是这样的： 计算节点只需要安装L2 Agent就可以，而使用dvr模式后： 计算节点除了L2 Agent外，还要安装L3 Agent以及Metadata Agent。也就是说，通过使用dvr，计算节点也有了网络节点的三层转发和NAT功能，起到了分流的作用。 安装配置 控制节点：修改neutron.conf 如下，重启neutron-server 12[DEFAULT]router_distributed = True 网络节点：修改 openswitch_agent.ini如下，重启Open vSwitch agent 12[DEFAULT]enable_distributed_routing = True 修改l3_agent.ini如下，重启Layer-3 agent， 12[DEFAULT]agent_mode = dvr_snat 计算节点：安装l3-agent,metadata-agent,修改 openswitch_agent.ini如下，重启Open vSwitch agent 12[DEFAULT]enable_distributed_routing = True 修改l3_agent.ini 如下，重启l3-agent 1234[DEFAULT]interface_driver = openvswitchexternal_network_bridge =agent_mode = dvr 可以通过 neutron agent-list 验证agent 是否启动。 网络流通方向 照着这张图梳理一遍以下几种情况的网络走向： 没有floating-ip的instance的南北向网络流通情况即图中蓝色标注的线路，以图中左边计算节点的instance为例，首先通过一对veth设备与一个linux-bridge相连（该linux-bridge的存在主要是为了利用iptables实现Sec-group），又是一对veth设备，连接到OVS生成的br-int网桥，该instance在三层网络的第一跳便是到与该br-int相连的Disk router namespace的网关接口，因为没有floating-ip ，所以不会通过FIP namespace，而是通过patch设备到br-tun，再到网络节点的br-int,到达snat namespace，进行snat(源地址转换)，最后通过ovs-provider-bridge与外网联通。 有floating-ip的instance的南北向网络流通情况刚开始与第一种情况类似，但是跳到本地router namespace后，接下来会跳到本地的RIP namespace，做snat,然后直接通过本地的ovs-provider-bridge连接到外网。 不同子网但有虚拟路由连接的instance东西向网络刚开始类似，之后在本地router namespace进行路由选择，并通过br-int,br-tun进入对应虚拟机的计算节点（该部分工作由ovs 的openflow完成，同时还完成了snat），到了 目标计算节点上，依次被 br-tun，br-int 处理，直到通过 tap 设备进入另一instance。 关于详细描述以及实验可以参考官方文档,以及这篇。 问题解决回到刚开始碰到的问题，确定了问题的所在，就是在本地计算节点neutron-ns-metadata-proxy通过 unix domian socket 转发给 neutron-metadata-agent的时候出现了问题，neutron-ns-metadata-proxy是正常的，那么问题就出在neutron-metadata-agent上，果然，该agent在计算节点并没有启动，启动后正常。 参考文章Network Troubleshooting Neutron Networking: Neutron Routers and the L3 Agent Neutron/DVR 理解 OpenStack 高可用（HA）（3）：Neutron 分布式虚拟路由（Neutron Distributed Virtual Routing） Neutron 理解 (6): Neutron 是怎么实现虚拟三层网络的 [How Neutron implements virtual L3 network] Neutron DVR实现multi-host特性打通东西南北流量提前看 Open vSwitch: High availability using DVR 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>neutron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-- python 中的一些隐藏特性]]></title>
    <url>%2F2017%2F06%2F14%2Fpython-hidden-feature%2F</url>
    <content type="text"><![CDATA[引出偶然间翻到Stackoverflow上的这个问题，浏览了一下，觉得有点意思，摘录了一些感觉能用到的纪录在此。 使用‘ * ** ’ 代表函数的list/dict参数1234567def draw_point(x, y): print(x,y)point_foo = [3, 4] # or (3,4)point_bar = &#123;'y': 3, 'x': 2&#125;draw_point(*point_foo)draw_point(**point_bar) 这个特性其实使用还算常见，不过pylint不建议这样使用，因为debug的时候会不清晰。 链式比较操作1234567891011&gt;&gt;&gt; x = 5&gt;&gt;&gt; 1 &lt; x &lt; 10True&gt;&gt;&gt; 10 &lt; x &lt; 20 False&gt;&gt;&gt; x &lt; 10 &lt; x*10 &lt; 100True&gt;&gt;&gt; 10 &gt; x &lt;= 9True&gt;&gt;&gt; 5 == x &gt; 4 True 比如最后一个比较，python会将这一句转化为 (5 == x and x &gt; 4) ，这是跟C或java不一样的。 装饰器装饰器用一个函数或方法封装另一个函数以增加功能或修改参数，结果等。 12345678910111213&gt;&gt;&gt; def print_args(function):&gt;&gt;&gt; def wrapper(*args, **kwargs):&gt;&gt;&gt; print 'Arguments:', args, kwargs&gt;&gt;&gt; return function(*args, **kwargs)&gt;&gt;&gt; return wrapper&gt;&gt;&gt; @print_args&gt;&gt;&gt; def write(text):&gt;&gt;&gt; print text&gt;&gt;&gt; write('foo')Arguments: ('foo',) &#123;&#125;foo 小心使用可变的默认参数12345678910&gt;&gt;&gt; def foo(x=[]):... x.append(1)... print x... &gt;&gt;&gt; foo()[1]&gt;&gt;&gt; foo()[1, 1]&gt;&gt;&gt; foo()[1, 1, 1] 可以加一个哨兵值来做判断： 123456789&gt;&gt;&gt; def foo(x=None):... if x is None:... x = []... x.append(1)... print x&gt;&gt;&gt; foo()[1]&gt;&gt;&gt; foo()[1] 利用python 正则语法树来debug正则表达式12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; re.compile("^\[font(?:=(?P&lt;size&gt;[-+][0-9]&#123;1,2&#125;))?\](.*?)[/font]", re.DEBUG)at at_beginningliteral 91literal 102literal 111literal 110literal 116max_repeat 0 1 subpattern None literal 61 subpattern 1 in literal 45 literal 43 max_repeat 1 2 in range (48, 57)literal 93subpattern 2 min_repeat 0 65535 any Nonein literal 47 literal 102 literal 111 literal 110 literal 116``` 当然，最常用的还是使用注解的方式以增强正则的可读性。```bash&gt;&gt;&gt; re.compile(""" ^ # start of a line \[font # the font tag (?:=(?P&lt;size&gt; # optional [font=+size] [-+][0-9]&#123;1,2&#125; # size specification ))? \] # end of tag (.*?) # text between the tags \[/font\] # end of the tag """, re.DEBUG|re.VERBOSE|re.DOTALL) 利用enumerate 获取list的下标123456789&gt;&gt;&gt; a = ['a', 'b', 'c', 'd', 'e']&gt;&gt;&gt; for index, item in enumerate(a, start=1): print index, item...1 a2 b3 c4 d5 e&gt;&gt;&gt; 生成器12345678&gt;&gt;&gt; n = ((a,b) for a in range(0,2) for b in range(4,6))&gt;&gt;&gt; for i in n:... print i (0, 4)(0, 5)(1, 4)(1, 5) 需要注意的是，列表生成式是实实在在生成了列表存在了内存里，可以多次迭代使用，而生成器只能迭代一次。 巧用list中的slice操作slice操作如下： 123a = [1,2,3,4,5]&gt;&gt;&gt; a[::2] # iterate over the whole list in 2-increments[1,3,5] 将一个list逆序，最简洁写法： 12&gt;&gt;&gt; a[::-1][5,4,3,2,1] for…else 语法12345for i in foo: if i == 0: breakelse: print("i was never 0") 上述代码如果break，那么else代码是不会执行的。上述代码的执行顺如跟下面的代码是一样的：1234567found = Falsefor i in foo: if i == 0: found = True breakif not found: print("i was never 0") 两个变量互换12345678&gt;&gt;&gt; a = 10&gt;&gt;&gt; b = 5&gt;&gt;&gt; a, b(10, 5)&gt;&gt;&gt; a, b = b, a&gt;&gt;&gt; a, b(5, 10) with 声明语句with 在Python2.5需要从future模块导入，2.6+已经可以直接使用。示例： 1234from __future__ import with_statementwith open('foo.txt', 'w') as f: f.write('hello!') with 语句在这里的作用是在调用with时，会调用这个文件实例的 enter 方法，和 exit 方法，如果发生异常，异常的详细信息会传给 exit 方法处理并抛出，with在这里的主要用途是保证文件句柄最终关闭，其实with 可以看作是异常处理的一种抽象。除了打开文件经常使用with外，线程锁以及数据库事务也经常用到。 参考文章Hidden features of Python 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>python</tag>
        <tag>hidden feature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-- ceph rbd快照的实现原理]]></title>
    <url>%2F2017%2F06%2F05%2Fceph-rbd-snapshot%2F</url>
    <content type="text"><![CDATA[引出关于openstack 中的快照，备份部分之前做过一次总结，openstack 中的快照与备份 ,因为目前基本使用ceph rbd作为共享存储后端，所以本篇文章想讨论以rbd作为存储后端时的备份与快照情况： nova image-create：具体实现参考openstack 中的快照与备份 nova backup: 同上。 cinder snapshot：直接利用 rbd snapshot 实现 cinder backup：支持增量备份，也用到了rbd snapshot，参考 cinder-backup 利用ceph实现增量备份 由以上情况可知，rbd snapshot 几乎都要用到，所以接下来探讨下rbd snapshot 的实现原理。 rbd snapshot vs rbd clone首先知道这两者的区别： snapshot：是某一镜像(image) 在特定时间点的一个只读副本,注意，是只读副本，所以该snapshot是无法作为一个image去进行写操作的（比如起一个虚拟机）。 clone: 是针对snapshot的一种操作，可以理解为对snapshot的再度拷贝，从而实现可写操作，简单讲就是将image的某一个snapshot 的状态复制变成另一个image，该image状态与snapshot完全一致，但是可读可写（可以从此image起一个虚拟机）。 弄懂了两者的区别，接下来需要认真探讨下两者实现的原理： rbd snapshot 实现原理注意：snapshot时，一定要对原rbd image 停止IO操作，否则会引起数据不一致。 跟git 实现版本控制类似，使用 COW （copy on write）方式实现。这里叙述下对某一pool中的某个image 进行snapshot 的过程： 直接引用自解析Ceph: Snapshot： 每个Pool都有一个snap_seq字段，该字段可以认为是整个Pool的Global Version。所有存储在Ceph的Object也都带有snap_seq，而每个Object会有一个Head版本的，也可能会存在一组Snapshot objects，不管是Head版本还是snapshot object都会带有snap_seq，那么接下来我们看librbd是如何利用该字段创建Snapshot的。 用户申请为”pool”中的”image-name”创建一个名为”snap-name”的Snapshot librbd向Ceph Monitor申请得到一个”pool”的snap sequence，Ceph Monitor会递增该Pool的snap_seq，然后返回该值给librbd。 librbd将新的snap_seq替换原来image的snap_seq中，并且将原来的snap_seq设置为用户创建的名为”snap-name”的Snapshot的snap_seq。 每个Snapshot都掌握者一个snap_seq，Image可以看成一个Head Version的Snapshot，每次IO操作对会带上snap_seq发送给Ceph OSD，Ceph OSD会查询该IO操作涉及的object的snap_seq情况。如”object-1″是”image-name”中的一个数据对象，那么初始的snap_seq就”image-name”的snap_seq，当创建一个Snapshot以后，再次对”object-1″进行写操作时会带上新的snap_seq，Ceph接到请求后会先检查”object-1″的Head Version，会发现该写操作所带有的snap_seq大于”object-1″的snap_seq，那么就会对原来的”object-1″克隆一个新的Object Head Version，原来的”object-1″会作为Snapshot，新的Object Head会带上新的snap_seq，也就是librbd之前申请到的。 实验过程可以参考理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone],实验结论摘抄如下： rbd clone 实现原理从用户角度来说，clone操作实现了对某个只读snapshot 的image化，即clone之后，可以对这个clone进行读写，snapshot等等，跟一个rbd image一样。从系统实现来说，也是利用 COW方式实现，clone会将clone与snapshot 的父子关系保存起来,以备IO操作时查找。 实验验证部分参考理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone]。 结论摘抄如下： COW VS ROW要想了解COW 与ROW，首先需要知道快照的两种类型，一种是全量快照，一种是增量快照。 全量快照，顾名思义，就是为源数据卷创建并维护一个完整的镜像卷。 增量快照，利用COW 或ROW的方式实现的差量快照。 COW(COPY-ON-WRITE) 写时复制在创建快照的时候，仅仅复制一份数据指针表，当读取快照数据时，快照本身没有的话，根据数据指针表，直接读取源数据卷的对应数据块，在更新或写入源数据卷中的数据块时，先将原始数据copy到快照卷中（预留空间），更新快照卷的数据指针表，再将更新数据写入源数据卷。 写时复制的优势在于创建快照速度快（仅复制数据指针表），且占用存储空间少，但因为创建快照后每次写入操作都要进行一次复制才开始写入源数据，所以降低了源数据卷的写性能。所以COW 适合读多写少的场景，除此之外，如果一个应用容易出现对存储设备的写入热点(只针对某个有限范围内的数据进行写操作),也是比较理想的选择.因为其数据更改都局限在一个范围内（局部性原理）, 对同一份数据的多次写操作只会出现一次写时复制操作。 ROW(REDIRECT-ON-WRITE)写时重定向在创建快照的时候，仅仅复制一份数据指针表，当读取快照数据时，快照本身没有的话，根据数据指针表，直接读取源数据卷的对应数据块，在更新或写入源数据卷中的数据块时，将源数据卷数据指针表中的被更新原始数据指针重定向到新的存储空间，所以由此至终, 快照卷的数据指针表和其对应的数据是没有被改变过的。恢复快照的时候,只需要按照快照卷数据指针表来进行寻址就可以完成恢复了. 除了COW的优势外，ROW 因为更新源数据卷只需要一次写操作, 解决了 COW写两次的性能问题. 所以 ROW 最明显的优势就是不会降低源数据卷的写性能。但ROW 的快照卷数据指针表保存的是源数据卷的原始副本,而源数据卷数据指针表保存的则是更新后的副本,导致在删除快照卷之前需要将快照卷数据指针表指向的数据同步至源数据卷中. 而且当创建了多个快照后, 会产生一个快照链,使原始数据的访问、快照卷和源数据卷数据的追踪以及快照的删除将变得异常复杂且消耗时间。除此之外,因为源数据卷数据指针指向的数据会很快的被重定向分散, 所以 ROW 另一个主要缺点就是降低了读性能(局部空间原理)。在传统存储设备上,ROW快照在多次读写后,源数据卷的数据被分散,对于连续读写的性能不如COW. 所以 ROW 比较适合Write-Intensive(写密集) 类型的存储系统. 但是, 在分布式存储设备上, ROW 的连续读写的性能会比 COW 更加好. 一般而言,读写性能的瓶颈都在磁盘上.而分布式存储的特性是数据越是分散到不同的存储设备中, 系统性能越高。所以ROW的源数据卷重定向分散性反而带来了好处。 因此, ROW 逐渐成为了业界分布式存储的主流。 参考文章SNAPSHOTS 解析Ceph: Snapshot 关于Ceph的snapshot和clone Ceph Snapshots: Diving into Deep Waters 理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone] ROW/COW 快照技术原理解析 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>ceph</tag>
        <tag>snapshot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-- python threadpool 的前世今生]]></title>
    <url>%2F2017%2F05%2F18%2Fpython-thread-pool%2F</url>
    <content type="text"><![CDATA[引出首先需要了解的是threadpool 的用途，他更适合于用到一些大量的短任务合集，而非一些时间长的任务，换句话说，适合大量的CPU密集型短任务，那些消耗时间较长的IO密集型长任务适合用协程去解决。 目前，python 标准库（特指python2.X）中的threadpool模块是在 multiprocessing.pool.threadpool，或者multiprocessing.dummy.ThreadPool（dummy模块是针对threading 多线程的进一步封装）。该模块有个缺点就是在所有线程执行完之前无法强制退出。实现原理大同小异：实例化pool的时候会创建指定数目的线程，把task 传给一个task-queue，线程会读取task-queue 的task，没有就阻塞，读取到后就执行，并将结果交给一个result-queue。 除了标准库中的threadpool，还有一些使用比较多的threadpool，以下展开。 pip 中的 ThreadPool安装简单：pip install threadpool使用如下： 1234pool = ThreadPool(poolsize) # 定义线程池，指定线程数量requests = makeRequests(some_callable, list_of_args, callback) # 调用makeRequests创建了要开启多线程的函数，以及函数相关参数和回调函数 [pool.putRequest(req) for req in requests] # 所有要运行多线程的请求扔进线程池pool.wait() # 等待所有线程完成后退出 原理类似，源码解读可以参考python——有一种线程池叫做自己写的线程池 ,该博客还给出了对其的一些优化。 自己定制 threadpool根据需要的功能定制适合自己的threadpool 也是一种常见的手段，常用的功能比如：是否需要返回线程执行后的返回值，线程执行完之后销毁还是阻塞等等。以下为自己经常用的的一个比较简洁的threadpool，感谢@kaito-kidd提供，源码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# coding: utf8"""线程池，用于高效执行某些任务。"""import Queueimport threadingclass Task(threading.Thread): """ 任务 """ def __init__(self, num, input_queue, output_queue, error_queue): super(Task, self).__init__() self.thread_name = "thread-%s" % num self.input_queue = input_queue self.output_queue = output_queue self.error_queue = error_queue self.deamon = True def run(self): """run """ while 1: try: func, args = self.input_queue.get(block=False) except Queue.Empty: print "%s finished!" % self.thread_name break try: result = func(*args) except Exception as exc: self.error_queue.put((func.func_name, args, str(exc))) else: self.output_queue.put(result)class Pool(object): """ 线程池 """ def __init__(self, size): self.input_queue = Queue.Queue() self.output_queue = Queue.Queue() self.error_queue = Queue.Queue() self.tasks = [ Task(i, self.input_queue, self.output_queue, self.error_queue) for i in range(size) ] def add_task(self, func, args): """添加单个任务 """ if not isinstance(args, tuple): raise TypeError("args must be tuple type!") self.input_queue.put((func, args)) def add_tasks(self, tasks): """批量添加任务 """ if not isinstance(tasks, list): raise TypeError("tasks must be list type!") for func, args in tasks: self.add_task(func, args) def get_results(self): """获取执行结果集 """ while not self.output_queue.empty(): print "Result: ", self.output_queue.get() def get_errors(self): """获取执行失败的结果集 """ while not self.error_queue.empty(): func, args, error_info = self.error_queue.get() print "Error: func: %s, args : %s, error_info : %s" \ % (func.func_name, args, error_info) def run(self): """执行 """ for task in self.tasks: task.start() for task in self.tasks: task.join()def test(i): """test """ result = i * 10 return resultdef main(): """ main """ pool = Pool(size=5) pool.add_tasks([(test, (i,)) for i in range(100)]) pool.run()if __name__ == "__main__": main() 参考文章Python Thread Pool 理解python的multiprocessing.pool threadpool多线程 thread_pools.py Cinder磁盘备份原理与实践 python线程池（threadpool）模块使用笔记 thread_pool 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>python</tag>
        <tag>threadpool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- cinder-backup 利用ceph实现增量备份]]></title>
    <url>%2F2017%2F05%2F09%2Fopenstack-cinder-incremental-backup-with-ceph%2F</url>
    <content type="text"><![CDATA[配置首先如果想要实现 ceph backend cinder的增量备份的话，要保证cinder-volume 与 cinder-backup 的backend 都是ceph，涉及到具体实践的话就是openstack 与 ceph integration 的 配置，ceph官网有详实的步骤 BLOCK DEVICES AND OPENSTACK ,当然，中文版的一搜也一大把openstack的glance、nova、cinder使用ceph做后端存储 ,这里提一点，就是 ，如果cinder-volume 是默认以ceph rbd为backend，那么glance 配置文件其实也可以直接默认用cinder 作为默认存储，但最终其实都是存在ceph 里，glance配置文件可以类似这样，这样cinder list 也可以列出镜像盘： 123456 ......[glance_store]stores=file,http,cinder,rbddefault_store=cindercinder_os_region_name=RegionOne..... 还想提一点就是，backup 尽量使用 multi-ceph 的情况，因为个人感觉备份主要是为了容灾，单 ceph的情况其实容灾能力有限。 使用注：以Liberty版本示例使用非常简单，首先对 volume 做一个全量备份： 1$ cinder backup-create --name fullbackup &lt;volume-ID&gt; 然后再做一次备份就是增量备份了， 1$ cinder backup-create --name incrementalbackup --incremental &lt;volume-ID&gt; 其实不用加incremental tag也是可以的。 可以利用删除backup 来证明一下，增量备份都会有一个 parent backup（父备份），就是基于父备份来做增量备份，通常来说，就是我们刚开始做的那个全量备份。在删除父备份之前，必须先把所有它的子备份删除，才可以成功删除父备份。示例如下： 实现原理关于备份的具体流程可以参考这张图： 熟悉ceph的话，应该知道ceph rbd的export-diff，import-diff 功能： export-diff ：将某个 rbd image 在两个不同时刻的数据状态比较不同后导出补丁文件。 import-diff :将某个补丁文件合并到某个 rbd image 中。 ceph 增量备份就是基于这两个基本功能，详细命令示例可以参考rbd的增量备份和恢复 首次备份 在用作备份的ceph集群中新建一个base image，与源 volume 大小一致，name 的形式为 “volume-VOLUMD_UUID.backup.base” 源 rbd image 新建一个快照，name形式为 backup.BACKUP_ID.snap.TIMESTRAMP 源rbd image 上使用 export-diff 命令导出从刚开始创建时到上一步快照时的差异数据，其实就是现在整个rbd的数据，然后通过管道将差量数据导入刚刚在备份集群上新创建的base image中 再次备份 在要备份的源volume 中找到满足 r”^backup.([a-z0-9-]+?).snap.(.+)$” 的最近一次快照。 源volume 创建一个新的快照，name 形式为 backup.BACKUP_ID.snap.TIMESTRAMP。 源rbd image 上使用 export-diff 命令导出与最近的一次快照比较的差量数据，然后通过管道将差量数据导入到备份集群的rbd image中 恢复时相反，只需要从备份集群找出对应的快照并导出差量数据，导入到原volume即可 参考文章BLOCK DEVICES AND OPENSTACK openstack的glance、nova、cinder使用ceph做后端存储 Ceph backup driver Cinder磁盘备份原理与实践 Openstack 中cinder backup三种backend的对比 Inside Cinder’s Incremental Backup 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>ceph</tag>
        <tag>incremental backup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- 关于openstack HA 的一些记录]]></title>
    <url>%2F2017%2F04%2F14%2Fopenstack-ha%2F</url>
    <content type="text"><![CDATA[概念所谓HA(High Availability),高可用,就是指在本地系统的某个组件出现故障的情况下，系统依然可访问,应用的能力。而为了达到这种目的，我们一般采用节点冗余的方法组成集群来提供服务。通常来说，可以分为两种冗余的方式： Active/Passive HA: 即A/P模式，中文主备模式，通常会有一个主节点来提供服务，备用节点会同步/异步与主节点进行数据同步。当主节点故障时，备用节点会启用代替主节点。通常会使用CRM软件比如Pacemaker来进行主备节点之间的切换并提供一个VIP提供服务。 Active/Active HA:也就是我们常说的A/A方式，中文也称作双活或多主模式。这种模式下，系统在集群的所有服务器上运行同样的负载，也就是说集群内服务器的功能都是一样的。这种模式下，通常会采用负载均衡软件如HAProxy提供VIP进行负载均衡。 涉及到具体的服务时，HA的部署方式也不尽相同，一般来说，一些无状态的服务比如提供http请求转发的http服务器等，可以直接部署A/A模式，无需考虑数据不一致的问题。而一些有状态的服务，比如数据库等，则需要具体问题具体对待，如果这类服务提供原生的A/A服务，那么尽量利用他们提供的原生A/A服务。如果实在不行那么只能从A/P角度去部署。 关于openstack 的HA根据openstack部署节点的功能来分，可以将各节点分为以下四种，这四种节点的HA方式也不尽相同： Cloud Controller Node （云控制节点）：安装各种 API 服务和内部工作组件（worker process）。同时，通常也会将共享的 DB 和 MQ 安装在该节点上。 Neutron Controller Node （网络控制节点）：安装 Neutron L3 Agent，L2 Agent，LBaas，VPNaas，FWaas，Metadata Agent 等 Neutron 组件。 Storage Controller Node （存储控制节点）：安装 Cinder volume 以及 Swift 组件。 Compute node （计算节点）：安装 Nova-compute 和 Neutron L2 Agent，在该节点上创建虚机。 本文接下来就主要按照这四个节点的顺序记录下对应的HA方案，通常来说，部署的原则遵循尽量A/A,尽量采用原生支持的HA方案,考虑负载均衡，以及方案尽量简单。 云控制节点 HA注：这里只考虑A/A方案，A/P方案可以通过Pacemaker+Corosync 的方式搭建集群，业界应用的比较少。 首先看下控制节点中的各种服务，对于无状态服务（API服务+内部工作组件如nova-scheduler等），我们可以直接多节点部署，如果是提供API 的利用pacemaker 提供VIP,HAproxy提供负载均衡。对于有状态的服务我们可以尽量使用原生的AA方案，罗列如下（摘自理解 OpenStack 高可用（HA）（1）：OpenStack 高可用和灾备方案 [OpenStack HA and DR]）： API 服务：包括 *-api, neutron-server，glance-registry(注：glance api v2 版本将该服务与glance-api集合到了一起), nova-novncproxy，keystone，httpd 等。由 HAProxy 提供负载均衡，将请求按照一定的算法转到某个节点上的 API 服务。由 Pacemaker 提供 VIP。 内部组件：包括 *-scheduler，nova-conductor，nova-cert 等。它们都是无状态的，因此可以在多个节点上部署，它们会使用 HA 的 MQ 和 DB。RabbitMQ：跨三个节点部署 RabbitMQ 集群和镜像消息队列。可以使用 HAProxy 提供负载均衡，或者将 RabbitMQ host list 配置给 OpenStack 组件（使用 rabbit_hosts 和 rabbit_ha_queues 配置项）。 MariaDB：至少跨三个节点部署 Gelera MariaDB 多主复制集群。由 HAProxy 提供负载均衡。 HAProxy：向 API，RabbitMQ 和 MariaDB 多活服务提供负载均衡，其自身由 Pacemaker 实现 A/P HA，提供 VIP，某一时刻只由一个HAProxy提供服务。在部署中，也可以部署单独的 HAProxy 集群。 Memcached：它原生支持 A/A，只需要在 OpenStack 中配置它所有节点的名称即可，比如，memcached_servers = controller1:11211,controller2:11211。当 controller1:11211 失效时，OpenStack 组件会自动使用controller2:11211。 从一个请求的角度大致时序如下： 这里在重点说一下rabbitmq 与mysql用的HA方案： rabbitmq 的HA首先了解下rabbitmq集群的消息传递模式，rabbitmq的集群有两种模式，如下： 普通模式：默认的集群模式，以两个节点（rabbit01、rabbit02）为例来进行说明。对于Queue来说，消息实体只存在于其中一个节点rabbit01（或者rabbit02），rabbit01和rabbit02两个节点仅有相同的元数据，即队列的结构。当消息进入rabbit01节点的Queue后，consumer从rabbit02节点消费时，RabbitMQ会临时在rabbit01、rabbit02间进行消息传输，把A中的消息实体取出并经过B发送给consumer。所以consumer应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理Queue。否则无论consumer连rabbit01或rabbit02，出口总在rabbit01，会产生瓶颈。当rabbit01节点故障后，rabbit02节点无法取到rabbit01节点中还未消费的消息实体。如果做了消息持久化，那么得等rabbit01节点恢复，然后才可被消费；如果没有持久化的话，就会产生消息丢失的现象。该模式只是起到单纯的扩展作用（如增加queue的存储空间等），并没有达到HA 的效果。 镜像模式：将需要消费的队列变为镜像队列，存在于多个节点，这样就可以实现RabbitMQ的HA高可用性。作用就是消息实体会主动在镜像节点之间实现同步，而不是像普通模式那样，在consumer消费数据时临时读取。缺点就是，集群内部的同步通讯会占用大量的网络带宽。 rabbitmq的 A/P方案可以采用Pacemaker + （DRBD 或者其它可靠的共享 NAS/SNA 存储） + （CoroSync 或者 Heartbeat 或者 OpenAIS）来实现，安装配置可以参考理解 OpenStack 高可用（HA）（5）：RabbitMQ HA. openstack 官方推荐采用 rabbitmq 镜像模式实现A/A, 虽然是A/A，但是rabbitmq 镜像模式集群中还是有master/slave的概念,这个概念是针对queue的，通常创建queue的节点为该queue 的master节点，其他节点为slave节点。通常是一个master,多个slave ，master节点失效后，会自动选出另一个master。对于消息的发布，producer可以连接任意节点，如果该节点不是master，则会转发给master，master会向其他slave节点发送该消息，后进行消息本地化处理，并组播复制消息到其他节点存储；对于consumer，可以选择任意一个节点进行连接，消费的请求会转发给master,为保证消息的可靠性，consumer需要进行ack确认，master收到ack后，才会删除消息，ack消息会同步(默认异步)到其他各个节点，进行slave节点删除消息。可以看到虽然起到了H/A的效果，但是并没有达到减轻负载的作用，所以rabbitmq mirror 不支持负载均衡。关于这部分详细讲解可以参考Highly Available (Mirrored) Queues openstack rabbitmq H/A 的具体安装配置参考Messaging service for high availability mysql 的HAmysql 的HA 方案有很多，这里只讨论openstack 官方推荐的mariadb galara 集群。Galera Cluster 是一套在innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到各个节点上去。在数据方面完全兼容 MariaDB 和 MySQL。 特点如下： 1）同步复制，（&gt;=3）奇数个节点 2）Active-active的多主拓扑结构 3）集群任意节点可以读和写 4）自动身份控制,失败节点自动脱离集群 5）自动节点接入 6）真正的基于”行”级别和ID检查的并行复制 7）无单点故障,易扩展 openstack doc 中提到在用HAproxy做galara cluster 的负载均衡时，因为该集群不支持跨节点对表加锁，也就是说如果OpenStack 某组件有两个会话分布在两个节点上同时写入某一条数据，会出现其中一个会话遇到死锁的情况。可以参考Understanding reservations, concurrency, and locking in Nova,讲解的比较清晰。 解决方案上文作者就提出了一种方式，楼主看了下最新版代码已经没有with_lockmode(‘update’) 语句。 安装配置参考Database (Galera Cluster) for high availability 网络控制节点 HA这里说的网络控制节点是为了方便描述，并不是neutron的所有服务都在该节点，看下图， 可以认为neutron-service在控制节点，L3 agent,DHCP agent, Metadata agent,L2 agent等所在节点为网络控制节点，接下来就是针对具体的服务讨论HA 的方案，通常来说，我们只需考虑L3 agent,DHCP agent 这两个服务的HA即可，L2 agent只在所在的网络或者计算节点上提供服务，不需要HA。neutron-metadata-agent需要和 neutron-ns-metadata-proxy 通过soket 通信，可以在所有 neutron network 节点上都运行该 agent，只有 virtual router 所在的L3 Agent 上的 neutron-metadata-agent 才起作用，别的都standby。 L3 agent HAL3 agent 的HA ,官方给出的方案有两种，一种是利用VRRP协议（虚拟路由冗余协议）实现，另一种是DVR(分布式虚拟路由)实现。关于VRRP协议，可以参考Neutron L3 Agent HA 之 虚拟路由冗余协议（VRRP）, 具体配置以及网络的连接情况直接参考官方给的guide 同样，关于DVR的资料参考Neutron 分布式虚拟路由（Neutron Distributed Virtual Routing）,以及官方guide DHCP agent HADHCP协议本身支持多个DHCP服务器，只需修改配置，为每个租户网络创建多个DHCP agent,即可实现HA。 存储控制节点 HAcinder-volume的HA A/A方案目前还未实现，只能采取pacemaker 实现H/A，不过，随着openstack Tooz 项目的开发完善，cinder-volume的A/A方案也渐渐明朗，就是采用openstack Tooz 项目实现的分布式锁来实现。详细信息参考cinder-volume如何实现AA高可用 计算节点 HA包括计算节点和虚拟机的HA,社区从2016年9月开始一直致力于一个虚拟机HA的统一方案，详细参考igh Availability for Virtual Machines.目前还是处于开发阶段。 业界目前使用的方案大致有以下几种： controller节点与compute节点通信（ping等），检查nova 服务运行状态，对于有问题的节点进行简单粗暴的evacuate. Pacemaker-remote： 突破Corosync的集群规模限制，参考RDO的方案 集中式检查 分布式健康检查，参考分布式健康检查：实现OpenStack计算节点高可用 参考文章openstack doc 世民谈云计算 Understanding reservations, concurrency, and locking in Nova Fuel openstack HA guide RabbitMQ分布式集群架构和高可用性（HA） Highly Available (Mirrored) Queues 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>HA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-- 常见的linux 服务器性能监控工具]]></title>
    <url>%2F2017%2F04%2F06%2Flinux-performance-monitoring-tool%2F</url>
    <content type="text"><![CDATA[引出性能监控是服务器运维非常重要的一部分，本文记录非常好用的几个性能监控工具，算是一个小的总结。本文组织方式是按cpu,mem(内存)，网络，磁盘io等具体性能监控的方向分类，末尾介绍一些相对大而全，涵盖各方面的企业级监控工具。。 cpu/mem 监控工具top这应该是第一个想到的性能监控命令，示例截图如下： 我们关注得点应该包括：cpu,mem负载，load-average，以及占用内存或cpu最多的进程。该命令还有一些交互的子命令，比如 “c” ,是按照CPU 占用排序，”m”是按照内存占用排序等。 sar该命令号称系统监控的瑞士军刀，目前Linux上最为全面的系统性能分析工具之一，可以从14个大方面对系统的活动进行报告，包括文件的读写情况、系统调用的使用情况、串口、CPU效率、内存使用状况、进程活动及IPC有关的活动等，使用也是较为复杂。sar 默认显示的是从零点开始每隔十分钟到现在的CPU情况，如果是查看之前的报告，需要指定日志报告，sar -f /var/log/sysstat/sa25 。 解释下各列的指标： %user 用户模式下消耗的CPU时间的比例； %nice 通过nice改变了进程调度优先级的进程，在用户模式下消耗的CPU时间的比例 %system 系统模式下消耗的CPU时间的比例； %iowait CPU等待磁盘I/O导致空闲状态消耗的时间比例； %steal 利用Xen等操作系统虚拟化技术，等待其它虚拟CPU计算占用的时间比例； %idle CPU空闲时间比例； 查看 内存使用情况：sar -r 查看内存页面交换发生状况：sar -W 查看带宽信息：sar -n DEV 要判断系统瓶颈问题，有时需几个 sar 命令选项结合起来； 怀疑CPU存在瓶颈，可用 sar -u 和 sar -q 等来查看 怀疑内存存在瓶颈，可用sar -B、sar -r 和 sar -W 等来查看 怀疑I/O存在瓶颈，可用 sar -b、sar -u 和 sar -d 等来查看 更详细的命令参数可以参考10 Useful Sar (Sysstat) Examples for UNIX / Linux Performance Monitoring 网络监控工具netstat该命令会显示各种与网络相关的信息，比如网络连接，路由表，接口状态等。 解释下主要有两个部分： Active Internet connections，称为源TCP连接，其中”Recv-Q”和”Send-Q”指接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积 Active UNIX domain sockets，称为有源Unix域套接口(跟网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。Proto显示连接使用的协议,RefCnt表示连接到本套接口上的进程号,Types显示套接口的类型,State显示套接口当前的状态,Path表示连接到套接口的其它进程使用的路径名。常用参数如下： a (all)显示所有选项，默认不显示LISTEN相关 t (tcp)仅显示tcp相关选项 u (udp)仅显示udp相关选项 n 拒绝显示别名，能显示数字的全部转化成数字。 l 仅列出有在 Listen (监听) 的服務状态 iptraf非常实用的tcp/udp网络监控工具，有一个非常简洁的界面，常用的功能包括： IP流量监控器，用来显示网络中的IP流量变化信息。包括TCP标识信息、包以及字节计数，ICMP细节，OSPF包类型。 简单的和详细的接口统计数据，包括IP、TCP、UDP、ICMP、非IP以及其他的IP包计数、IP校验和错误，接口活动、包大小计数。 TCP和UDP服务监控器，能够显示常见的TCP和UDP应用端口上发送的和接收的包的数量。局域网数据统计模块，能够发现在线的主机，并显示其上的数据活动统计信息。 TCP、UDP、及其他协议的显示过滤器，允许你只查看感兴趣的流量。 以下为一个查看网卡统计信息的界面： 大而全的企业级性能监控工具目前开源的企业级性能监控工具应用比较广泛的应该属Nagios 与Zabbix了，关于两者对比，可以参考： Zabbix vs Nagios vs PandoraFMS: an in depth comparison 开源监控系统中 Zabbix 和 Nagios 哪个更好？ 参考文章10 Useful Sar (Sysstat) Examples for UNIX / Linux Performance Monitoring 你需要知道的16个Linux服务器监控命令 linuxtools-rst iptraf：一个实用的TCP/UDP网络监控工具 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>linux</tag>
        <tag>performance monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- Open vSwitch 安装以及常用操作]]></title>
    <url>%2F2017%2F03%2F16%2Fopenvswitch-quick-start%2F</url>
    <content type="text"><![CDATA[引出openvswitch,简称ovs, 是一个开源的分布式虚拟多层交换机，随着云计算的崛起，它逐渐成为网络虚拟化的基石。在openstack 中，默认也是采用ovs作为二层网络实现的组件。本篇博客即是对ovs安装，以及框架，常用命令的一个记录。 在CentOS 7 上安裝 Open vSwitch确定linux核心版本,以安装对应版本OVS1$ uname -r 根据下图查看需要安装哪一个版本，一般来说，如果kernel支持的话，最好下载2.5.1版，也就是目前的LTS(Long Term Support)版。 安装 toolchain 以及一些依赖包目前ovs安装还是需要自己编译，所以需安装toolchain。 123$ yum groupinstall "Development Tools"$ yum install openssl-devel wget kernel-devel 下载源文件并打包为rpm包123456789$ wget http://openvswitch.org/releases/openvswitch-2.5.1.tar.gz #下载$ tar xzvf openvswitch-2.5.1.tar.gz #解压缩$ mkdir -p ~/rpmbuild/SOURCES &amp; cp openvswitch-2.5.1.tar.gz ~/rpmbuild/SOURCES/ #创建一个打包目录并复制源文件$ sed 's/openvswitch-kmod, //g' openvswitch-2.5.1/rhel/openvswitch.spec &gt; openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec # 修改spec文件$ rpmbuild -bb --nocheck ~/openvswitch-2.5.1/rhel/openvswitch_no_kmod.spec # 打包 完成后我们在~/mbuild/RPMS/x86_64/目录下有两个rpm包，其中，openvswitch-2.5.1-1.x86_64.rpm就是我们需要的rpm包。 安装rpm包1234567$ yum localinstall /root/rpmbuild/RPMS/x86_64/openvswitch-2.5.1-1.x86_64.rpm$ ovs-vsctl -V #查安装是否成功$ systemctl start openvswitch.service #启动ovs service（即守护进程）$ chkconfig openvswitch on #设置开机自启动 OpenvSwitch的架构以及基本概念ovs架构 主要由三个组件组成： ovs-vswitchd ：ovs的守护进程。 ovsdb-server ：ovs的轻量数据库服务。 openvswitch_mod.ko ：ovs的内核模块，主要是利用datapath将flow的match结果缓存起来。 ovs 基本概念因为ovs最常用到的场景是neutron，所以这里以neutron vxlan为例子讲下，下图就是一个neutron vxlan的示意图： bridge: ovs中的网桥可以理解为现实世界中的物理交换机，作用就是根据一定流规则（如openflow），把从某个端口(port)收到的数据包转发到另一个或多个端口。图中br-int与br-tun就是两个bridge。 port: ovs 中的port是收发数据包的单元,每个端口都附属于某一个bridge。图中qvoxxx，以及patch-tun,patch-int等都是port。 interface: 接口是ovs与外部交换数据包的组件。一个接口就是操作系统的一块网卡，这块网卡可能是ovs生成的虚拟网卡，也可能是物理网卡挂载在ovs上，也可能是操作系统的虚拟网卡（TUN/TAP）挂载在ovs上。 flowtable: 上文讲过，ovs会根据流表进行包的转发，最常用的就是openflow协议，下图为openflow的流表匹配顺序：首先按照从小到大的顺序匹配流表，表内按照优先级匹配表项，示例参见openstack– neutron 二/三层网络实现探究。 OpenvSwitch的常用命令ovs的操作命令大致有如下四种： ovs-vsctl用于控制ovs db ovs-ofctl用于管理OpenFlow switch 的 flow ovs-dpctl用于管理ovs的datapath ovs-appctl用于查询和管理ovs daemon ovs的常用子命令如下,直接引用自OVS常用操作总结： ovs-dpctl show -s ovs-ofctl show, dump-ports, dump-flows, add-flow, mod-flows, del-flows ovsdb-tools show-log -m ovs-vsctl show 显示数据库内容 关于桥的操作 add-br, list-br, del-br, br-exists. 关于port的操作 list-ports, add-port, del-port, add-bond, port-to-br. 关于interface的操作 list-ifaces, iface-to-br ovs-vsctl list/set/get/add/remove/clear/destroy table record column [value], 常见的表有bridge, controller,interface,mirror,netflow,open_vswitch,port,qos,queue,ssl,sflow. ovs-appctl list-commands, fdb/show, qos/show OpenvSwitch实践参考基于 Open vSwitch 的 OpenFlow 实践 参考文章CentOS 7 安裝 Open vSwitch Open vSwitch and OpenStack Neutron troubleshooting ovs wiki Open vSwitch的ovs-vsctl命令详解 OVS常用操作总结 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>openvswitch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- 对neutron一些常见网络问题的troubleshooting]]></title>
    <url>%2F2017%2F03%2F13%2Fneutron-troubleshooting-common-problems%2F</url>
    <content type="text"><![CDATA[引出一直对neutron网络的troubleshooting 感到头疼，今天偶然翻到一篇博客，感觉写的非常棒，Openstack Neutron: troubleshooting and solving common problems,这篇博客也是两个教学视频的总结，顺便看了一下视频，感觉内容差不多就没看完，链接在后面。 本篇博客算是对上述博客以及视频的一个翻译（没有严格翻译）与整理，以及自己补充的一些东西。主要解决四类常见的网络问题： 无法用private ip ping/ssh 通虚拟机 虚拟机无法连通外部网络 虚拟机无法连通 metadata server VIF plugging timeout (vif 插件超时) 常见问题分类问题的原因通常来说分为以下两种： 配置错误这是最常见的错误原因，因为配置文件的错误配置导致失败。这里的配置文件不止是neutron的配置文件，还有相关的nova,cinder,各种agent甚至物理机网络的配置。需要注意的是，宿主机网络的配置错误会严重影响neutron 的使用，因为所有的网络包最终都是通过物理网络来传输。所以，对neutron 进行trouble shooting之前，首先确定宿主机之间是可以联通的。 代码出现bug出现这种情况的话，一般在neutron的launchpad 会有对应的bug report。如果没有的话，那就自己提一个，会有社区的developer进行 bug fixed。 第一类问题：无法用private ip ping/ssh 通虚拟机在解决这个问题之前，我们首先要了解一个虚拟机是如何获取IP的。 在neutron 中，有一个agent叫做 DHCP agent，它通过 RPC 跟 neutron-server 通信，它会为每一个 network 创建一个dhcp namaspace，以实现网络的隔离。在这个namespace里，有一个dnsmasq的进程，它是真正实现分配IP 的服务。 关于这部分的具体代码分析，可以参考Neutron 理解（5）：Neutron 是如何向 Nova 虚机分配固定IP地址的 （How Neutron Allocates Fixed IPs to Nova Instance）接下来我们看下packets 的flow流向，以便更精准的定位问题,因为二层网络的实现方式不一样——————openvswitch方式和Linux bridge方式，所以流向也不尽相同，先看下ovs的流向。 流程图一目了然，原文其实做了很多解释，这里就不多说了，如果不清楚的可以参看原文，或者看我之前的一篇openstack– neutron 二/三层网络实现探究 同样，linux bridge实现的二层网络，流向如下： ok,接下来开始debug: 首先利用nova list 确定虚拟机是 active状态,如果虚拟机是failure，那么可以用 nova show 查看failure原因，以及查看 nova日志。 确定虚拟机是running 的，接下来就要进行网络的分析，首先需要确定两件事：一是物理网络是没问题的，二是虚拟机的security group是允许ping/ssh的。 接下来查看port绑定是否成功，分为在虚拟机上的port绑定和在DHCP/router上的绑定。一般来说虚拟机上的port绑定是比较容易被发现的，因为在boot虚拟机的过程中一旦绑定不成功就会报错。而在DHCP/router上的绑定，因为是异步过程所以不容易被发现（当创建一个port的时候即使最终port binding失败，但是也会返回创建成功）。这就需要我们使用 neutron port-show PORT_ID 查看对应port是否绑定成功，不成功的话会如下图所示： 一般来说，port绑定不成功的话有两个原因：第一个原因是OVS agent 挂掉了，可以使用 neutron agent-list查看，OVS agent挂掉的另一个显著特征是 ovs-vsctl show | grep tap -A 3 发现br-int网桥的tap设备没有vlan tag. 第二个原因就是neutron agent 或者neutron server配置错误。 接下来看一下虚拟机是否获取到IP。通过 vnc-console 进入虚拟机，执行ip a 命令，如果没有ip的话，首先查看neutron dhcp agent是否挂掉，还是用neutron agent-list命令查看。如果agent没有挂掉的话，再去查看下对应network的dnsmasq服务是否挂掉，使用该命令： ps -ef | grep dnsmasq | grep Network_Id 。再去查看一下对应network的dhcp host文件，是否有对应虚拟机的mac与ip 记录，该文件的位置为：/var/lib/neutron/dhcp/Network_ID/host。如果还是一切正常，那么去查看dhcp-agent 的日志。同时，我们要保证宿主机与虚拟机之间网络是可以联通的，可以通过手动设置虚拟机ip的方法进行验证下。 如果还是没有解决问题，那就只有一个方法了，使用tcpdump 抓包吧。 第二类问题：虚拟机无法连通外部网络解决这类问题，首先要弄懂L3 agent的工作原理。说白了，就是利用namespace+iptables实现的，参考openstack– neutron 二/三层网络实现探究。 还是看下包流向，因为2层网络的实现方式不同，流向也不尽相同。首先看下ovs方式的流向： 再看下linux bridge的方式，图示为从外部网络ping 虚拟机floating ip过程。 debug步骤如下： 查看security group 是否允许ping。可以的话，首先要保证宿主机与private ip之间是可以ping通的，如果两者ping不通，那么使用floating ip也是绝壁不可能ping通的。 接下来从两个角度入手：从虚拟机到网络节点的对应router是否联通，从router到external net是否联通。 从虚拟机到router:通过 vnc-console进入虚拟机，查看ip a 是否获取到ip,查看路由信息：route -n，查看能否ping 通默认网关（即路由器IP） 从router到external net：查看在router namespace内能否ping通外部网络：ip netns exec qrouter-078766fe-c4b6-4a14-82fa-e7f85e26c248 ping www.baidu.com 。查看router namespace能否ping通 floating ip: ip netns exec qrouter-078766fe-c4b6-4a14-82fa-e7f85e26c248 ping Floating-IP .这看起来可能有点傻，因为floating ip其实就在这个namespace中，不过这会让我们对问题的严重性有一个认识。 如果还是没有找到问题，就去查看l3-agent 日志。 第三类问题：虚拟机无法连通 metadata server还是先要搞明白metadata server的原理。metadata 获取有两种方式，一种是config drive，一种是metadata server。这里就是讨论metadata server的方式，metadata server 的路由方式有两种：如果虚拟机与router联通的话，就通过router路由，如果虚拟机没有router直连的话，就通过对应dhcp namespace路由。参考openstack– openstack instance metadata 服务机制探索。 首先看下通过路由器路由的work-flow: 注意：metdata proxy 是由l3-agent实现，它会在指定端口监听（默认 9697），接收到request后，会增加虚拟机ip,router id到request header并转发到metadata agent。 再看下没有路由器直连（isolated-network）的情况: 注意：要想开启这一功能，需要在dhcp-agent配置文件中设置：enable_isolated_metadata = True. 接下来开始debug: 首先查看metadata-agent是否挂掉：neutron agent-list 还是两个角度：从虚拟机到router/dhcp namespace，从router/dhcp namespace 到nova-api-metadata。 从虚拟机到router/dhcp namespace：ping一下查看是否连通。 查看router/dhcp namespace 是否有metadata-proxy 在监听：ip netns exec qrouter-f6396cfe-2ac9-4d6a-9437-eb8e7d26c776 ps -ef | grep metadata-proxy 。如果这里出现错误的话，可以去查看下metadata-agent 的日志，以及对应 namespace的neutron-ns-metadata-proxy的日志。 从router/dhcp namespace 到nova-api-metadata：ip netns exec qrouter-f6396cfe-2ac9-4d6a-9437-eb8e7d26c776 ping Metadata-Server IP 终极大招：tcpdump 第四类问题： VIF plugging timeout这类问题通常出现在boot虚拟机的时候，跟L2 agent(ovs/bridge agent)有关。 workflow如下： 当nova 发出allocate_network 请求时，会设置一个默认5分钟的等待时间，如果5分钟过后还没有收到neutron 回复，就会报 VIF plugging timeout 这个错误。 debug步骤： 查看对应host的nova-computer日志，以及openvswitch-agent日志。 查看网络节点neutron-server日志。 如果是做压测，在Nova的配置文件中，可以适当调大这个vif_plugging_timeout 时间，或增加rpc_thread_pool_size &amp; rpc_conn_pool_size 。 原文还列举了一些常用的网络调试命令，因为之前总结过，openstack–openstack 常用网络调试命令,不再赘述。 参考文章Openstack Neutron: troubleshooting and solving common problems I Can’t Ping My VM! Learn How to Debug Neutron and Solve Common Problems OpenStack Neutron Troubleshooting Neutron 理解（5）：Neutron 是如何向 Nova 虚机分配固定IP地址的 （How Neutron Allocates Fixed IPs to Nova Instance） 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>neutron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-- 利用LVM 对虚拟机进行手动扩容]]></title>
    <url>%2F2017%2F03%2F10%2FLVM-command%2F</url>
    <content type="text"><![CDATA[引出一般来说，如果openstck镜像中安装了growpart(之前的版本叫做growroot),在创建虚拟机的时候就会实现自动扩容。但是今天碰到几个虚拟机没有实现自动扩容的情况，需要手动进行扩容，用到lvm的一些知识，这里记录一下。 关于LVM的一些知识LVM利用Linux内核的device-mapper来实现存储系统的虚拟化（系统分区独立于底层硬件）。通过LVM，你可以实现存储空间的抽象化并在上面建立虚拟分区，可以更简便地扩大和缩小分区，可以增删分区时无需担心某个硬盘上没有足够的连续空间。 简单来说：LVM是Linux环境中对磁盘分区进行管理的一种机制，是建立在硬盘和分区之上、文件系统之下的一个逻辑层，可提高磁盘分区管理的灵活性。 LVM的基本组成块如下： 物理卷Physical volume (PV)：可以在上面建立卷组的媒介，可以是硬盘分区，也可以是硬盘本身或者回环文件（loopback file）。物理卷包括一个特殊的header，其余部分被切割为一块块物理区域（physical extents）。 卷组Volume group (VG)：将一组物理卷收集为一个管理单元。 逻辑卷Logical volume (LV)：虚拟分区，由物理区域（physical extents）组成。 对虚拟机进行手动扩容注：本文测试为实验环境，直接利用workstation 开启虚拟机，扩展磁盘，最后实现手动扩展。 首先，确认磁盘使用lvm以及确定磁盘大小： 可以看到磁盘/dev/sda大小总共约为53G，/dev/sda2是已经加入卷组的一个物理卷。 /dev/mapper/cl-root 与/dev/mapper/cl-swap 设备其实是逻辑卷。 接下来在workstation面板进行磁盘扩展： 查看磁盘情况： 可以看到磁盘已经扩展为60G+了，但是磁盘分区还是两个且大小没变，而且挂载root的/dev/mapper/cl-root依然是50G,所以接下来我们需要做的就是将多出来的10G空间进行磁盘分区并最终扩展到逻辑卷dev/mapper/cl-root。 首先磁盘分区并将该分区设置为LVM格式。 重启机器（或者不重启，执行 partprobe 命令让内核更新分区表），将/dev/sda3创建物理卷，并将该物理卷加入卷组。 扩展逻辑卷并将对应的文件系统扩展，如果是ext3/4文件系统用resize2fs 命令。 扩展成功。 参考文章LVM WIKI LVM逻辑卷管理配置小结 Linux LVM逻辑卷配置过程详解 How to Increase the size of a Linux LVM by expanding the virtual machine disk 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>linux</tag>
        <tag>lvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- 如何升级现有镜像]]></title>
    <url>%2F2017%2F03%2F07%2Fupdate-your-openstack-image%2F</url>
    <content type="text"><![CDATA[引出公司云平台有些镜像的密码注入出了点问题，问题的原因初步诊断为镜像问题，需要对这些镜像进行cloudinit/cloudbase的升级操作。对于已知密码的镜像，可以直接开启一个虚拟机，完成升级后，对该虚拟机做snapshot 操作（nova image-create）即可。对于不知道密码的镜像，经过一番搜索后，可以通过将镜像挂载到本地宿主机，然后chroot的方法进行升级,后来又从官网了解到利用libguestfish 的方法。以下为一些简单记录。 利用 nova image-create 升级镜像 利用镜像开启一个虚拟机 进入虚拟机做升级操作，并删除/var/lib/cloud 下的文件，windows需删除LocalScripts/下的文件。 完成操作后，退出虚拟机并对该虚拟机做snapshot操作。 镜像完成。 利用 本地挂载的方法升级镜像如果忘记密码的话我们可以使用该方法进行镜像的升级，不过因为glance后端存储方式的不同，挂载到本地HOST的方法也不同，下文分别以本地存储，ceph存储讲解。 glance 存储为本地存储镜像格式为raw格式且镜像文件系统中使没有用LVM1234567891011$ fdisk -lu cirros-0.3.0-x86_64-disk.raw # 查看该镜像的分区情况以及是否使用LVM$ kpartx -av cirros-0.3.0-x86_64-disk.raw #读取镜像的分区表，然后生成代表相应分区的设备/dev/loop0 ，/dev/loop1等等$ mount /dev/mapper/loop0p1 /mnt/image # 将需要挂载的分区设备进行挂载，此时进入/mnt/image 应该就可以看到熟悉的系统文件目录了 $ cd /mnt/image &amp;&amp; cp /etc/resolv.conf etc/resolv.conf # 覆盖dns$ chroot /mnt/image bash #chroot到镜像文件中$ source /etc/profile &amp;&amp; source ~/.bashrc #初始化环境变量 然后就可以进行升级操作了，操作完成后，需要将临时文件清除，yum clean all 等。最后退出chroot 并进行unmount 操作： 12345$ exit # 退出chroot$ umount /mnt/image $ kpartx -d cirros-0.3.0-x86_64-disk.raw # 删除设备映射关系 镜像格式为raw格式且镜像文件系统中使用LVM1234567891011121314151617$ fdisk -lu centos7-x86_64.img # 查看该镜像的分区情况以及是否使用LVM$ kpartx -av centos7-x86_64.img # 读取镜像的分区表，然后生成代表相应分区的设备/dev/loop0 ，/dev/loop1等等$ pvscan # 查找lvm设备并记下vg的名字示例为centos$ vgchange -ay centos # 激活vg中的lvm$ lvs # 查看lvm设备列表$ mount /dev/centos/root /media/ #挂载对应的lvm$ cd /media/ &amp;&amp; cp /etc/resolv.conf etc/resolv.conf$ chroot /mnt/image bash #chroot到镜像文件中$ source /etc/profile &amp;&amp; source ~/.bashrc #初始化环境变量 退出chroot 并进行unmount 操作如下： 1234567$ exit # 退出chroot$ umount /media/ $ vgchange -an centos # deactive vg 中的lvm$ kpartx -d centos7-x86_64.img # 删除设备映射关系 如下为一个完整实例（省略chroot操作）： 镜像格式为qcow2格式qcow2 格式的镜像需要使用qemu自带的一个工具qemu-nbd，该工具会生成一个nbd(network block device)，然后将镜像映射到该nbd,便可以像普通block设备一样挂载了。 1234567891011$ modinfo nbd # 查看系统kernel是否支持nbd模块$ modprobe nbd max_part=16 # 加载 nbd模块$ lsmod | grep nbd # 查看nbd模块是否加载$ qemu-nbd -c /dev/nbd0 centos7.qcow2 # 将qcow2镜像映射为网络块设备(nbd)$ ll /dev/nbd0* # 查看映射情况$ mount /dev/nbd0p1 /media/ # 挂在对应的nbd设备 卸载的步骤如下： 123$ umount /media$ qemu-nbd -d /dev/nbd0 如果该qcow2镜像使用LVM的话，前半段挂载nbd设备，后半段参照raw格式。 glance 存储为ceph存储首先需要知道ceph作为glance存储后端的时候，openstack会充分利用ceph 的分层clone的特性来支持快速创建虚拟机。镜像会首先生成一个以snap结尾命名的快照，以后每次开启一个虚拟机都会clone该snapshot。该snapshot是protected，所以无法更改，而如果直接修改image的话，无法同步对应的snapshot。所以最稳妥的方法是先复制一个rbd image，然后修改这个复制的image，修改成功后覆盖或直接删除之前的image。 1234567$ rbd cp $POOL/$&#123;IMAGE_ID&#125; $POOL/$&#123;IMAGE_ID&#125;_copy # 复制image （允许跨pool）$ rbd map $POOL/$&#123;IMAGE_ID&#125;_copy # 挂载rbd镜像到本地（注意kernel的支持）$ mount /dev/rbd0p1 /mnt # 挂载对映的rbd到挂载点(rbd0p1 只是示例，依据真实情况填写)$ ll /mnt 之后就可以执行chroot 以及镜像的更新操作了。之后的umount操作如下：123$ umount -r -l /mnt/ $ rbd unmap /dev/rbd0 #从rbd中卸载 再然后就是更新glance了，大致步骤如下： 123456789101112131415$ glance image-create --name NewImage # 创建image，占个坑，不上传，得到new imageID $ rbd mv $POOL/$&#123;IMAGE_ID&#125;_copy $POOL/$&#123;NEW_IMAGE_ID&#125; # rbd改名$ rbd --pool=$POOL --image=$&#123;NEW_IMAGE_ID&#125; --snap=snap snap create # 创建snapshot$ rbd --pool=$POOL --image=$&#123;NEW_IMAGE_ID&#125; --snap=snap snap protect #对snapshot做保护$ glance image-update --name="$DISPLAY_NAME" --disk-format=raw --container-format=bare --is-public=True $&#123;NEW_IMAGE_ID&#125; # 更新glance 元数据$ glance image-update --property image_meta="$image_meta" $&#123;NEW_IMAGE_ID&#125;$ glance image-update --property hw_qemu_guest_agent=yes $&#123;NEW_IMAGE_ID&#125;$ glance image-update --location rbd://$FSID/$POOL/$NEW_IMAGE_ID/snap $&#123;NEW_IMAGE_ID&#125; #更新镜像的地址 使用 libguestfs 修改虚拟机镜像libguestfs项目介绍libguestfs 其实是一系列工具组成的，目的就是为了连接并修改本地虚拟机镜像。可以实现的功能有很多，包括：修改镜像内文件，脚本，查看镜像文件系统容量使用情况，物理机与虚拟镜像之间的文件传递，备份，克隆，甚至新建虚拟机实例，格式化磁盘,修改磁盘大小等等。参考libguestfs 部分示例安装比较简单，如果是rh/centos系列，直接yum安装： 1$ yum install -y libguestfs-tools 下面主要分三部分guestfish，guestmount 以及virt-* tools 讲解 guestfish 讲解与示例通俗讲，guestfish 的作用就是修改镜像内的文件。它并不会将镜像文件系统直接挂载到本地，而是提供了一个类似shell的交互接口允许你查看，编辑，删除文件。 示例： 1$ guestfish --rw -a centos63.raw # 进入guestfish shell ,Mount the image in read-write mode as root 进入之后 ，先执行run ，会创建一个虚拟机实例。 如果出现错误的话，可以 export LIBGUESTFS_DEBUG=1 打开debug模式查找错误。或者利用 libguestfs-test-tool 命令测试一下。接下来示例编辑网卡配置： 1234567&gt;&lt;fs&gt; list-filesystems #列出可挂载的文件系统&gt;&lt;fs&gt; mount /dev/vg_centosbase/lv_root / #挂载&gt;&lt;fs&gt; edit /etc/sysconfig/network-scripts/ifcfg-eth0 #编辑文件&gt;&lt;fs&gt; exit #退出 guestmount 讲解与示例guestmount 可以实现直接将镜像的文件系统挂载到本地。示例如下： 12345$ guestmount -a centos63_desktop.qcow2 -i --rw /mnt # i 参数表示自动查找root分区并挂载$ rpm -qa --dbpath /mnt/var/lib/rpm #示例查看rpm包$ umount /mnt 若umount失败（通常报错 device is busy），可以使用lazy umount，加 l 参数，即： 1$ umount /mnt -l virt-* tools 讲解与示例大概以下几种工具： virt-edit : 修改镜像内的文件。 virt-df : 查看镜像磁盘占用情况。 virt-resize : resize 镜像。 virt-sysprep : 准备发布镜像前的一系列操作（比如删除SSH HOST，删除mac地址，删除user 信息） virt-sparsify : 镜像稀疏（消除镜像空洞） virt-p2v : 物理机转换为虚拟机（kvm）. virt-v2v : xen或vmware镜像转换为kvm镜像。 几个示例（来自openstack doc）: 第一个示例是修改镜像文件： 12345$ virsh shutdown instance-000000e1$ virt-edit -d instance-000000e1 /etc/shadow # d means domain$ virsh start instance-000000e1 第二个示例是 resize image： 12345$ virt-filesystems --long --parts --blkdevs -h -a /data/images/win2012.qcow2 #查看分区$ qemu-img create -f qcow2 /data/images/win2012-50gb.qcow2 50G #新建一块qcow2 image 空间$ virt-resize --expand /dev/sda2 /data/images/win2012.qcow2 /data/images/win2012-50gb.qcow2 # 扩展空间 更新 2017-03-23 增加 libguestfs 修改虚拟机镜像的方法。 2017-03-34 增加 libguestfs 部分示例 参考文章Modify images 在线升级glance镜像技巧 挂载raw和qcow2格式的KVM硬盘镜像 使用Yum快速更新升级CentOS内核 如何挂载一个镜像文件(HOW TO MOUNT AN IMAGE FILE) 挂载虚拟机镜像文件里的 LVM 逻辑分区 libguestfs kvm虚拟化小结（六）libguestfs-tools libguestfs详解 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-- 关于ceph rbd 的一些记录]]></title>
    <url>%2F2017%2F03%2F03%2Fsomething-about-ceph-rbd%2F</url>
    <content type="text"><![CDATA[引出ceph 的中文文档做的真是非常棒，比起openstack的中文文档来说，不仅全，而且更新也比较及时，openstack的文档还是直接查阅英文版的比较好。 本文是在查阅ceph官网文档过程中，联系实际操作做出的一些思考与记录。 ceph rbd 的简单说明官网对rbd的定义是这样的：Ceph 块设备是精简配置的、大小可调且将数据条带化存储到集群内的多个 OSD。 ceph rbd是基于 rados来做的，也就是说，一个 rbd image 首先会被分成多个等大的 object，这些object再 根据 crush map 存储到对应的3个（或n个，n为奇数）OSD中，理解这一点对于理解 rbd 中的 stripe-unit和 stripe-count 有帮助。所以如果考虑深层的话需要结合RADOS 的多种能力，如快照、复制和一致性。 对于openstack来说，通常是通过libvirt 来控制磁盘的虚拟化(比如qemu),再通过qemu来与rbd打交道，如下图： qemu会调用librbd 实现rbd的增删改查等一系列操作，当然也有配套的 qemu命令，详见QEMU 与块设备 libvirt 是一套虚拟机抽象层，它隐藏了底层多种虚拟化的具体实现，提供给client一套统一的通用API和shell接口。关于如何用 virsh 新建一个挂载rbd 的vm，可以参考通过 LIBVIRT 使用 CEPH RBD 关于openstack 与ceph 的整合与配置，参考块设备与 OPENSTACK rbd 常用操作以及命令rbd 的常用操作大致包括三个部分，一个是 常见的增删改查，一个是 针对 snapshot的操作，再加一个map/unmap操作。 rbd的增删改查12345678910$ rbd create --size &#123;megabytes&#125; &#123;pool-name&#125;/&#123;image-name&#125; # 创建 rbd$ rbd ls &#123;poolname&#125; # 罗列 rbd$ rbd info &#123;pool-name&#125;/&#123;image-name&#125; # 查看某一rbd具体信息$ rbd resize --size 2048 foo (to increase) # 增加尺寸$ rbd resize --size 2048 foo --allow-shrink (to decrease) # 减小尺寸$ rbd rm &#123;pool-name&#125;/&#123;image-name&#125; # 删除rbd镜像 rbd snapshot 相关操作快照是映像在某个特定时间点的一份只读副本,ceph 的快照有一个比较高级的特性就是分层clone，在openstack 中会实现非常快的创建VM，下文会详细讲解。 rbd snapshot 的基本操作123456789$ rbd snap create &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; # 创建快照$ rbd snap ls &#123;pool-name&#125;/&#123;image-name&#125; #罗列 某个镜像的快照$ rbd snap rollback &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; # 快照回滚$ rbd snap rm &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; # 删除某一快照$ rbd snap purge &#123;pool-name&#125;/&#123;image-name&#125; # 清除某一镜像的所有快照 ceph 快照的分层 cloneCeph 支持为某一rbd snapshot创建很多个写时复制（ COW ）克隆。所谓写时复制，就是执行 clone命令后，并没有立即执行clone操作，而是引用原文件，当发生写入操作时才真正执行clone操作。 注意： clone只能用于 protected 的snapshot,且仅支持克隆 format 2 的映像（即用 rbd create –image-format 2 创建的）。 分层快照使得 Ceph 块设备客户端可以很快地创建映像，这样便能实现openstack 中非常快的创建VM,而且，随着 openstack 的Nova image-create与ceph的snapshot的无缝连接，镜像的制作更快捷方便了，关于这部分，可以参考基于Ceph RBD的OpenStack Nova快照 关于 rbd snapshot 的内在原理 ，参考ceph rbd快照原理解析 clone 的 步骤如下： 相关命令如下： 123456789$ rbd snap protect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; # 保护快照$ rbd clone &#123;pool-name&#125;/&#123;parent-image&#125;@&#123;snap-name&#125; &#123;pool-name&#125;/&#123;child-image-name&#125; #克隆快照$ rbd snap unprotect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; # 删除快照前，必须先取消保护。此外，不可以删除被克隆映像引用的快照，所以在删除快照前，必须先拍平（ flatten ）此快照的各个克隆。$ rbd children &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; # 罗列快照的子孙$ rbd flatten &#123;pool-name&#125;/&#123;image-name&#125; # 拍平克隆映像 rbd 与内核相关的map/unmap注意：由于Linux kernel支持rbd的版本较高，一般需要升级到较高版本（建议升级到3.11+） 升级内核参考使用Yum快速更新升级CentOS内核 12345$ rbd map &#123;pool-name&#125;/&#123;image-name&#125; --id &#123;user-name&#125; # 映射块设备$ rbd showmapped # 查看已映射块设备$ rbd unmap /dev/rbd/&#123;poolname&#125;/&#123;imagename&#125; # 取消块设备映射 参考文章CEPH 块设备 基于Ceph RBD的OpenStack Nova快照 ceph rbd快照原理解析 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-- 使用国内yum源搭建ceph集群]]></title>
    <url>%2F2017%2F02%2F28%2Finstall-ceph-with-domestic-yum-source%2F</url>
    <content type="text"><![CDATA[引出初衷是自己搭建一套ceph的实验集群，按照官方的quick start,按理说应该是分分钟的事，因为GFW的原因，搞得忙活了一上午，再加上心情烦躁,中间出现了各种差错，这里记录一下。 系统预检并安装ceph-deploy准备集群系统，网络设置，SSH无密码登录等设置略过，详细参考PREFLIGHT CHECKLIST 这里有一个地方需要注意的是：centos7 在修改HOSTNAME 的时候命令如下，无需重启： 1$ sudo hostnamectl --static set-hostname &lt;host-name&gt; 预检完成后，ceph-deploy 的安装以及 ceph在各节点的安装如果出现错误的话很大可能上是yum 源的问题，所以重点说下如何配置yum国内源。 更换国内 YUM 源 更换base源为国内163源: 1$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo 更换epel源为中科大的源(注：随着版本更迭，以下链接可能会失效，如果出现404错误，那就需要更改链接)： 123$ rpm -Uvh http://mirrors.ustc.edu.cn/centos/7/extras/x86_64/Packages/epel-release-7-9.noarch.rpm$ rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 更换ceph源为国内163源 建议不用改变ceph.repo,直接设置环境变量即可，如果更改ceph.repo，那么执行ceph-deploy的时候还是需要在命令行中指定国内源url，否则又会使用官方源。 123456789$ export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/rpm-jewel/el7$ export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc$ yum clean all$ yum makecache$ yum update -y 安装并配置ceph cluster接下来的事情就跟官网的步骤差不多了，大致如下： 123456789101112131415161718# Create monitor nodeceph-deploy new node1 node2 node3# Software Installationceph-deploy install deploy node1 node2 node3# Gather keysceph-deploy mon create-initial# Ceph deploy parepare and activateceph-deploy osd prepare node1:/dev/sdb node2:/dev/sdb node3:/dev/sdbceph-deploy osd activate node1:/var/lib/ceph/osd/ceph-0 node2:/var/lib/ceph/osd/ceph-1 node3:/var/lib/ceph/osd/ceph-2# Make 3 copies by defaultecho "osd pool default size = 3" | tee -a $HOME/ceph.conf# Copy admin keys and configuration filesceph-deploy --overwrite-conf admin deploy node1 node2 node3 一个尚未解决的顽疾在公司使用国内源安装ceph的时候，出现了一个这样的错误： 1http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/ceph-mds-10.2.5-0.el7.x86_64.rpm: [Errno -1] Package does not match intended download. Suggestion: run yum --enablerepo=ceph clean metadata Package does not match intended 这个错误在我安装其他软件的时候也出现过，用尽了各种办法也没有解决该问题，但我在家里的时候却没有出现过该问题。所以推测是公司网络用了类似缓存的机制。 更新-2017-3-18找到了解决上述问题的方法，就是先直接wget下来rpm包，然后用yum localinstall 或者rpm -ivh 安装,如果出现两个包互相依赖的情况，就两个包同时安装。 123wget http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/ceph-mds-10.2.5-0.el7.x86_64.rpmyum localinstall ceph-mds-10.2.5-0.el7.x86_64.rpm 参考文章PREFLIGHT CHECKLIST STORAGE CLUSTER QUICK START 如何使用国内源部署Ceph？ CentOS7修改主机名 CentOS 7 x86_64适用的EPEL安装源 国内镜像列表 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- 如何调试openstack rest API]]></title>
    <url>%2F2017%2F02%2F22%2Fhow-to-debug-openstack-rest-api%2F</url>
    <content type="text"><![CDATA[引出在学习openstack 的过程中，基本都是这样一个过程，先是通过 dashboard 对openstack 有一个感性的认识，之后通过命令行了解openstack 更多的功能，但openstack 的内部实现还是一个黑盒。这时就需要了解 openstack 提供的最原始的 rest API, rest API 不仅是提供给终端 user使用，它内部组件的通信机制也用的rest API ，当然还有AMQP,关于 rest与AMQP 的比较见这篇文章 。本篇文章主要讲如何使用 postman 调试openstack 的rest api，之前会花一些篇幅讲一点openstack api version 的知识。 openstack api 版本演变openstack 自开源以来发展还是挺迅猛的，对应各组件的api version演变也很快，特别是 nova, cinder, keystone等组件的api。在最新的官方文档中，也就是 N 版本的openstack,目前使用的API(只列出一些常用的) 为 ： Bare Metal API v1 (microversions) Block Storage API v3 (microversions) Clustering API v1 Compute API (microversions) Container Infrastructure Management API (microversions) Identity API v3 Identity API v3 extensions Image service API v2 Messaging API v2 Networking API v2.0 Object Storage API v1 还支持或者兼容的older api为： Block Storage API v2 废弃的api 为： Block Storage API v1 Identity API v2.0 Identity admin API v2.0 Identity API v2.0 extensions Image service API v1 microversion 解释这里简单解释下 microversion,microversion的使用是为了向后兼容性，因为openstack的版本迭代比较快，当我们对openstack进行版本升级后，对应的client 或其他调用openstack api的系统因为某些原因没法升级，仍然依赖之前的API，这个时候就可以使用microversion来解决这个问题。举个栗子，比如Compute API，我们先通过一个 get请求( youropenstackIP:8774/ )获取支持的version 版本，返回如下： 123456789101112131415161718192021222324252627282930&#123; "versions": [ &#123; "id": "v2.0", "links": [ &#123; "href": "http://openstack.example.com/v2/", "rel": "self" &#125; ], "status": "SUPPORTED", "version": "", "min_version": "", "updated": "2011-01-21T11:33:21Z" &#125;, &#123; "id": "v2.1", "links": [ &#123; "href": "http://openstack.example.com/v2.1/", "rel": "self" &#125; ], "status": "CURRENT", "version": "2.42", "min_version": "2.1", "updated": "2013-07-23T11:33:21Z" &#125; ]&#125; 可以看到支持 V2 版本以及V2.1 的microversion,该 microversion，支持从 V2.1 到 V2.42的所有版本的API，但如果我们使用的话还是需要指定一个版本，在 header中加一个参数：1X-OpenStack-Nova-API-Version: 2.4 keystone v3 简介这里简单说下keystone v3，因为 keystone v3相对于 v2版本来说增加了很多新的特性，这里简单记录一下。 v2版本对用户的权限管理以每一个用户为单位，需要对每一个用户进行角色分配，并不存在一种对一组用户进行统一管理的方案，v3版本中引入了 Domain 和 Group的概念，将 tenant改为project,从而实现对一组用户进行统一管理。 直接引用OpenStack Keystone V3 简介 V3 利用 Domain 实现真正的多租户（multi-tenancy）架构，Domain 担任 Project 的高层容器。云服务的客户是 Domain 的所有者，他们可以在自己的 Domain 中创建多个 Projects、Users、Groups 和 Roles。通过引入 Domain，云服务客户可以对其拥有的多个 Project 进行统一管理，而不必再向过去那样对每一个 Project 进行单独管理。 Group 是一组 Users 的容器，可以向 Group 中添加用户，并直接给 Group 分配角色，那么在这个 Group 中的所有用户就都拥有了 Group 所拥有的角色权限。通过引入 Group 的概念，Keystone V3 实现了对用户组的管理，达到了同时管理一组用户权限的目的。这与 V2 中直接向 User/Project 指定 Role 不同，使得对云服务进行管理更加便捷。 使用 postman 调试openstack 的rest apiopenstack 常用组件默认监听端口在使用postman调试之前，先列出openstack 中常用组件默认监听的端口： 其他服务组件的默认监听端口： 使用 postman 调试 openstack rest api调试 rest api 的客户端工具有很多，官网用的是curl，但不是很直观，postman 算是颜值比较高的一款 http api调试工具。我比较习惯于使用 chrome插件版本的postman，安装完毕后，开始进行调试： 向 keystone 发送post请求获取 token 以及 service endpoint: request headers 内容如下： request body 内容如下： 返回结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; "access": &#123; "token": &#123; "issued_at": "2017-02-22T08:17:13.025922", "expires": "2017-02-23T08:17:12Z", "id": "1213f82cd41f472ba4291e75a2de8288", "tenant": &#123; "description": "Admin Tenant", "enabled": true, "id": "585324ae7e934e2eb50d84ad7aee3179", "name": "admin" &#125;, "audit_ids": [ "T7g-8ZGYTZOUN5oEI-2q5w" ] &#125;, "serviceCatalog": [ &#123; "endpoints": [ &#123; "adminURL": "http://192.168.24.161:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "region": "RegionOne", "internalURL": "http://192.168.24.161:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "id": "899cc922536f4038922bac198616d086", "publicURL": "http://192.168.24.161:8774/v2/585324ae7e934e2eb50d84ad7aee3179" &#125;, &#123; "adminURL": "http://192.168.24.100:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "region": "testVlan", "internalURL": "http://192.168.24.100:8774/v2/585324ae7e934e2eb50d84ad7aee3179", "id": "2e4cf2fac2be47a4936ba19e15894030", "publicURL": "http://192.168.24.100:8774/v2/585324ae7e934e2eb50d84ad7aee3179" &#125; ], "endpoints_links": [], "type": "compute", "name": "nova" &#125;, ........................................ ], ] &#125; &#125;&#125; 可以看到 token id 即我们要的token标示，而且下一步需要的url即endpoint也已经出现在该json串中，以 compute service 的endpoint 为例，获取 虚拟机列表。 向 nova 发送 获取虚拟机列表的rest 请求，将上一步获取到的 token id填入 X-Auth-Token中。 发送该请求即可得到我们的结果： 1234567891011121314151617181920212223242526272829303132&#123; "servers": [ &#123; "id": "712ea870-9ac7-467d-a2d7-5e3caa8499a6", "links": [ &#123; "href": "http://192.168.24.161:8774/v2/ca36ea9826b04643897f19b7c03be011/servers/712ea870-9ac7-467d-a2d7-5e3caa8499a6", "rel": "self" &#125;, &#123; "href": "http://192.168.24.161:8774/ca36ea9826b04643897f19b7c03be011/servers/712ea870-9ac7-467d-a2d7-5e3caa8499a6", "rel": "bookmark" &#125; ], "name": "test" &#125;, &#123; "id": "257a2852-940e-4271-b188-fcd4d0a8c10c", "links": [ &#123; "href": "http://192.168.24.161:8774/v2/ca36ea9826b04643897f19b7c03be011/servers/257a2852-940e-4271-b188-fcd4d0a8c10c", "rel": "self" &#125;, &#123; "href": "http://192.168.24.161:8774/ca36ea9826b04643897f19b7c03be011/servers/257a2852-940e-4271-b188-fcd4d0a8c10c", "rel": "bookmark" &#125; ], "name": "testmime9" &#125; ]&#125; 其他请求类似。 参考文章OpenStack API versions penStack API Documentation OpenStack DocumentationAppendix Firewalls and default ports 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- openstack instance metadata 服务机制探索]]></title>
    <url>%2F2017%2F02%2F17%2Fopenstack-instance-metadata-discovery%2F</url>
    <content type="text"><![CDATA[引出在调试 cloud-init的时候发现在虚拟机内执行“ curl 169.254.169.254” 的时候出现“host not reachable”错误。在对 openstack instance（确切的说应该是server,因为还没有正式生成云主机） metadata 服务机制探索了一番之后，发现了错误所在，在分析该问题之前先记录下metadata 服务的原理，最后再记录下问题的解决过程。 openstack instance metadata 服务机制广义上的metadata包含很多，不光 instance 有metadata属性，image,host,aggregate等都有。在openstack中，因为instance利用metadata 属性可以做很多事情，研究的价值更高一些，所以这里只讨论 instance 的metadata。(注：这里的metadata 服务 泛指 提供userdata &amp;&amp; metadata 的服务，因为userdata与metadata只是 内容以及 虚拟机获取之后处理方式不同，虚拟机获取方式是一样的)。关于 userdata 与 metadata，这里直接引用OpenStack 的 metadata 服务机制中的一段： 在创建虚拟机的时候，用户往往需要对虚拟机进行一些配置，比如：开启一些服务、安装某些包、添加 SSH 秘钥、配置 hostname 等等。在 OpenStack 中，这些配置信息被分成两类：metadata 和 user data。Metadata 主要包括虚拟机自身的一些常用属性，如 hostname、网络配置信息、SSH 登陆秘钥等，主要的形式为键值对。而 user data 主要包括一些命令、脚本等。User data 通过文件传递，并支持多种文件格式，包括 gzip 压缩文件、shell 脚本、cloud-init 配置文件等。这些信息被获取到后被cloudinit 利用并实现各自目的。虽然 metadata 和 user data 并不相同，但是 OpenStack 向虚拟机提供这两种信息的机制是一致的，只是虚拟机在获取到信息后，对两者的处理方式不同罢了 openstack 中，虚拟机获取metadata 的信息有两种，即config drive 和 metadata restful 服务（即EC2方式）。下面分别记录下两种方式的原理： config drive 的方式所谓 config drive的方式就是将metadata信息写入虚拟机的一个特殊配置设备中，虚拟机启动的时候会挂载并读取metadata信息。要想实现这个功能，还需要宿主机和镜像满足一些条件： 宿主机满足条件： 虚拟化方式为以下几种：libvirt,xen,hyper-v,vmware. 对于 libvirt, xen, vmware的虚拟化方式，需要安装genisoimage.并且需要设置mkisofs为 程序的安装目录，如果是跟nova-compute安装在一个目录内就不用了。 如果是 hyper-v的虚拟化方式，需要用mkisofs的命令行指定mkisofs.exe 程序的绝对安装路径，且在hyperv的配置文件中设置qemu_img_cmd 为qemu-img 的命令行安装路径。 镜像满足条件： 安装了cloud-init，且最好是0.7.1+ 的版本。如果是没有安装的话，就需要自己去定制脚本了。 如果使用xen 虚拟化，需要配置 xenapi_disable_agent 为true。 在openstack中，在使用命令行创建虚拟机的时候指定 –config-drive true 即可使用config drive。如下实例（也可以用 nova command）： 12345openstack server create --config-drive true --image my-image-name \ --flavor 1 --key-name mykey --user-data ./my-user-data.txt \ --file /etc/network/interfaces=/home/myuser/instance-interfaces \ --file known_hosts=/home/myuser/.ssh/known_hosts \ --property role=webservers --property essential=false MYINSTANCE 也可以在/etc/nova/nova.conf配置文件中指定 force_config_drive = true ，那么默认就使用config drive。 如果镜像操作系统支持通过标签访问磁盘的话，可以使用如下命令查看config drive中的内容：123mkdir -p /mnt/configmount /dev/disk/by-label/config-2 /mnt/configcd /mnt/config &amp; ls -l 进入该目录后，看到的内容大致如下：1234567891011ec2/2009-04-04/meta-data.jsonec2/2009-04-04/user-dataec2/latest/meta-data.jsonec2/latest/user-dataopenstack/2012-08-10/meta_data.jsonopenstack/2012-08-10/user_dataopenstack/contentopenstack/content/0000openstack/content/0001openstack/latest/meta_data.jsonopenstack/latest/user_data 可以看出，config drive支持 openstack以及EC2的方式获取数据，官方手册建议使用opensatck的方式（即读取opensatck目录里的内容），因为EC2的目录以后可能会弃用。而且最好使用最近的版本目录读取。 metadata restful 服务的方式就是我们在虚拟机通过169.254.169.254的方式来获取metadata的方式。由以下三个组件来完成这项工作： Nova-api-metadata：运行在计算节点，启动restful服务，真正负责处理虚拟机发送来的rest 请求。从请求头中获取租户，虚拟机ID，再去数据库中查询相应的metadata信息并返回结果。 Neutron-metadata-agent：运行在网络节点，负责将接收到的获取 metadata 的请求转发给 nova-api-metadata。 Neutron-ns-metadata-proxy：由于虚拟机获取 metadata 的请求都是以路由和 DHCP 服务器作为网络出口，所以需要通过 neutron-ns-metadata-proxy 联通不同的网络命名空间，将请求在网络命名空间之间转发。 流程大概如下： 具体看下路由的过程，如果虚拟机所在的subnet连接在router上，该请求会先通过router来发送请求，如果没有router相连的话会通过对应dns 的网络空间进行传递，示例可以查看OpenStack 的 metadata 服务机制 文章末。 问题解决了解了metadata rest的原理后，解决上述问题的思路就比较明确了，首先看下虚拟机确是使用restful 服务的方式而不是config drive的方式，从网络拓扑看，虚拟机所在的子网是连接着router的，但是查看虚拟机的路由表，却发现169.152.169.254发送到了dns的网络空间，而不是router的网络空间，然后再去查看对应dns的网络空间，发现并没有监听 80 端口的 neutron-ns-metadata-proxy 服务，而在router的网络空间却有对应的路由规则。大致猜测虚拟机boot的时候没有发现对应子网有router相连，所以该虚拟机boot后会将169.152.169.254路由到对应dns，但其实该子网是有router相连的，所以neutron还是将监听的路由规则放到了router上。与router相关的服务就是neytron-l3-agent,重启该服务后，一切正常。 参考文章OpenStack 的 metadata 服务机制 Store metadata on a configuration drive metadata-service 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack-- neutron 二/三层网络实现探究]]></title>
    <url>%2F2017%2F02%2F12%2Fneutron-layer2-3-realization-discovry%2F</url>
    <content type="text"><![CDATA[引出 Neutron 是openstack 中提供网络虚拟化的组件，根据二层网络的实现方式不同(即agent的不同)，可以分为Linux bridge的方式，Openvswitch的方式。而且，lay2 network分为local,flat,vlan,vxlan 等类型(gre与vxlan类似，不再考虑)，本文就分析两种实现方式在这四种网络中的具体实现异同。因为vxlan会依赖lay3层网络，所以还会分析下lay3网络的实现。 本文内容主要来自每天5分钟玩转 OpenStack,图片也是来自对应博客，本文仅是对其总结对比并作记录。 lay2 local 型网络生产环境中，local与flat网络是不会被使用的,vlan与vxlan是使用比较多的layer2网络。因为local网络只支持在同一宿主机的虚拟机互联，而flat网络会每个网络独占宿主机的一个物理接口，这在现实世界中是不允许的。但是vlan与vxlan的实现都是在local,flat网络的基础上实现的，所以还是有必要看一下这两种类型的网络的实现的。 linux bridge 实现local 型网络对于每个local network，ML2 linux-bridge agent 会创建一个bridge,instance的tap设备连接到该bridge。位于同一local network的instance会连接同一个bridge，这样instance之间就可以通信了。但因为bridge没有与宿主机物理网卡相连，所以跟宿主机无法通信，也没法与宿主机之外的其他机器通信。下图为示例： 图中创建了两个local network,对应两个网桥。 VM0 与 VM1 在同一个 local network中，它们之间可以通信. VM2 位于另一个 local network，无法与 VM0 和 VM1 通信。 两个local network都有自己的DHCP server,由dnsmasq在各自独立的net namespace实现，通过tap设备挂接在各自network的bridge上。 openvswitch 实现local 型网络ovs的实现要相对复杂些，neutron完成相应配置并启动后，ovs-agent会调用ovs自动创建三个ovs bridge，br-ex,br-int和br-tun，看名字大概也可以猜出，br-ex是用于外部网络，br-int是内部虚拟机的网络，br-tun是用于overlay network,也就是vxlan类型的网络会用到该bridge。local network 只需考虑br-int bridge。示例图如下： 图中每个instance并不是跟之前一样通过tap设备直接挂载在ovs的 br-int上，而是通过一个新建的linux bridge以及一对veth pair跟br-int相连，为什么这样做呢，因为Open vSwitch 目前还不支持将 iptables 规则放在与它直接相连的 tap 设备上，这样就实现不了Security group，所以就加了一个linux bridge以支持iptables。 两个local network都有自己的DHCP server,由dnsmasq在各自独立的net namespace实现，通过tap设备挂接在各自network的bridge上。 都挂载在同一个br-int上，如何区分不同的local network呢，其实，br-int上挂载的虚拟网卡或DHCP对应的port都有一个特殊的tag属性。同一网络的tag相同，不同网络的tag不同，这里跟vlan的vlan ID类似，同一tag属性的port是二层连通的。 lay2 flat 型网络flat型网络是在local 型网络的基础上实现不同宿主机的instance之间二层互联。但是每个flat network都会占用一个宿主机的物理接口，所以生产环境中也不会使用。 linux bridge 实现flat 型网络每一个flat network对应一个物理网卡，该对应关系需要在ml2_conf.ini配置文件中指明。下图为flat network示例： 该 flat network 在计算和控制节点都使用eth1作为接口。如果是新建另一个flat network,只能使用除eth1之外的其他物理接口，且在配置文件中添加相应的mapping配置。 同一网络，不同宿主机上的Linux bridge 名称是一样的。 只在控制节点有DHCP的设备，因为同一网络只使用一个DHCP server。 其他跟local network类似。 openvswitch 实现flat 型网络同linux bridge实现的flat network一样，每一个flat network会占用一个物理接口，需要在配置文件ml2_conf.ini中指定对应关系，这里，需要利用ovs新建一个ovs bridge，将该bridge挂载对应物理接口。下图为flat network 示例： br-eth1就是我们新建的ovs bridge,为什么要新建一个ovs bridge与物理网卡连接，而不是直接用br-int与物理网卡连接呢，因为我们在配置文件中需要配置的是某一flat network 的label(flat network type,就是一个标识)与ovs-bridge的mapping,如果是直接用br-int与物理网卡连接的话，那么当建立多个flat-network的时候，br-int与多个物理网卡相连，br-int是无法辨识的。而linux bridge实现flat network的时候是不同的linux bridge与不同的物理网卡连接。 当再新建一个flat network 的时候需要再新建一个ovs bridge，连接另一网卡， 并于配置文件指定mapping。 这里出现了一种新型的网络设备叫做patch port,就是br-eth1与br-int相连的设备，与veth pair的功能类似，只不过这种设备只适用于两个ovs bridge之间互联。 只在控制节点有DHCP的设备，因为同一网络只使用一个DHCP server。 br-int内不同网络的区分跟local network一样，也是通过tag实现。 lay2 vlan 型网络linux bridge 实现vlan 型网络vlan network是在flat network的基础上实现多个不同的vlan network 共用同一个物理接口。需要在配置文件中指定vlan的范围（租户的网络id 范围），以及 vlan network 与物理网卡的对应关系。vlan network如下示例： 可以看出，bridge上除了挂载instance的tap,dhcp的tap设备外，还挂载了一个vlan interface eth1.10x ,该vlan interface 便是区分不同vlan的关键。每一个vlan network有一个对应的vlan id，该vlan id 对应相应数字的vlan interface。 上图所示，有两个vlan,vlan 100 和vlan 101,vm1 与 vm2 在同一网络vlan100，是可以联通的，vm3 与vm1,vm2不连通。 openvswitch 实现vlan 型网络 配置文件指定tenant网络类型,以及vlan id范围，对应的ovs bridge mapping。 与 Linux Bridge driver 不同，Open vSwitch driver 并不通过 eth1.100, eth1.101 等 VLAN interface 来隔离不同的 VLAN。所有的 instance 都连接到同一个网桥 br-int。Open vSwitch 通过 flow rule（流规则）来指定如何对进出 br-int的数据进行转发，进而实现 vlan 之间的隔离。vlan network如下示例： 乍一看，跟ovs 实现的flat network差不多，不过该图示有两个vlan network，都通过br-eth1与外界宿主机的instance联通。 下面看下Open vSwitch 如何通过 flow rule（流规则）进行vlan 的隔离。通过 ovs-ofctl dump-flows $BRIDGE 命令查看指定bridge 的flow rule。简单解释下：每一行代表一条rule,priority代表该rule的优先级，值越大优先级越高，in_port代表传入数据的port编号，每个port在ovs bridge中 都会有一个编号，可以通过 ovs-ofctl show $BRIDGE 命令查看 port 编号。如上图：eth1 编号为 1；phy-br-eth1 编号为 2。dl_vlan是数据包原始的 VLAN ID，actions表示对数据包进行的操作。那么第一条rule表示的意思就是：从 br-eth1 的端口 phy-br-eth1（in_port=2）接收进来的包，如果 VLAN ID 是 1（dl_vlan=1），那么需要将 VLAN ID 改为 100（actions=mod_vlan_vid:100。为什么要转换呢，之前讲local/flat network时讲过， br-int为了区分不同网络会给挂载的设备打上tag,这里的tag就类似vlan id，不过是仅限于br-int可以识别，宿主机之外是无法识别的，所以需要将该tag转换为外界可以识别的vlan id。再看下br-int 的flow rule: 图中圈出来的第一条rule的意思就是：从int-br-eth1接收进来的数据包，如果 VLAN 为 100，则改为内部 VLAN 1。其他rule分析类似。 lay3 网络lay3 网络主要是一个路由的功能，即实现两个不同subnet之间instance的互联。除此之外，与外网的互联，firewall以及instance attach floating-ip 等功能也都是由L3 agent实现的虚拟路由器（net namespace + iptables）完成的。 linux bridge 实现lay3 网络首先需要修改配置文件，启用 l3 agent,配置文件位于控制节点/网络节点的/etc/neutron/l3_agent.ini，mechanism driver 是linux bridge(或openvswitch)。创建一个router后，如下示例： 看一下router是如何连接两个vlan 的subnet: l3 agent 会为每个 router 创建了一个 namespace，通过 veth pair 与 TAP 相连，然后将 Gateway IP 配置在位于 namespace 里面的 veth interface 上，这样就能提供路由了。这样便实现 vlan100 和 vlan 101 的连通了。 再简单说下flaoting-ip attach的实现原理，floating-ip 的出现是为了让外网能够直接访问到被attached的instance。如图，是有两个内部vlan network和一个external network（网络类型可能是flat或vlan）,一个虚拟路由器将这三个网络连在一起，vm1通过虚拟路由器可以访问外网，但是外网没法直接主动连接vm1.我们将一个flaoting-ip 如10.10.10.3，attach到vm1之后，发现外网可以通过该floating-ip 直接连接vm1.进入vm1，看下它的网卡配置，发现并没有floating-ip对应的网卡存在。怎么回事呢，其实真正实现floating-ip attach操作的是发生在虚拟路由器上，查看下对应的虚拟路由器的网卡， 再看下虚拟路由器iptables的nat配置， 这样，便明白了，floating IP 是配置在 router 的外网 interface 上的，而非 instance，floating IP 能够让外网直接访问租户网络中的 instance，这是通过在 router 上应用 iptalbes 的 NAT 规则实现的。 openvswitch 实现lay3 网络与linux bridge 的实现类似，只不过这里不是挂载在不同bridge上，而是直接挂载在br-int上。 两个 Gateway IP 分别配置在 qr-2ffdb861-73 和 qr-d295b258-45 上,从而实现两个subnet的连接。floating-ip attach 与Linux bridge类似实现。 lay2 vxlan 型网络除了local, flat, vlan 这几类网络，OpenStack 还支持 vxlan 和 gre 这两种 overlay network。所谓overlay，是指指建立在其他网络上的网络，比如vxlan就是在udp的基础上实现。 vxlan 和 gre 都是基于隧道技术实现的。目前 linux bridge 只支持 vxlan，不支持 gre；open vswitch 两者都支持。因为vxlan与gre类似，这里只讨论vxlan。VXLAN 提供与 VLAN 相同的以太网二层服务，但是拥有更强的扩展性和灵活性。与 VLAN 相比，VXLAN 有下面几个优势： 支持更多的二层网段。 VLAN 使用 12-bit 标记 VLAN ID，最多支持 4094 个 VLAN，这对于大型云部署会成为瓶颈。VXLAN 的 ID （VNI 或者 VNID）则用 24-bit 标记，支持 16777216 个二层网段。 能更好地利用已有的网络路径。 VLAN 使用 Spanning Tree Protocol 避免环路，这会导致有一半的网络路径被 block 掉。VXLAN 的数据包是封装到 UDP 通过三层传输和转发的，可以使用所有的路径。 避免物理交换机 MAC 表耗尽。 由于采用隧道机制，TOR (Top on Rack) 交换机无需在 MAC 表中记录虚拟机的信息。 VXLAN 是将二层建立在三层上的网络。 通过将二层数据封装到 UDP 的方式来扩展数据中心的二层网段数量。 VXLAN 是一种在现有物理网络设施中支持大规模多租户网络环境的解决方案。 VXLAN 的传输协议是 IP + UDP。 VXLAN 定义了一个 MAC-in-UDP 的封装格式。 在原始的 Layer 2 网络包前加上 VXLAN header，然后放到 UDP 和 IP 包中。 通过 MAC-in-UDP 封装，VXLAN 能够在 Layer 3 网络上建立起了一条 Layer 2 的隧道。 关于 vxlan 的包格式以及相关的设备的，参考VXLAN 概念（Part I） - 每天5分钟玩转 OpenStack（108） linux bridge 实现vxlan 网络VXLAN 概念（Part II）- 每天5分钟玩转 OpenStack（109） ovs 实现vxlan 网络OVS vxlan 底层结构分析 - 每天5分钟玩转 OpenStack（148） 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack--openstack 常用网络调试命令]]></title>
    <url>%2F2017%2F02%2F10%2Fopenstack-common-net-command%2F</url>
    <content type="text"><![CDATA[引出openstack网络部分应该算是最为复杂的，这里简单罗列下常用的命令作为备注，主要包括以下三个部分： neutron 命令 ip netns 命令（即网络命名空间） ovs/brctl 命令（即ovs和网桥命令） iptables 命令 neutron 命令 对net, subnet, port 这些core service的操作，略 对ext service 的操作(略)，常见ext service 包括：1. router 2. firewall 3. loadbalance 4. security-group 5. vpn 6. floatingip 查看agent列表：neutron agent-list 查看service provider: neutron service-provider-list ip netns 命令 列出宿主机所有网络命名空间：ip netns 在某一网络命名空间执行命令：ip netns exec $NAMESPACE COMMAND ,示例：1. ip netns exec qrouter-a123f550-747b-4b67-8006-2c29a5dd4c6d ip a 2. ip netns exec qrouter-a123f550-747b-4b67-8006-2c29a5dd4c6d ping www.baidu.com ovs/brctl 命令主要是针对 openvswitch 创造的网桥信息 和 linux bridge 创造的网桥信息 显示ovs创建的网桥信息：ovs-vsctl show 查看网桥br-tun上的流表信息：ovs-ofctl dump-flows br-tun ovs 创建网桥br-eth0：ovs-vsctl add-br br-eth0 将网卡eth0 桥接在br-eth0上：ovs-vsctl add-port br-eth0 eth0 查看datapath统计信息：ovs-dpctl show -s 显示linux bridge 创建的网桥信息：brctl show linux bridge 创建网桥br-eth1: brctl addbr br-eth1 将网卡eth1 桥接在br-eth1上: brctl addif br-eth1 eth1 iptables 命令neutron 中的L3-agent 默认是用iptables来实现3层网络的实现，而且，security-group, Firewall的实现也都与iptables息息相关。 新增规则到某个规则链的最后一个：iptables -A INPUT … 删除某个规则：iptables -D INPUT –dport 80 -j DROP 取代现行规则，顺序不变：iptables -R INPUT 1 -s 192.168.0.1 -j DROP 插入一条规则：iptables -I INPUT 1 –dport 80 -j ACCEPT 列出某规则链中的所有规则：iptables -L INPUT 列出nat表所有链中的所有规则：iptables -t nat -L 参考文章Linux 防火墙和 iptables Linux iptables 命令行操作常用指令 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[other--博客从jekyll搬到hexo记录]]></title>
    <url>%2F2017%2F02%2F03%2Fform-jekyll-to-hexo-blog%2F</url>
    <content type="text"><![CDATA[目录结构：引出 blog 搬家的步骤 引出 之前写blog的初衷是作为自己以后翻阅的记录，所以就用了一个非常简单的jekyll theme，随着写的越来越多，因为这个theme没有归档功能，发现越来越不好打理，所以决定换个主题。 网上搜了一下后，发现hexo可以实现本地预览的功能，就决定用hexo来搭，next这个主题是个比较经典的主题，作者的维护也一直很棒，甚至出了一个用户手册,基本能实现我的所有需求。 因为原blog就是搭在github上，所以现在所要做的工作就是利用hexo做的blog替换原有的在github page上的blog。 blog 搬家的步骤 删除原github page 项目。注意保留本地之前写的blog。 安装hexo，并按照用户手册做一些初始化的工作。 做blog的迁移工作，将blog的md文件复制到 source/_post文件夹，并在 _config.yml 中修改 new_post_name 参数。 1new_post_name: :year-:month-:day-:title.md 修改blog中的部分md格式，比如代码高亮格式等。 下载 next theme 到themes文件夹，并配置该theme，参考next theme。注意如果是使用git clone下来的，那么会存在两个远程分支（github page 一个，next theme一个），容易出错，所以最好还是直接下载下来放到themes文件夹（当然，也可以使用git submodules来管理）。 git bash 中运行依次 hexo g （生成静态文件），hexo s（启动本地服务器） ，然后本地查看。 接下来需要将blog部署到github pages。因为我需要在家里和公司的电脑进行博客的更新，所以涉及到一个协作的问题。github page 上面挂的都是一些静态的文件，而如果修改主题或者配置的话，就没法做到同步了。搜了一下，知乎上找到一个比较完美的解决方案，通过创建两个分支来解决，参考使用hexo，如果换了电脑怎么更新博客？。 参考文章Next theme hexo doc 使用hexo，如果换了电脑怎么更新博客？ 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--openstack 中的快照与备份]]></title>
    <url>%2F2017%2F01%2F23%2Fopenstack-incremental-backup%2F</url>
    <content type="text"><![CDATA[目录结构：引出 nova 相关的快照/备份机制 cinder相关的快照/备份机制 引出 备份作为一种常见的容灾机制，是系统设计中不可或缺的一环，openstack自然也不例外，本文列举了一些opensatck中常见的snapshot,backup等操作。 重点讨论nova,cinder。其他备份比如trove不再考虑 简单谈一下快照与备份的区别，快照保存的是瞬时数据，依赖于源数据，主要用于做重要升级之前先做个snapshot，以备出现差错回滚。备份主要是为了容灾，不依赖于源数据，且一般备份数据在不同数据中心。 nova 相关的快照/备份机制Nova 快照 命令为nova image-create 严格来说，该快照并不是真正意义的快照，更像是创建一个镜像，所以命令为image-create.它并没有使用virt提供的virsh snapshot-create来创建snapshot，仅仅将虚拟机的系统磁盘文件完整的复制一份，然后把复制的文件上传至 glance 中作为镜像，之后二者毫无关联，类似于全量快照(没有考虑ceph作为统一存储)check this。 nova 中的快照支持冷快照和live snapshot,两者的命令都是 nova image-create。当满足QEMU 1.3+ and libvirt 1.0+版本要求时会使用live snapshot,否则使用冷快照。关于两者区别以及流程，check this 如果是使用ceph作为统一存储(即nova,glance,cinder都将ceph rbd作为默认存储)，那么snapshot可以利用ceph的cow等特性实现更快速的快照，L版本已实现，流程大概如下： 这样做的坏处是会产生新建镜像对原虚拟机的依赖性，需要定期flatten。还不知道L版是怎么处理的，挖个坑，回头看下具体实现。详细参考基于rbd提升虚机快照创建速度 Nova 备份 命令为 nova backup 其实该命令跟image-create的作用差不多，也是创建一个image,最大的区别就是它有一个rotation，会保证备份的个数为rotation个。详细的用法参考这个实例Openstack - Nova Backup/Restore nova backup 不会自动每天/月周期执行，只能是我们自己命令，所以可以利用nova backup 命令写cron脚本或其他脚本实现自动化备份。这里有一个实例OpenStack: Quick and automatic instance snapshot backup and restore (and before an apt upgrade) with nova backup cinder相关的快照/备份机制cinder 快照 命令为 cinder snapshot-create 同nova volume-snapshot-create一样效果。除了create,还有其他list,delete,metadata等命令。 可以用snapshot 对volume进行恢复，见示例Openstack - Cinder Snapshot/Restore 如果cinder使用rbd作为后端存储，那么这时候的snapshot 便是利用rbd snapshot来实现的，关于ceph rbd snapshot的实现原理，参考ceph– ceph rbd快照的实现原理 cinder 备份 命令 cinder backup-create cinder支持多种backend,对全量备份，增量备份的支持也不尽相同，这里只讨论以ceph rbd 作为backend的情况，其他的backend可以参考这里，Openstack 中cinder backup三种backend的对比- 使用 rbd作为后端，支持全量备份和增量备份。backup-create时先尝试创建增量备份，如果不成功，会创建全量备份，不需要指明专门的参数。 参考这篇, cinder-backup 利用ceph实现增量备份。 参考文章Nova 快照分析 OpenStack: Quick and automatic instance snapshot backup and restore (and before an apt upgrade) with nova backup 基于rbd提升虚机快照创建速度 OpenStack Nova snapshots on Ceph RBD Openstack - Nova Backup/Restore openstack liberty版本使用 ceph 作为存储后端 Openstack 中cinder backup三种backend的对比 Inside Cinder’s Incremental Backup 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--重设虚拟机实例密码几种方法]]></title>
    <url>%2F2017%2F01%2F19%2Fopenstack-reset-instance-password%2F</url>
    <content type="text"><![CDATA[目录结构：引出 采用 nova get-password 方式 采用 libvirt-set-admin-password 采用 nova rebuild instance 的方式 采用 cloud-init 的方式 引出 要解决的问题很明确：就是如果虚拟机的连接采用用户名密码登录的方式，而密码忘记的话，需要采取什么手段解决。 其实解决方案是要取决于真实的生产环境，虚拟化方式的不同，初始化虚拟机密码方式的不同，openstack版本的不同，都会造成某个方案的可行不可行。以下几种方案可能或多或少会出现无法实现的情况，楼主尽量把条件讲清楚。 采用 nova get-password 方式 利用nova 提供的这个接口可以获取instance的password,就不用密码reset了。 适用条件：虚拟化方式为XEN，不支持libvirt. 采用 libvirt-set-admin-password Openstack L 版本新加入的功能，直接使用 “nova set-password “(或早期版本client的”nova root-password”) 就可以，之前的版本该命令不支持Libvirt,仅支持XEN。 适用条件：Openstack Libvirt+ 版本，宿主机libvirt版本1.2.16+，虚拟机镜像安装2.3+ 版本的qemu-guest-agent，详见虚拟机系统密码的修改方案¶ 博主试验了linux 几个主要版本（debian,ubuntu，centos）,只要满足以上的限制条件，都能修改成功，不过官网提供的cloud 版本镜像大都没有安装 qemu-guest-agent，或者版本太低，需要自己安装并制作成镜像。windows 的镜像相对比较麻烦点，宿主机装的virtio-win，通过文件挂载的方式给guest安装对应驱动和QGA,其实不用安装也可以，实现最终都是guest 安装virtio-serial驱动,然后安装quemu-guest-agent就可以了，还有一点就是要设置镜像的property ,例如：hw_qemu_guest_agent=yes，os_type=windows。参考Running the QEMU Guest Agent on a Windows Guest，Can I have virtio-win package on CentOS CentOS 7.1 QEMU guest agent 安装 与 使用 (注：这篇文章有点过时了，因为QGA以及驱动的版本已经升级了好几版，大部分功能都已经实现，比如密码修改等，但是整个流程是一致的) 关于qemu-guest-agent ，可以参考nova 通过 qemu-guest-agent 修改用户密码 采用 nova image-create / nova rebuild的方式 如果虚拟机是根据user-data来设定初始密码的，那么cloud-init只在第一次创建虚拟机执行一次，以后不会执行（reboot也不会执行）。那么我们也只能再次launch一下，方法如下。 首先对当前虚拟机做一次snapshot. 利用该snapshot ，设定好user-data重新boot 一个新的虚拟机 注意：此处只是保证系统盘数据是不变的，如果是数据盘的话还要将对应的数据盘detach再attch到新建的虚拟机中。当然，如果虚拟机是直接用的adminPass的话（即injectPassword的方式）也可以直接利用rebuild命令（rebuild只能用于image启动的instance,而不能用于volume 启动的instance）。 这种方法其实比较笨拙，不到万不得已一般不会这么做。 采用 cloud-init 的方式 这种方法算是所有方法里面最轻便的，但坏处是需要自己定制脚本。对于cloud-init，不熟悉的话可以先翻一下官方手册 原理很简单：借助cloud-init,在虚拟机启动的时候开启一个服务，用来监测metadata中设定的某个值，如果该值发生改变（或者满足其他条件）即做出密码更改的动作并reboot。 可喜的是，我在github找到了类似的代码openstack-password-reset ,不过这个代码只是考虑了RH7系列，而且密码是随机生成的，如果再推给openstack，可能更复杂了。我又更改一下脚本，支持更多Linux版本，且把重设后的密码定死了。年后会把改过的代码挂到github上。 这里面的reset Python程序是通过外链获取的，于是干脆在nova里加了一个API，用来获取该程序。 如果是传递多个文件给cloud-init的话，需要使用MIME的格式，tips:一般是把多个脚本/cloud-config文件 打包成MIME格式文件，然后压缩成gzip格式，传给cloud-init。 参考文章Password Reset 虚拟机系统密码的修改方案 CloudInit &amp; User-Data 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack 系列--利用Vmware workstation手动制作 openstack centos镜像简记]]></title>
    <url>%2F2017%2F01%2F17%2Fmake-openstack-image%2F</url>
    <content type="text"><![CDATA[目录结构：准备工作 创建虚拟机 安装Centos系统 配置系统 上传镜像 自动创建镜像工具 参考文章 tips:在制作镜像的时候最好先知晓镜像所要满足的要求，比如：是否支持磁盘分区，重设根分区大小，密码初始化等等。最常见的需求有如下几种： Disk partitions and resize root partition on boot (cloud-init) No hard-coded MAC address information SSH server running Disable firewall Access instance using ssh public key (cloud-init) Process user data and other metadata (cloud-init) Paravirtualized Xen support in Linux kernel (Xen hypervisor only with Linux kernel version &lt; 3.0) 这部分内容可参考官方文档，Image requirements 。 准备工作 下载Vmware workstation,并安装Centos7,配置好网络。注：刚开始是用的virtualbox,发现不支持nested KVM,Vmware workstation 10+版本以上应该都可以,注意开启workstation的虚拟化设置。 宿主机（刚装好的Centos）安装libvirt系列工具: 1234yum groupinstall Virtualization "Virtualization Client" yum install libvirtyum install libguestfs-toolsservice libvirtd restart 从Centos镜像源下载一个最小的Centos7镜像。本文是以centos为例,其他操作系统类似，详见Create images manually 1wget - O http://mirrors.163.com/centos/7.2.1511/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso 创建虚拟机 创建一个容量为5G的磁盘,并更改所属者： 123cd /imageqemu-img create -f qcow2 centos.qcow2 5Gchown qemu:qemu /image/centos.qcow2 -R 创建并启动虚拟机 123456sudo virt-install --virt-type kvm --name centos7 --ram 1024 \ # name 是自己取得 --disk centos.qcow2,format=qcow2 \ #disk参数为上面创建的磁盘 --network network=default \ # 采用KVM虚拟机网络为nat，其实更建议用bridge模式，删除本行，默认即为bridge模式，不过bridge模式需要自己手动配置宿主机网桥 --graphics vnc,listen=0.0.0.0 --noautoconsole \ --os-type=linux --os-variant=rhel7 \ --cdrom=/image/CentOS-7-x86_64-Minimal-1511.iso #下载的镜像 用“virsh list” 命令查看虚拟机是否启动，如果没有启动的话用“virsh start $NAME” 启动。 执行 “virsh vncdisplay $NAME” ,之后就可以利用vnc-client 访问该虚拟机了。 安装Centos系统 利用vnc-viewer 连接 虚拟机，默认是宿主机IP+5900端口。并进行虚拟机的安装。安装时注意大多数配置默认即可，SOFTWARE SELECTION选择Minimal Install，INSTALLATION DESTINATION需要选择手动配置分区，我们只需要一个根分区即可，不需要swap分区，文件系统选择ext4，存储驱动选择Virtio Block Device。 安装完成后，为了能够联外网，以及用宿主机直接SSH 虚拟机，还要进行网络的配置。KVM虚拟机的网络配置方式主要有两种，Bridge模式和NAT 模式，关于两种模式的介绍可以参考这篇文章，KVM虚拟机网络配置 Bridge方式，NAT方式。我们这里使用默认的NAT模式，但是NAT模式有个弊端就是无法从宿主机SSH虚拟机，这样我们还要做些额外的工作实现该功能。 通过端口转发来实现宿主机直连虚拟机，具体参考这里。 1234567echo 1 &gt;/proc/sys/net/ipv4/ip_forward # 打开ip转发功能，这种方法是暂时的，可以直接修改/etc/sysctl.conf 文件，增加net.ipv4.ip_forward = 1 达到永久效果，文件修该完毕后，要使用sysctl –p使其生效iptables -t nat -A POSTROUTING -o br0 -j MASQUERADE # 开启KVM服务器的IPtables的转发功能iptables -t nat -A PREROUTING -d 192.168.1.102 -p tcp -m tcp --dport 8022 -j DNAT --to-destination 192.168.122.173:22iptables -t nat -A POSTROUTING -s 192.168.122.0/255.255.255.0 -d 192.168.122.173 -p tcp -m tcp --dport 22 -j SNAT --to-source 192.168.122.1 配置系统 SSH到虚拟机后就可以开始一系列的配置工作，首先修改yum源为国内源。 安装acpid服务，并设置开机自启动（acpid服务是用于hypervisor与虚拟机的交互）。 12yum install -y acpidsystemctl enable acpid 虚拟机需要打开boot日志功能，并指定console，这样nova console-log才能获取虚拟机启动时的日志。修改配置文件/etc/default/grub，设置GRUB_CMDLINE_LINUX为： 1GRUB_CMDLINE_LINUX=" vconsole.keymap=us console=tty0 vconsole.font=latarcyrheb-sun16 console=ttyS0,115200" 执行以下语句： 1grub2-mkconfig -o /boot/grub2/grub.cfg ubuntu 的话执行 update-grub。 手动安装qemu-guest-agent(版本必须是2.3+，L 版的动态修改密码需要用到)： 1yum install -y qemu-guest-agent 配置qemu-ga，修改/etc/sysconfig/qemu-ga，配置内容为: 123456TRANSPORT_METHOD="virtio-serial"DEVPATH="/dev/virtio-ports/org.qemu.guest_agent.0"LOGFILE="/var/log/qemu-ga/qemu-ga.log"PIDFILE="/var/run/qemu-ga.pid"BLACKLIST_RPC=""FSFREEZE_HOOK_ENABLE=0 为了使虚拟机能够和外部的metadata service通信，需要禁用默认的zeroconf route： 1echo "NOZEROCONF=yes" &gt;&gt; /etc/sysconfig/network 最后安装cloud-init： 1yum install -y cloud-init 为了实现自动扩容, 需要安装growpart： 1234yum update -yyum install -y epel-releaseyum install -y cloud-utils-growpartrpm -qa kernel | sed 's/^kernel-//' | xargs -I &#123;&#125; dracut -f /boot/initramfs-&#123;&#125;.img &#123;&#125; 基本完成，虚拟机关机。 宿主机上进行清理工作： 12virt-sysprep -d centos7 # 移除mac地址信息virsh undefine centos7 # 删除虚拟机 上传镜像 镜像格式转换（ceph只支持raw格式） 1qemu-img convert -f qcow2 -O raw centos.qcow2 centos.raw 直接用glance上传镜像就不赘述了，这里有个窍门就是如果后端是ceph的话可以利用rbd import 的方式来加快速度，如下。 首先glance create 一条记录，但没有执行镜像上传操作，只是新建一条数据库记录。并记录下image-id。 1glance image-create 利用ceph import 镜像并执行快照 123rbd --pool=volumes import centos.raw --image=$IMAGE_ID --new-format --order 24rbd --pool=volumes --image=$IMAGE_ID --snap=snap snap createrbd --pool=volumes --image=$IMAGE_ID --snap=snap snap protect 完善镜像的必要属性 1234glance image-update --name="centos-7.2-64bit" --disk-format=raw --container-format=bare $IMAGE_ID# 配置qemu-ga，该步骤是必须的，否则libvert启动虚拟机时不会生成qemu-ga配置项，导致虚拟机内部的qemu-ga由于找不到对应的虚拟串行字符设备而启动失败，提示找不到channelglance image-update --property hw_qemu_guest_agent=yes $IMAGE_ID 设置镜像的location url1glance location-add --url rbd://$FS_ROOT/glance_images/$IMAGE_ID/snap $IMAGE_ID#这里的$FS_ROOT 可以通过查看ceph -s 中的cluster. 具体参考手动制作Openstack镜像 的上传镜像部分。 自动创建镜像工具openstack 文档提供了几个类似的工具，没有尝试，Tool support for image creation. 参考文章openstack doc:Example: CentOS image 手动制作Openstack镜像 制作openstack用的centos6.5镜像 KVM虚拟机网络配置 Bridge方式，NAT方式 烂泥：KVM使用NAT联网并为VM配置iptables端口转发，kvmiptables 谈谈Openstack的CentOS镜像 Image requirements 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--初始化虚拟机实例密码几种方法]]></title>
    <url>%2F2017%2F01%2F13%2Fopenstack-init-instance-password%2F</url>
    <content type="text"><![CDATA[目录结构：引出 关于cloud init openstack 初始化虚拟机密码几种方式 引出 最初的问题是这样的：公司的云平台没有提供虚拟机密码重置的功能，需要加上，但我看到创建虚拟机的时候是可以设置密码的，于是想将重置密码的功能重用这部分，结果发现问题不是这么简单。 关于虚拟机重置密码放在下一篇，今天主要写写初始化密码的几种方式。 其实虚拟机初始化设置密码，跟虚拟机初始化设置网络，主机名是一样的道理，都是通过cloud-init来实现。 先看下cloud-init。 关于cloud init Cloud-Init 是一个用来自动配置虚拟机的初始设置（如主机名，网卡和密钥）的工具（支持Linux,对于Windows系列有一个类似的工具，cloudbase-init）。它可以在使用模板部署虚拟机时使用，从而达到避免网络冲突的目的。在使用这个工具前，cloud-init 软件包必须在虚拟机上被安装。安装后，Cloud-Init 服务会在系统启动时搜索如何配置系统的信息。您可以使用只运行一次窗口来提供只需要配置一次的设置信息；或在新建虚拟机、编辑虚拟机和编辑模板窗口中输入虚拟机每次启动都需要的配置信息。（以上内容直接引用自red hat 官网） cloud-init 的配置数据有两种： userdata:文件的形式，常见的如yaml文件，shell scripts，cloud config file。（cloud-init会自动识别以 “#!” 或 “#cloud-config” 开头的文件。） metedata:键值对的形式，常见的如：server name, instance id, display name and other cloud specific details. cloud-init 的数据源：即cloud-init会从以下几种方式获取userdata或metedata： EC2方式：顾名思义，就是AWS的那一套，其实本质是创建一个http server,虚拟机通过这个http server获取到instance的userdata和metadata。这个http server的IP 通常为169.254.169.254。 Config Drive方式：主要是openstack提供的一种方式，本质是把data写入一个特殊的配置设备中，然后在虚拟机启动时，自动挂载并读取 metadata 信息。 Alt cloud：通常用于 RHEVm 和 vSphere中获取userdata。 vSphere 用于为了向VMWare的vSphere虚拟机中注入userdata。 No Cloud 允许用户在没有网络的情况下提供user-data和medatata给instances openstack 初始化虚拟机密码几种方式因为目前大部分私有云平台基本是KVM虚拟化，后台存储为ceph，所以只选适合这种架构的方案，早期的inject-password,貌似只支持QCOW2，不支持RAW镜像(没有验证，但在ceph的文档里确实把这个功能的相关配置默认为false,而且推荐使用cloud-init)以及只支持XEN的clear-password ， root-password等API不再考虑。 利用user-data注入：我司的云平台就是使用这种方式，用法很简单 cloud-init数据源是采用最广泛的EC2模式，EC2的restAPI相关知识check this。关于metadata server的配置不在描述，主要是Nova，和Neutron 的相关配置（默认配置即可）。 登录到对应的虚拟机验证下： 关于metadata和userdata的服务机制,参考OpenStack 的 metadata 服务机制 修改cloud-init的方式：注意在执行命令nova boot的时候会生成一个adminPass的字段。当然，如果你想用自己的密码，也可以在meta中指定，然后修改下代码即可，参考这里 。 我们可以获取该字段，然后交给cloud-init进行密码的设置。前文所说，虚机在通过cloud-init获取元数据时可以使用api-metadata、ConfigDrive等方式，但是因为只有ConfigDrive方式才会把adminPass字段传递给虚机，所以这里我们只能用config drive的方式。关于config drive的使用方式，check this 。而且，为了能够使cloud-init获取到该字段，需要加个patch: 12345678910111213141516171819202122232425diff --git a/cloudinit/config/cc_set_passwords.py b/cloudinit/config/cc_set_passwords.pyindex 4ca85e2..5b5cae4 100644--- a/cloudinit/config/cc_set_passwords.py+++ b/cloudinit/config/cc_set_passwords.py@@ -44,6 +44,12 @@ def handle(_name, cfg, cloud, log, args): else: password = util.get_cfg_option_str(cfg, "password", None)- # use the admin_pass available in the ConfigDrive- if not password:- metadata = cloud.datasource.metadata- if metadata and 'admin_pass' in metadata:- password = metadata['admin_pass']+ expire = True pw_auth = "no" change_pwauth = False@@ -59,6 +65,8 @@ def handle(_name, cfg, cloud, log, args): (user, _user_config) = ds.extract_default(users) if user: plist = "%s:%s" % (user, password)- #add change root password- plist = plist + "\nroot:%s" % password else: log.warn("No default or defined user to change password for.") 同时cloud.cfg中也要添加: chpasswd: { expire: False } 这样保证修改的密码不是过期的，否则vm启动后输入密码，系统让你重新修改才能进入。 3, 利用L版的新特性set-admin-password：这种方式本身要求的条件有点高，没有去尝试，详见虚拟机系统密码的修改方案 参考文章How to reset password of openstack instance using KVM and libvirt? Cloud Init and Config Drive Cloud-init初探 和 OpenStack应用 openstack 中 metadata 和 userdata 的配置和使用 虚拟机系统密码的修改方案 设置虚拟机密码 Cloud config examples 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack 系列--nova-api 的paste&route的简单抽象]]></title>
    <url>%2F2017%2F01%2F12%2Fopenstack-pasteDeploy%26route%26application%2F</url>
    <content type="text"><![CDATA[目录结构：由Python的调用实例想到的 nova-api 的paster&amp;route 模型 由Python的调用实例想到的 公司的云平台有两个自己写的服务，highland和rainflow。也没有开发文档，只能自己去看代码琢磨一下，从paste配置文件看下最终调用的route app，是一个factory方法，我记得factory方法应该返回一个WSGI APP,但是看下源码是这样的： 1234@classmethoddef factory(cls, global_config, **local_config): """Simple paste factory, :class:`rainflow.wsgi.Router` doesn't have""" return cls() 是一个类方法返回这个类，我就有点懵了，这样是执行什么操作呢，难道是再创建一个实例。而且nova-api也是这么干的，证明不是错误。后来搜了一下资料，再试验了一下大致知道怎么回事了。 看下下面这个示例： 123456789101112131415#!/usr/bin/env pythonclass demo: @classmethod def iam_classmethod(cls): print("classmethod") return cls() def __init__(self): print("i am init") def __call__(self): print"i am calling"test=demo.iam_classmethod()test() 运行结果为： 123classmethodi am initi am calling iam_classmethod方法会返回一个类实例，执行这个类实例就是调用call() 方法。同理，上面那个代码其实就是调用call方法，我们可以在对应的父类找到。 nova-api 的paster&amp;route 模型 nova-api的路由模型其实不难，只不过层层封装，比较复杂，在网上找到一个比较好理解的示例。 首先要明白路由的生成过程跟路由的寻址过程是两条线，当我们启动nova-api的时候，路由的map就已经生成了，当我们发送rest请求时，就会根据路由的map去找对应的application以及controller。所以，重点是看下路由的生成过程。 看下一个简单的代码实现（代码来自网上，没有经过验证，只是为了说明）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293from __future__ import print_functionfrom routes import Mapperimport webob.decimport webob.excimport routes.middlewareimport testtoolsclass MyController(object): def getlist(self, mykey): print("step 4: MyController's getlist(self, mykey) is invoked") return "getlist(), mykey=" + mykeyclass MyApplication(object): """Test application to call from router.""" def __init__(self, controller): self._controller = controller def __call__(self, environ, start_response): print("step 3: MyApplication is invoked") action_args = environ['wsgiorg.routing_args'][1].copy() try: del action_args['controller'] except KeyError: pass try: del action_args['format'] except KeyError: pass action = action_args.pop('action', None) controller_method = getattr(self._controller, action) result = controller_method(**action_args) start_response('200 OK', [('Content-Type', 'text/plain')]) return [result]class MyRouter(object): """Test router.""" def __init__(self): route_name = "dummy_route" route_path = "/dummies" my_application = MyApplication(MyController()) self.mapper = Mapper() self.mapper.connect(route_name, route_path, controller=my_application, action="getlist", mykey="myvalue", conditions=&#123;"method": ['GET']&#125;) self._router = routes.middleware.RoutesMiddleware(self._dispatch, self.mapper) @webob.dec.wsgify(RequestClass=webob.Request) def __call__(self, req): """Route the incoming request to a controller based on self.map. If no match, return a 404. """ print("step 1: MyRouter is invoked") return self._router @staticmethod @webob.dec.wsgify(RequestClass=webob.Request) def _dispatch(req): """Dispatch the request to the appropriate controller. Called by self._router after matching the incoming request to a route and putting the information into req.environ. Either returns 404 or the routed WSGI app's response. """ print("step 2: RoutesMiddleware is invoked, calling our _dispatch back") match_dict = req.environ['wsgiorg.routing_args'][1] if not match_dict: return webob.exc.HTTPNotFound() app = match_dict['controller'] return app class RoutingTestCase(testtools.TestCase): def test_router(self): router = MyRouter() result = webob.Request.blank('/dummies').get_response(router) self.assertEqual(result.body, "getlist(), mykey=myvalue") 关于添加一个路由请求的操作参考这里openstack-wsgi的route中增加api流程详解（os-networks）增加 参考文章使用Python Routes + webob 的 REST API的测试程序. openstack-wsgi的route中增加api流程详解（os-networks）增加 浅析openstack中WSGI+Restful router“框架？” 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web系列--关于http，tcp,amqp的一些思考]]></title>
    <url>%2F2017%2F01%2F10%2Fsomething-about-http%26tcp%26amqp%2F</url>
    <content type="text"><![CDATA[目录结构：为什么TCP是可以保持长连接的，而基于TCP的HTTP却不能 AMQP VS HTTP 为什么TCP是可以保持长连接的，而基于TCP的HTTP却不能 可能这个问题本身表述就不是很清楚，但我也想不出其他合适的词汇了。直接上例子吧：我们在做web应用的时候有时会碰到需要实时推送的情况，但是HTTP协议本身是不支持长时间的连接的，因为HTTP本身是request/response型的协议，可以理解为半双工通讯。而TCP 是支持长连接的，是通过heartbeat来进行连接的保持。这样，就引出我上面提到的问题了。 在解答这个问题之前，还有几点是要说明的：http1.1中其实也有长连接的概念，具体到header参数就是keep-alive，但是这个keep-alive是一个数值，比如20s,就是这个http连接对应的tcp连接会保持20S,这个时间段过来的http连接会复用上次的tcp连接。如果没有的话，那么会开启一个新的tcp连接。而tcp协议里面的keep-alive就是heartbeat发送的包内容，表示“我还活着”。 接下来回答一下上面那个问题：其实很简单，我们混淆了“基于TCP协议”这个概念，TCP本身是一个传输层的协议，就是用来作数据传输的，也决定了它必然是全双工，可以保持连接的特性。再来看下http协议，一种request/response型的协议，为什么要做成request/response这种类型呢，因为server端要服务的client太多了，如果每一个连接都保持而不断开，那么服务器的IO很快就会被堵死了。所以http基于tcp,并非就是全部利用的tcp的特性，在http连接建立之前，的确是需要tcp三次握手建立tcp连接，然后利用这个tcp连接传输http包数据，服务端收到请求的数据后，发送对应的response,之后便关闭了这个tcp连接（如果没有keep-alive）。 再回到刚才的例子：其实现在解决这种实时推送的方案还是很丰富的，比较主流的应该是利用websocket协议来实现。当然，其他的比如：ajax实现的轮询等。就不细讲了，感兴趣就自行Google吧。 AMQP VS HTTP其实这个问题应该这样描述：在做一个SOA服务时，用HTTP好还是AMQP好？ 该问题的提出主要是因为openstack的内部通信中主要包含rest和Rabbimq两种通信方式，所以不禁对于两种方式的优劣提出了疑问，经过一番思考以及搜索资料后，找到了相对满意的答案： 主要参考When designing Service Oriented Architectures, which one do you use - HTTP or AMQP? AMQP is damn awesome! 确实，amqp更快，可定制性更强，可以实现异步传输，相对于http来说，作为SOA的信息传输协议真是再好不过了。简单列举下AMQP碾压HTTP 的优点，此处以rabbitmq为例： rabbitmq可以提供面向连接，可靠的信息传递。 rabbitmq提供过载保护：比如设置queue的timeout,限制 message rate,甚至转移message到别的queue. http需要有一个service discovery 机制，rabbitmq 不用。 rabbitmq 的loadbalance 使用非常简单 rabbitmq 的后端如果做了热备，如果一个后端节点挂掉，会自动切换到另一节点。 再说下http的优点，毕竟年岁要长一些，SOA的框架非常成熟，学习的曲线也不高，lz之前用过阿里出品的dubbo框架，简单易用。更重要的是，http作为一种成熟的协议标准，想要满足任何需求基本都会有对应的中间件，而且scale up 的成本低。 综上所述，不难看出，在真正用到SOA时，还是需要取决于实际情况来取舍。如果只是一个内部的系统，交互的模块相对紧密的话推荐用AMQP。如果是一个大型的web系统，与外部交互比较多，而且是多部门协作完成多个模块的开发，那么模块件的交互还是使用标准的http吧。 对应到openstack也可以看出一二：模块之间交互都是使用http/rest,比如nova与keystone的认证交互，因为每个独立的模块都是一个项目，都有独立的开发团队。而在模块内部，比如，Nova内部，nova-api与nova-scheduler,nova-scheduler与nova-compute 都是使用amqp来实现。 参考文章The Problem We Solve 用两条 HTTP 请求就可以实现双向实时通讯吧？为什么还需要 WebSocket 微服务与SOA之间差了一个ESB When designing Service Oriented Architectures, which one do you use - HTTP or AMQP? 关于 RabbitMQ 的实时消息推送 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python系列--python decorator 简 记]]></title>
    <url>%2F2016%2F12%2F30%2Fpython-decorator%2F</url>
    <content type="text"><![CDATA[目录结构：装饰器的本质 装饰器的几种形式 注：以下内容皆出自 酷壳 PYTHON修饰器的函数式编程 ,本文只是便于自己记忆，做的一篇简单记录。 装饰器的本质看下一个最简单的装饰器的例子： 123456789101112def hello(fn): def wrapper(): print "hello, %s" % fn.__name__ fn() print "goodby, %s" % fn.__name__ return wrapper @hellodef foo(): print "i am foo" foo() @注解 是一个Python 中的语法糖，比如一个如下语句123@decoratordef func(): pass 它会被解释为 ：1func = decorator(func) 其实就是把一个函数当作参数传到另一个函数，然后再回调，最后再把decorator这个函数的返回值赋值回了原来的func。 落实到刚开始的例子就是：12345678910111213def hello(fn): def wrapper(): print "hello, %s" % fn.__name__ fn() print "goodby, %s" % fn.__name__ return wrapperdef foo(): print "i am foo"foo = hello(foo)foo() OK,这样就理清了！ 装饰器的几种形式 带参数的形式： 1234567891011121314151617def makeHtmlTag(tag, *args, **kwds): def real_decorator(fn): css_class = " class='&#123;0&#125;'".format(kwds["css_class"]) \ if "css_class" in kwds else "" def wrapped(*args, **kwds): return "&lt;"+tag+css_class+"&gt;" + fn(*args, **kwds) + "&lt;/"+tag+"&gt;" return wrapped return real_decorator @makeHtmlTag(tag="b", css_class="bold_css")@makeHtmlTag(tag="i", css_class="italic_css")def hello(): return "hello world" print hello()# 输出：# &lt;b class='bold_css'&gt;&lt;i class='italic_css'&gt;hello world&lt;/i&gt;&lt;/b&gt; 为了让 hello = makeHtmlTag(arg1, arg2)(hello) 成功，makeHtmlTag 必需返回一个decorator（这就是为什么我们在makeHtmlTag中加入了real_decorator()的原因），这样一来，我们就可以进入到 decorator 的逻辑中去了—— decorator得返回一个wrapper，wrapper里回调hello。 类的形式 1234567891011121314151617181920212223class myDecorator(object): def __init__(self, fn): print "inside myDecorator.__init__()" self.fn = fn def __call__(self): self.fn() print "inside myDecorator.__call__()" @myDecoratordef aFunction(): print "inside aFunction()" print "Finished decorating aFunction()" aFunction() # 输出：# inside myDecorator.__init__()# Finished decorating aFunction()# inside aFunction()# inside myDecorator.__call__() 可以看出在构造装饰器装饰的函数时，执行init函数，在真正调用aFunction()函数时会调用call。 看下重写上面的html.py的代码： 1234567891011121314151617class makeHtmlTagClass(object): def __init__(self, tag, css_class=""): self._tag = tag self._css_class = " class='&#123;0&#125;'".format(css_class) \ if css_class !="" else "" def __call__(self, fn): def wrapped(*args, **kwargs): return "&lt;" + self._tag + self._css_class+"&gt;" \ + fn(*args, **kwargs) + "&lt;/" + self._tag + "&gt;" return wrapped @makeHtmlTagClass(tag="b", css_class="bold_css")@makeHtmlTagClass(tag="i", css_class="italic_css")def hello(name): return "Hello, &#123;&#125;".format(name) 注意参数的调用，如果decorator有参数的话，init()成员就不能传入fn了，而fn是在call的时候传入的。 被decorator的函数其实已经是另外一个函数了，对于最前面那个hello.py的例子来说，如果你查询一下foo.name的话，你会发现其输出的是“wrapper”，而不是我们期望的“foo”，这会给我们的程序埋一些坑。所以，Python的functool包中提供了一个叫wrap的decorator来消除这样的副作用。所以就有了下面这种形式： 123456789101112131415161718from functools import wrapsdef hello(fn): @wraps(fn) def wrapper(): print "hello, %s" % fn.__name__ fn() print "goodby, %s" % fn.__name__ return wrapper @hellodef foo(): '''foo help doc''' print "i am foo" pass foo()print foo.__name__ #输出 fooprint foo.__doc__ #输出 foo help doc 参考文章PYTHON修饰器的函数式编程 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--AllInOne openstack 实验环境搭建]]></title>
    <url>%2F2016%2F12%2F26%2FAllInOne-openstack-environ%2F</url>
    <content type="text"><![CDATA[目录结构：虚拟机搭建 利用RDO 搭建allinone openstack 环境 利用Fuel 搭建openstack 环境 虚拟机搭建 virtualbox 下载安装，centos 7下载 利用virtualbox启动centos 7.遇到问题如下： 网络一直连不通，后来参考这篇文章VirtualBox下虚拟机和主机内网互通+虚拟机静态IP的网络配置 不过我只用了一个虚拟网卡就够用了，因为只建一个虚拟机，不用虚拟机之间联通。可以使用nat，或者桥接模式。最好使用桥接模式，因为可以使用客户端SSH连接。如果网络没通的话可能还需要配置一下网络参数，比如IP地址，dns地址等。 利用RDO 搭建allinone openstack 环境 搭建openstack实验环境大致有三种：RDO的packstack,Mirantis 的Fuel安装，Devstack。这三种方式所有的安装过程都不难，但是因为网络的问题，总会出现很多意料之外的事情。总体来说，RDO这种allinone的方式是最简单的，就算出了问题，80%的可能出在网络上。 按照quick start进行安装。 出现的问题： 安装 yum install -y openstack-packstack 时出现“Package does not match intended download” ，这个问题google看了好几页一直没解决，后来我在家里的时候用家里的网络就可以了，估测是公司的网络用了代理，存在缓存的原因。 最后执行 packstack –allinone 时经常卡住，一般卡在 testing if puppet apply is finished 的居多，其实也是网络问题，我在深夜网速较好的时候成功率比较高。 搭建完成后就可以在虚拟机上进行调试了，推荐一个vim 的设置A set of vim, zsh, git, and tmux configuration files.(*nix开发环境一键配置） 利用Fuel 搭建openstack 环境 目前最新版本还是openstack M版。主要是参考这篇文章部署安装Mirantis OpenStack Fuel 9.0。 没有部署成功，fuel server 部署成功了，但是最终部署openstack的时候死活过不去，上篇文章的作者也说可重复性不高，看报错日志是download error，也是网络问题。以后有机会大半夜再搞一发。 上篇文章没有提到的一件事是 在fuel server部署完成后，ubuntu镜像最好是自己下载然后上传到 Fuel server,设置resposity 的时候设置为本地源，这样可以通过网络验证。 参考文章部署安装Mirantis OpenStack Fuel 9.0 quick start A set of vim, zsh, git, and tmux configuration files.(*nix开发环境一键配置） 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--cinder 源码解读(二)---常见cinder操作工作流程]]></title>
    <url>%2F2016%2F12%2F26%2Fcinder-source-code%20explain-2%2F</url>
    <content type="text"><![CDATA[目录结构：cinder 源码大致解析 以新建 cinder volume 为例 深入 cinder 代码 关于cinder不再赘述，之前文章由介绍。 cinder 的 API 目前有三个版本，本次讨论以v2为基准。 因为版本不同，代码也可能不同，该版本为Openstack Liberty cinder 源码大致解析看下cinder项目的入口文件setup.cfg： 123456789console_scripts = cinder-api = cinder.cmd.api:main cinder-backup = cinder.cmd.backup:main cinder-manage = cinder.cmd.manage:main cinder-rootwrap = oslo_rootwrap.cmd:main cinder-rtstool = cinder.cmd.rtstool:main cinder-scheduler = cinder.cmd.scheduler:main cinder-volume = cinder.cmd.volume:main cinder-volume-usage-audit = cinder.cmd.volume_usage_audit:main cinder-api: 启动cinder 的wsgi server,每一个API对应一种资源，在setup.cfg 的entry point 中进行了配置，使用stevedore进行加载。 cinder-backup:将volume 备份到其他存储系统 cinder-manage：提供对cinder其他服务的管理操作。 cinder-rootwrap : 某些cinder操作需要root权限 cinder-rtstool :如果iSCSI target管理工具不是使用的默认的tgtadm，而是使用LIO iSCSI的话，就可以使用该命令，能够让你创建，删除，和验证卷，以及决定target，且可添加iSCSI initiator到系统。 cinder-scheduler :用于根据提供的策略调度cinder-volune节点 cinder-volume：管理 volume的生命周期，与底层存储交互。 cinder-volume-usage-audit：用于卷使用情况统计。 以新建 cinder volume 为例 深入 cinder 代码首先看下cinder –debug create volume 都发送了那些请求： 可以看到先去keystone验证，然后向cinder-api发送一个v2/{tenant-id}/volumes 的post请求，volume创建，然后发送一个v2/{tenant-id}/volumes/{volume-id} 的get请求获取创建的volume数据。 接下来看下cinder-api中接受create volume 的post请求代码，路由过程略过： /cinder/api/v2/volumes.py 主要代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@wsgi.response(202) def create(self, req, body): """Creates a new volume.""" .....对传入参数的检测....... kwargs['metadata'] = volume.get('metadata', None) snapshot_id = volume.get('snapshot_id') if snapshot_id is not None: if not uuidutils.is_uuid_like(snapshot_id): msg = _("Snapshot ID must be in UUID form.") raise exc.HTTPBadRequest(explanation=msg) # Not found exception will be handled at the wsgi level kwargs['snapshot'] = self.volume_api.get_snapshot(context, snapshot_id) else: kwargs['snapshot'] = None source_volid = volume.get('source_volid') if source_volid is not None: # Not found exception will be handled at the wsgi level kwargs['source_volume'] = \ self.volume_api.get_volume(context, source_volid) else: kwargs['source_volume'] = None source_replica = volume.get('source_replica') if source_replica is not None: # Not found exception will be handled at the wsgi level src_vol = self.volume_api.get_volume(context, source_replica) if src_vol['replication_status'] == 'disabled': explanation = _('source volume id:%s is not' ' replicated') % source_replica raise exc.HTTPBadRequest(explanation=explanation) kwargs['source_replica'] = src_vol else: kwargs['source_replica'] = None consistencygroup_id = volume.get('consistencygroup_id') if consistencygroup_id is not None: # Not found exception will be handled at the wsgi level kwargs['consistencygroup'] = \ self.consistencygroup_api.get(context, consistencygroup_id) else: kwargs['consistencygroup'] = None size = volume.get('size', None) if size is None and kwargs['snapshot'] is not None: size = kwargs['snapshot']['volume_size'] elif size is None and kwargs['source_volume'] is not None: size = kwargs['source_volume']['size'] elif size is None and kwargs['source_replica'] is not None: size = kwargs['source_replica']['size'] LOG.info(_LI("Create volume of %s GB"), size) if self.ext_mgr.is_loaded('os-image-create'): image_ref = volume.get('imageRef') if image_ref is not None: image_uuid = self._image_uuid_from_ref(image_ref, context) kwargs['image_id'] = image_uuid kwargs['availability_zone'] = volume.get('availability_zone', None) kwargs['scheduler_hints'] = volume.get('scheduler_hints', None) kwargs['multiattach'] = utils.get_bool_param('multiattach', volume) new_volume = self.volume_api.create(context, size, volume.get('display_name'), volume.get('display_description'), **kwargs) retval = self._view_builder.detail(req, new_volume) return retval 参考文章深入理解Openstack QoS控制实现与实践 Volume Transfer(volume过户) openstack使用Ceph存储后端创建虚拟机快照原理剖析 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--cinder 源码解读(一)---常见命令行操作]]></title>
    <url>%2F2016%2F12%2F23%2Fcinder-sourcecode-explain-1%2F</url>
    <content type="text"><![CDATA[目录结构：cinder 的命令行操作 关于cinder不再赘述，之前文章由介绍。 cinder 的 API 目前有三个版本，本次讨论以v2为基准。 因为版本不同，代码也可能不同，该版本为Openstack Liberty cinder 的命令行是由python-cinderclient 项目提供（已逐渐整合进python-openstackclient项目） 除此之外，还有一个cinder-manage 的命令行，主要是对cinder 各服务的管理，是由cinder项目本身cinder-manage模块提供。 cinder 的命令行操作直接命令行：cinder help ，查看一下subcommand 都包含哪些操作。（本想用图片展示，画了半天，太麻烦了。） 关于 volume 的操作： create/delete/list/rename/show force-delete :不管state如何，强制删除 migrate:迁移至另一主机 set-bootable:设置为可启动盘 reset-state:设置volume 的state upload-to-image : bootable 必须为True readonly-mode-update :设置为只读模式 关于 type 的操作： 操作如图： volume type 还是比较重要的，后面qos的设置的时候就是与volume-type 绑定的。 还有一个encryption-type 也是 type相关，绑定instance之后，该磁盘上的内容会加密。 关于 snapshot 的操作： 关于 backup 的操作： backup 与 snapshot 的区别：1，snapshot 对于原volume是强依赖的，要想删除volume，必须先删除依赖他的snapshot，snapshot主要用于恢复数据到某个时间点，且原volume破损，那么对应的snapshot也不可用。而backup则对volume就不具有依赖性。所以即使备份相关的卷出现故障，还是可以恢复备份中数据。操作方法是创建一个新的空白卷，将备份restore到这个空白卷即可。2，snapshot 只能在同一存储区域（比如 ceph 里就是同一volume,backup 可以转存到别的地方） 补：发现如果是从snapshot 创建的volume，删除snapshot 之前要先删除依赖的volume，否则，删除不了。而且没有提示，不知道是否有bug，或是版本太低，下次用最新版试下。 关于 qos的操作： qos 要与volume type 一块使用。 深入理解Openstack QoS控制实现与实践 关于 quota 的操作： 关于 transfer 的操作： 所谓 transfer 就是admin把 volume 从一个project 转到另一个project。 关于 service 的操作： 看下service-list: enable/disable 即是对status的操作 参考文章深入理解Openstack QoS控制实现与实践 Volume Transfer(volume过户) openstack使用Ceph存储后端创建虚拟机快照原理剖析 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph系列-- ceph 运维(二)-----常见TroubleShooting]]></title>
    <url>%2F2016%2F12%2F21%2Fceph-command-2%2F</url>
    <content type="text"><![CDATA[目录结构：监视器MON故障排除 OSD 故障排除 归置组排障 监视器MON故障排除 简单记下修复 monmap的过程： 1. 如果该mon节点是quorum 节点，直接获取monmap: ceph mon getmap -o /tmp/monmap 2. 如果该mon节点不是quorum 节点，从别的quorum节点(假设有ID，且已stop)获取:ceph-mon -i ID --extract-monmap /tmp/monmap 3. stop 你要修复的mon节点的mon进程 4. 注入monmap:ceph-mon -i ID --inject-monmap /tmp/monmap 5. start monitor OSD 故障排除 收集 OSD 数据 查看ceph日志：/var/log/ceph 利用管理套接字：ceph daemon {socket-file} help （可以获取在运行时配置，历史操作，操作的优先队列状态，在进行的操作，列出性能计数器） 查看空间占用：df -h I/O 统计信息: iostat -x 诊断消息: dmesg | grep scsi 停止自动重均衡 如果不想在停机维护 OSD 时让 CRUSH 自动重均衡，提前设置 noout ：ceph osd set noout 停机维护失败域内的 OSD 维护结束后，重启OSD。 解除 noout 标志：ceph osd unset noout OSD 未成功运行通常情况下，简单地重启 ceph-osd 进程就可以重回集群并恢复。 OSD 运行不起来：检查配置文件（语法错误），或配置路径错误。查看是否超过系统默认最大线程数，以及与内核版本是否冲突。 OSD down: 查看日志是否因为硬盘错误 OSD 龟速或无响应 有很多原因会导致此问题： 网络问题：netstat -s 驱动器配置:一个存储驱动器应该只用于一个 OSD 。 坏扇区和碎片化硬盘. 监视器和 OSD 在同一主机：监视器是普通的轻量级进程，但它们会频繁调用 fsync() ，这会妨碍其它工作量，特别是监视器和 OSD 共享驱动器时。 日志记录级别：如果你为追踪某问题提高过日志级别、但结束后忘了调回去，这个 OSD 将向硬盘写入大量日志。如果你想始终保持高日志级别，可以考虑给默认日志路径挂载个硬盘。 内核与 SYNCFS 问题：试试在一主机上只运行一个 OSD ，看看能否提升性能。老内核未必支持有 syncfs(2) 系统调用的 glibc 。 文件系统问题：推荐基于 xfs 或 ext4 部署集群。 内存不足：建议为每 OSD 进程规划 1GB 内存。 OLD REQUESTS 或 SLOW REQUESTS：如果某 ceph-osd 守护进程对一请求响应很慢，它会生成日志消息来抱怨请求耗费的时间过长。 一会up一会down 的OSD 即俗称的打摆子。网络部署时建议同时部署公网（前端）和集群网（后端），这样能更好地满足对象复制的容量需求。另一个优点是你可以运营一个不连接互联网的集群，以此避免拒绝攻击。 OSD 们互联和检查心跳时会优选集群网（后端），如果集群网（后端）失败、或出现了明显的延时，同时公网（前端）却运行良好， 这时 OSD 们会向监视器报告邻居 down 了、同时报告自己是 up 的，我们把这种情形称为打摆子（ flapping ）。可以强制OSD停止标记： 12ceph osd set noup # prevent OSDs from getting marked upceph osd set nodown # prevent OSDs from getting marked down 下列命令可清除标记： 12ceph osd unset noupceph osd unset nodown 归置组排障在我们查看ceph 状态的时候，通常会看到pgmap的状态：active+clean。这是正常状态。不过也有不正常的时候。 pg 状态为 inactive—–归置组长时间无活跃（即它不能提供读写服务了）：OSD 的互联心跳检测失败，标记为down,重启对应osd进程。 pg 状态为unclean——归置组长时间不干净（例如它未能从前面的失败完全恢复）：找不到对象。 pg 状态为stale—–归置组状态没有被 ceph-osd 更新，表明存储这个归置组的所有节点可能都挂了。 详参归置组排障 参考文章监视器故障排除 OSD 故障排除 归置组排障 日志记录和调试 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph系列-- ceph 运维(一)-----常用命令]]></title>
    <url>%2F2016%2F12%2F20%2Fceph-command-1%2F</url>
    <content type="text"><![CDATA[目录结构：ceph 数据组织方式 ceph 增加/删除OSD ceph 增加/删除MON 操作 ceph集群中的服务 监控 ceph 集群的一些命令 ceph 用户管理 ceph 数据管理命令 关于ceph 之前只是简单介绍过，并没有系统的去学习，总结过。之后几篇文章会详细介绍下ceph的常用运维命令。 ceph 数据组织方式首先看下ceph的数据组织方式，因为ceph与openstack结合时用到最多的便是块存储服务rbd，所以这里主要以rbd为例。 ceph 数据的组织方式是在物理资源的基础上进行了多层映射。 客户想要创建一个rbd设备前，必须创建 一个pool，需要为这个pool指定pg的数量，在一个pool中的pg数量是不一定的，同时这个pool中要指明保存数据的副本数量3个副本。再在这个pool中创建一个rbd设备rbd0，那么这个rbd0都会保存三份，在创建rbd0时必须指定rbd的size，对于这个rbd0的任何操作不能超过这个size。之后会将这个块设备进行切块，每个块的大小默认为4M，并且每个块都有一个名字，名字就是object+序号。将每个object通过pg进行副本位置的分配，pg会寻找3个osd，把这个object分别保存在这三个osd上。osd上实际是把底层的disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统。最后对于object的存储就变成了存储一个文件rbd0.object1.file。 客户端使用rbd设备时，一般有两种方法。 第一种 是kernel rbd。就是创建了rbd设备后，把rbd设备map到内核中，形成一个虚拟的块设备，这时这个块设备同其他通用块设备一样，一般的设备文件为/dev/rbd0，后续直接使用这个块设备文件就可以了，可以把/dev/rbd0格式化后mount到某个目录，也可以直接作为裸设备使用。这时对rbd设备的操作都通过kernel rbd操作方法进行的。 第二种是librbd方式。就是创建了rbd设备后，这时可以使用librbd、librados库进行访问管理块设备。这种方式不会map到内核，直接调用librbd提供的接口，可以实现对rbd设备的访问和管理，但是不会在客户端产生块设备文件。 简单看下客户端写数据到osd的过程。 假设这次采用的是librbd的形式，使用librbd创建一个块设备，这时向这个块设备中写入数据，在客户端本地同过调用librados接口，然后经过pool，rbd，object、pg进行层层映射,在PG这一层中，可以知道数据保存在哪3个OSD上，这3个OSD分为主从的关系，也就是一个primary OSD，两个replica OSD。客户端与primay OSD建立SOCKET 通信，将要写入的数据传给primary OSD，由primary OSD再将数据发送给其他replica OSD数据节点。 ceph 增加/删除OSD在官方文档里已经非常非常详细，增加/删除 OSD 简单总结一下： 增加一个osd的流程大致： 预备工作：部署硬件，网络，安装ceph软件包 增加OSD: 创建osd 创建默认目录并挂载硬盘 初始化 OSD 数据目录 注册 OSD 认证密钥 把 OSD 加入 CRUSH 图（crushmap其实就是部署物理环境的一个描述，比如哪几台机器在同一机架上，哪几个机架在同一机房里等等） 启动 OSD 观察数据迁移 删除osd的流程： 踢出集群，置为out 观察数据迁移: ceph -w 停止 OSD 进程 删除 OSD：移出集群 CRUSH 图、删除认证密钥、删除 OSD 图条目、删除 ceph.conf 条目。如果主机有多个硬盘，每个硬盘对应的 OSD 都得重复此步骤。 ceph 增加/删除MON详见官方文档，增加/删除监视器 增加一个mon流程： 预备工作：部署硬件，网络，安装ceph软件包 增加MON: 在新监视器主机上创建mon默认目录 获取监视器密钥环,保存到临时目录 获取监视器运行图，保存到临时目录 准备监视器的数据文件（包括密钥环，运行图文件） 启动监视器，指定绑定端口 删除mon的流程： 停止监视器。 从集群删除监视器。 删除 ceph.conf 对应条目。 操作 ceph集群中的服务ceph 中的服务包含 OSD ,MON两种，对这些服务的操作也无非是stop，start,restart等操作。有很多种方式，详见操纵集群 常用的是以service 的形式： 启动所有守护进程： sudo service ceph -a start (加 -a 即在所有节点上执行) 启动一类守护进程： sudo service ceph start osd (只在本节点启动OSD进程) 启动单个守护进程： sudo service ceph start osd.0 监控 ceph 集群的一些命令 检查鸡群状态：ceph status 或 ceph -s 实时检查集群：ceph -w 检查集群的容量使用情况：ceph df 检查OSD状态：ceph osd stat 或 ceph osd tree 检查MON状态： ceph mon stat 查看MON 选举状态：ceph quorum_status 检查归置组（PG）状态 ：ceph pg stat 归置组的状态有多个，详见监控 OSD 和归置组 管理套接字命令允许你在运行时查看和修改配置 ceph 用户管理 罗列用户：ceph auth list 用户的增删修改参见 用户管理 ceph 数据管理命令只说下最常见的三种数据：pool,pg,和crushmap 的管理。 pool 存储池管理：Ceph 在存储池内存储数据，它是对象存储的逻辑组；存储池管理着归置组数量、副本数量、和存储池规则集。要往存储池里存数据，用户必须通过认证、且权限合适，存储池可做快照。主要操作如下： 列出存储池： ceph osd lspools 创建存储池： ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] [crush-ruleset-name] [expected-num-objects] 设置存储池配额： ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}] 删除存储池： ceph osd pool delete {pool-name} [{pool-name} –yes-i-really-really-mean-it] 重命名存储池： ceph osd pool rename {current-pool-name} {new-pool-name} 查看存储池统计信息： rados df 拍下存储池快照： ceph osd pool mksnap {pool-name} {snap-name} 删除存储池快照：ceph osd pool rmsnap {pool-name} {snap-name} 设置对象副本数：ceph osd pool set {poolname} size {num-replicas} 获取对象副本数：ceph osd dump | grep ‘replicated size’ 归置组(Placement Group)管理：Ceph 把对象映射到归置组（ PG ），归置组是一逻辑对象池的片段，这些对象组团后再存入到 OSD 。归置组减少了各对象存入对应 OSD 时的元数据数量，根据N副本策略，每个归置组对应N个OSD。主要的操作如下： 设置归置组数量：ceph osd pool set {pool-name} pg_num {pg_num}虽然 pg_num 的增加引起了归置组的分割，但是只有当用于归置的归置组（即 pgp_num ）增加以后，数据才会被迁移到新归置组里，所以还要增加一步：ceph osd pool set {pool-name} pgp_num {pgp_num} 获取归置组数量：ceph osd pool get {pool-name} pg_num 获取归置组统计信息：ceph pg dump [–format {format}] 获取一归置组运行图：ceph pg map {pg-id} 获取一 PG 的统计信息：ceph pg {pg-id} query CRUSH 图(CRUSH Map): CRUSH 是重要组件，它使 Ceph 能伸缩自如而没有性能瓶颈、没有扩展限制、没有单点故障，它为 CRUSH 算法提供集群的物理拓扑，以此确定一个对象的数据及它的副本应该在哪里、怎样跨故障域存储，以提升数据安全。详见CRUSH 图。编辑crushmap的步骤如下： 获取 CRUSH 图：ceph osd getcrushmap -o {compiled-crushmap-filename} 反编译 CRUSH 图：crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename} 至少编辑一个设备、桶、规则 编译 CRUSH 图：crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename} 注入 CRUSH 图：ceph osd setcrushmap -i {compiled-crushmap-filename} 参考文章ceph的数据存储之路(2) —– rbd到osd的数据映射 集群运维–ceph doc 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--glanceApi 源代码解读(三)]]></title>
    <url>%2F2016%2F12%2F16%2Fglance-api-sourcecode-explain-3%2F</url>
    <content type="text"><![CDATA[目录结构：glanceApi V2 代码解读 glanceApi V2 代码解读 还是以image create为例，首先debug一下，看下都是发出了哪些restful请求： 本来是要截图的，网络出了点问题，云环境暂时用不了，直接写吧，发出了三个restful请求： 1. GET /v2/schemas/image 2. POST /v2/images 3. PUT /v2/images/xxxxxxxxxxxxxxxx/file 大致猜出第二，三个请求分别是image metedata的数据库操作请求，上传镜像的操作请求。接下来逐一分析： 先看下GET /v2/schemas/image 请求，根据路由的规则router.py找到对应的执行函数： 123456789101112131415161718192021222324252627class API(wsgi.Router): """WSGI router for Glance v2 API requests.""" def __init__(self, mapper): #默认scheme的property就几个， #如果自己增加的话需要写在一个schema-image.json文件中，通过此函数load custom_image_properties = images.load_custom_properties() reject_method_resource = wsgi.Resource(wsgi.RejectMethodController()) #根据配置文件create 一个controller，看下文分析 schemas_resource = schemas.create_resource(custom_image_properties) mapper.connect('/schemas/image', controller=schemas_resource, action='image', conditions=&#123;'method': ['GET']&#125;, body_reject=True) mapper.connect('/schemas/image', controller=reject_method_resource, action='reject', allowed_methods='GET') mapper.connect('/schemas/images', controller=schemas_resource, action='images', conditions=&#123;'method': ['GET']&#125;, body_reject=True) ............................................ schemas.create_resource，看下该方法的实现： 123def create_resource(custom_image_properties=None): controller = Controller(custom_image_properties) return wsgi.Resource(controller) 可以看到该方法初始化了一个Controller,然后封装成一个WSGI APP，看下初始化： 12345678910111213141516171819202122232425262728293031323334class Controller(object): def __init__(self, custom_image_properties=None): #创建一个image_schema self.image_schema = images.get_schema(custom_image_properties) self.image_collection_schema = images.get_collection_schema( custom_image_properties) self.member_schema = image_members.get_schema() self.member_collection_schema = image_members.get_collection_schema() self.task_schema = tasks.get_task_schema() self.task_collection_schema = tasks.get_collection_schema() # Metadef schemas self.metadef_namespace_schema = metadef_namespaces.get_schema() self.metadef_namespace_collection_schema = ( metadef_namespaces.get_collection_schema()) self.metadef_resource_type_schema = metadef_resource_types.get_schema() self.metadef_resource_type_collection_schema = ( metadef_resource_types.get_collection_schema()) self.metadef_property_schema = metadef_properties.get_schema() self.metadef_property_collection_schema = ( metadef_properties.get_collection_schema()) self.metadef_object_schema = metadef_objects.get_schema() self.metadef_object_collection_schema = ( metadef_objects.get_collection_schema()) self.metadef_tag_schema = metadef_tags.get_schema() self.metadef_tag_collection_schema = ( metadef_tags.get_collection_schema()) #调用此image方法，并最终返回image_schema.raw()的对象 def image(self, req): return self.image_schema.raw() 看下创建image_schema的方法images.get_schema： 12345678910111213141516def get_schema(custom_properties=None): #获取描述一个image的基本property，id,name ,status,owner等 properties = get_base_properties() links = _get_base_links() #根据配置文件创建Schema,allow_additional_image_properties默认True, #区别是PermissiveSchema多设置了links参数 if CONF.allow_additional_image_properties: schema = glance.schema.PermissiveSchema('image', properties, links) else: schema = glance.schema.Schema('image', properties) #如果是自己定制的property,需要做合并操作 if custom_properties: for property_value in custom_properties.values(): property_value['is_base'] = False schema.merge_properties(custom_properties) return schema 最后看下Schema对象包含什么内容 12345678910111213141516171819202122232425262728293031323334353637383940class Schema(object): def __init__(self, name, properties=None, links=None, required=None, definitions=None): self.name = name if properties is None: properties = &#123;&#125; self.properties = properties self.links = links self.required = required self.definitions = definitions ......................... #该方法返回一个dick def raw(self): raw = &#123; 'name': self.name, 'properties': self.properties, 'additionalProperties': False, &#125; if self.definitions: raw['definitions'] = self.definitions if self.required: raw['required'] = self.required if self.links: raw['links'] = self.links return raw #Schema的子类 ，分析同上class PermissiveSchema(Schema): @staticmethod def _filter_func(properties, key): return True def raw(self): raw = super(PermissiveSchema, self).raw() raw['additionalProperties'] = &#123;'type': 'string'&#125; return raw def minimal(self): minimal = super(PermissiveSchema, self).raw() return minimal 第一个请求的分析结束，所以该请求的目的就是获取镜像所支持的属性字典定义，接下来就该根据这些信息来验证用户输入参数。 看下第二条请求，路由到glance/api/v2/images.py.ImagesController.create方法： 1234567891011121314151617181920212223242526272829303132@utils.mutating def create(self, req, image, extra_properties, tags): #责任链的模式， #分别创建image_factory,image_repo对象 image_factory = self.gateway.get_image_factory(req.context) image_repo = self.gateway.get_repo(req.context) try: image = image_factory.new_image(extra_properties=extra_properties, tags=tags, **image) image_repo.add(image) except (exception.DuplicateLocation, exception.Invalid) as e: raise webob.exc.HTTPBadRequest(explanation=e.msg) except (exception.ReservedProperty, exception.ReadonlyProperty) as e: raise webob.exc.HTTPForbidden(explanation=e.msg) except exception.Forbidden as e: LOG.debug("User not permitted to create image") raise webob.exc.HTTPForbidden(explanation=e.msg) except exception.LimitExceeded as e: LOG.warn(encodeutils.exception_to_unicode(e)) raise webob.exc.HTTPRequestEntityTooLarge( explanation=e.msg, request=req, content_type='text/plain') except exception.Duplicate as e: raise webob.exc.HTTPConflict(explanation=e.msg) except exception.NotAuthenticated as e: raise webob.exc.HTTPUnauthorized(explanation=e.msg) except TypeError as e: LOG.debug(encodeutils.exception_to_unicode(e)) raise webob.exc.HTTPBadRequest(explanation=e) #返回一个 image对象 return image 关于责任链模式，check this 先看下gateway.get_image_factory方法： 1234567891011121314151617181920212223242526272829303132class Gateway(object): def __init__(self, db_api=None, store_api=None, notifier=None, policy_enforcer=None): self.db_api = db_api or glance.db.get_api() self.store_api = store_api or glance_store self.store_utils = store_utils self.notifier = notifier or glance.notifier.Notifier() self.policy = policy_enforcer or policy.Enforcer() #这里可以清晰地看出是责任链模式， def get_image_factory(self, context): image_factory = glance.domain.ImageFactory() #对location的检验 store_image_factory = glance.location.ImageFactoryProxy( image_factory, context, self.store_api, self.store_utils) #对quota的检验 quota_image_factory = glance.quota.ImageFactoryProxy( store_image_factory, context, self.db_api, self.store_utils) #policy检验 policy_image_factory = policy.ImageFactoryProxy( quota_image_factory, context, self.policy) notifier_image_factory = glance.notifier.ImageFactoryProxy( policy_image_factory, context, self.notifier) if property_utils.is_property_protection_enabled(): property_rules = property_utils.PropertyRules(self.policy) pif = property_protections.ProtectedImageFactoryProxy( notifier_image_factory, context, property_rules) authorized_image_factory = authorization.ImageFactoryProxy( pif, context) else: authorized_image_factory = authorization.ImageFactoryProxy( notifier_image_factory, context) return authorized_image_factory 可以看出ImageFactoryProxy类都继承自glance/domain/proxy.py.ImageFactory，通过类名可以大致猜出其功能：镜像工厂，那就是用来创建封装镜像对象的；各个子类也分别实现:权限检查、消息通知、策略检查、配额检查等。另外各个ImageFactoryProxy类都依赖于ImageProxy类。而各ImageProxy类都继承自glance/domain/proxy.py.Image，该类描述的是镜像的属性信息，包括：name，image_id, status等。各*ImageProxy类是对Image的扩展。 关于这个责任链的代码一直没搞清楚，感兴趣的话，看这篇文章吧，Openstack liberty Glance上传镜像源码分析,以后有机会再去解读。 按照上述给出的那篇参考文章的解释，new_image()方法和add()方法，类似一个封包，一个解包的过程，主要完成权限检查，配额检查，策略检查，发布通知以及记录数据库等操作。下面来看看镜像文件的上传过程。 第三条请求，路由到glance/api/v2/image_data.py/ImageDataController.upload方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@utils.mutating def upload(self, req, image_id, data, size): image_repo = self.gateway.get_repo(req.context) image = None refresher = None cxt = req.context try: #get方法与前述的add方法类似，首先从数据库取出image_id指向的条目， #封装成`domain/__init__.py/Image`对象， #然后经过层层封装返回 `authorization/ImageProxy`对象 image = image_repo.get(image_id) #更新镜像状态为saving - ‘保存中’ image.status = 'saving' try: if CONF.data_api == 'glance.db.registry.api': # create a trust if backend is registry try: # request user plugin for current token user_plugin = req.environ.get('keystone.token_auth') roles = [] # use roles from request environment because they # are not transformed to lower-case unlike cxt.roles for role_info in req.environ.get( 'keystone.token_info')['token']['roles']: roles.append(role_info['name']) refresher = trust_auth.TokenRefresher(user_plugin, cxt.tenant, roles) except Exception as e: LOG.info(_LI("Unable to create trust: %s " "Use the existing user token."), encodeutils.exception_to_unicode(e)) #和上面的save方法相似的处理方式，逐层调用`ImageProxy`的set_data， #在该过程中会检查用户配额，发送通知， #最后根据glance-api.conf文件中配置存储后端上传镜像文件（通过add方法）到指定地方存储 image_repo.save(image, from_state='queued') #镜像上传成功后（在`location.py/set_data方法中上传文件成功后， #修改状态为active），更新数据库状态为active（这个时候可以在 #Dashboard上看到状态为'运行中'） image.set_data(data, size) try: image_repo.save(image, from_state='saving') except exception.NotAuthenticated: if refresher is not None: # request a new token to update an image in database cxt.auth_token = refresher.refresh_token() image_repo = self.gateway.get_repo(req.context) image_repo.save(image, from_state='saving') else: raise ......................................... 主体代码也是责任链的模式，太蛋疼了，看不明白。 参考文章openstack 设计与实现 Openstack liberty Glance上传镜像源码分析 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--glanceApi 源代码解读(二)]]></title>
    <url>%2F2016%2F12%2F15%2Fglance-api-sourcecode-explain-2%2F</url>
    <content type="text"><![CDATA[目录结构：glanceApi V1 代码解读 glanceApi V1 代码解读 首先简单看下V1 代码与V2 代码的目录结构： 也可以明显看出V2相对V1加了许多API。 下面以glance image-create 为例解读下glanceAPI V1 的代码，glanceClient发出rest请求后，glance-api Server监听到对应端口的请求，根据api-paste文件经过middleware的过滤后，根据配置文件中选定的版本，最终路由到v1/images.py文件中的对应函数，先看下WSGI 路由文件router.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class API(wsgi.Router): """WSGI router for Glance v1 API requests.""" def __init__(self, mapper): reject_method_resource = wsgi.Resource(wsgi.RejectMethodController()) images_resource = images.create_resource() mapper.connect("/", controller=images_resource, action="index") mapper.connect("/images", controller=images_resource, action='index', conditions=&#123;'method': ['GET']&#125;) mapper.connect("/images", controller=images_resource, action='create', conditions=&#123;'method': ['POST']&#125;) mapper.connect("/images", controller=reject_method_resource, action='reject', allowed_methods='GET, POST') mapper.connect("/images/detail", controller=images_resource, action='detail', conditions=&#123;'method': ['GET', 'HEAD']&#125;) mapper.connect("/images/detail", controller=reject_method_resource, action='reject', allowed_methods='GET, HEAD') mapper.connect("/images/&#123;id&#125;", controller=images_resource, action="meta", conditions=dict(method=["HEAD"])) mapper.connect("/images/&#123;id&#125;", ............................... members_resource = members.create_resource() mapper.connect("/images/&#123;image_id&#125;/members", controller=members_resource, action="index", conditions=&#123;'method': ['GET']&#125;) mapper.connect("/images/&#123;image_id&#125;/members", controller=members_resource, action="update_all", conditions=dict(method=["PUT"])) mapper.connect("/images/&#123;image_id&#125;/members", controller=reject_method_resource, action='reject', allowed_methods='GET, PUT') mapper.connect("/images/&#123;image_id&#125;/members/&#123;id&#125;", controller=members_resource, action="show", conditions=&#123;'method': ['GET']&#125;) .................................. 主要有两部分，一部分是镜像image的操作，一部分是members的操作。POST /images 请求最终路由到create函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@utils.mutating def create(self, req, image_meta, image_data): """ Adds a new image to Glance. Four scenarios exist when creating an image: 1. If the image data is available directly for upload, create can be passed the image data as the request body and the metadata as the request headers. The image will initially be 'queued', during upload it will be in the 'saving' status, and then 'killed' or 'active' depending on whether the upload completed successfully. 2. If the image data exists somewhere else, you can upload indirectly from the external source using the x-glance-api-copy-from header. Once the image is uploaded, the external store is not subsequently consulted, i.e. the image content is served out from the configured glance image store. State transitions are as for option #1. 3. If the image data exists somewhere else, you can reference the source using the x-image-meta-location header. The image content will be served out from the external store, i.e. is never uploaded to the configured glance image store. 4. If the image data is not available yet, but you'd like reserve a spot for it, you can omit the data and a record will be created in the 'queued' state. This exists primarily to maintain backwards compatibility with OpenStack/Rackspace API semantics. The request body *must* be encoded as application/octet-stream, otherwise an HTTPBadRequest is returned. Upon a successful save of the image data and metadata, a response containing metadata about the image is returned, including its opaque identifier. :param req: The WSGI/Webob Request object :param image_meta: Mapping of metadata about image :param image_data: Actual image data that is to be stored :raises: HTTPBadRequest if x-image-meta-location is missing and the request body is not application/octet-stream image data. """ #上文注释里面提到create image时出现的四种情况.不赘述 #查看policy是否允许‘add_image’操作,下同 self._enforce(req, 'add_image') is_public = image_meta.get('is_public') if is_public: self._enforce(req, 'publicize_image') if Controller._copy_from(req): self._enforce(req, 'copy_from') if image_data or Controller._copy_from(req): self._enforce(req, 'upload_image') #查看property是否合法 self._enforce_create_protected_props(image_meta['properties'].keys(), req) #验证property的个数是否超过配置文件规定项目的配额quota self._enforce_image_property_quota(image_meta, req=req) #将image_meta传给glance-registry,调用glance-registrycli image_meta = self._reserve(req, image_meta) id = image_meta['id'] #上传镜像数据操作。 image_meta = self._handle_source(req, id, image_meta, image_data) location_uri = image_meta.get('location') if location_uri: self.update_store_acls(req, id, location_uri, public=is_public) # Prevent client from learning the location, as it # could contain security credentials image_meta = redact_loc(image_meta) return &#123;'image_meta': image_meta&#125; 先看下metadata 的处理过程_reserve方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374def _reserve(self, req, image_meta): """ Adds the image metadata to the registry and assigns an image identifier if one is not supplied in the request headers. Sets the image's status to `queued`. :param req: The WSGI/Webob Request object :param id: The opaque image identifier :param image_meta: The image metadata :raises: HTTPConflict if image already exists :raises: HTTPBadRequest if image metadata is not valid """ #注释里说得很清楚了，给rigistry增加一个image metedata,如果没有id的话 #给他绑定一个id,并将该image status 设置为‘queued’ location = self._external_source(image_meta, req) scheme = image_meta.get('store') if scheme and scheme not in store.get_known_schemes(): msg = _("Required store %s is invalid") % scheme LOG.warn(msg) raise HTTPBadRequest(explanation=msg, content_type='text/plain') image_meta['status'] = ('active' if image_meta.get('size') == 0 else 'queued') #如果不是直接上床，路径为外部路径，那么先对外部路径validate以及获取image size if location: try: backend = store.get_store_from_location(location) except (store.UnknownScheme, store.BadStoreUri): LOG.debug("Invalid location %s", location) msg = _("Invalid location %s") % location raise HTTPBadRequest(explanation=msg, request=req, content_type="text/plain") # check the store exists before we hit the registry, but we # don't actually care what it is at this point self.get_store_or_400(req, backend) # retrieve the image size from remote store (if not provided) image_meta['size'] = self._get_size(req.context, image_meta, location) #如果是本地直接上传，那么将size设置为0，上传的时候会再设置 else: # Ensure that the size attribute is set to zero for directly # uploadable images (if not provided). The size will be set # to a non-zero value during upload image_meta['size'] = image_meta.get('size', 0) try: #调用registry.client，向glance-registry发送rest请求 image_meta = registry.add_image_metadata(req.context, image_meta) self.notifier.info("image.create", redact_loc(image_meta)) return image_meta except exception.Duplicate: msg = (_("An image with identifier %s already exists") % image_meta['id']) LOG.warn(msg) raise HTTPConflict(explanation=msg, request=req, content_type="text/plain") except exception.Invalid as e: msg = (_("Failed to reserve image. Got error: %s") % encodeutils.exception_to_unicode(e)) LOG.exception(msg) raise HTTPBadRequest(explanation=msg, request=req, content_type="text/plain") except exception.Forbidden: msg = _("Forbidden to reserve image.") LOG.warn(msg) raise HTTPForbidden(explanation=msg, request=req, content_type="text/plain") 进入registry.client，看下相关代码： 12345def add_image_metadata(context, image_meta): LOG.debug("Adding image metadata...") #获取registry_client ,code explain veerything c = get_registry_client(context) return c.add_image(image_meta) 看下最终的add_image()方法： 1234567891011121314151617181920def add_image(self, image_metadata): """ Tells registry about an image's metadata """ headers = &#123; 'Content-Type': 'application/json', &#125; if 'image' not in image_metadata: image_metadata = dict(image=image_metadata) encrypted_metadata = self.encrypt_metadata(image_metadata['image']) image_metadata['image'] = encrypted_metadata body = jsonutils.dump_as_bytes(image_metadata) #向registry发送POST请求。返回一个类似JSON形式 的dict res = self.do_request("POST", "/images", body=body, headers=headers) # Registry returns a JSONified dict(image=image_info) data = jsonutils.loads(res.read()) image = data['image'] return self.decrypt_metadata(image) 再往下就是glance-registry 接收到rest请求后对数据的操作。 不像nova-conductor通过Object Model进行db访问，glance-registry相对比较简单，直接通过glance db模块进行访问，就不再另写一篇文章介绍glance-registry,直接看下经过路由后的执行函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@utils.mutating def create(self, req, body): """Registers a new image with the registry. :param req: wsgi Request object :param body: Dictionary of information about the image :returns: The newly-created image information as a mapping, which will include the newly-created image's internal id in the 'id' field """ image_data = body['image'] # Ensure the image has a status set image_data.setdefault('status', 'active') # Set up the image owner if not req.context.is_admin or 'owner' not in image_data: image_data['owner'] = req.context.owner image_id = image_data.get('id') if image_id and not uuidutils.is_uuid_like(image_id): LOG.info(_LI("Rejecting image creation request for invalid image " "id '%(bad_id)s'"), &#123;'bad_id': image_id&#125;) msg = _("Invalid image id format") return exc.HTTPBadRequest(explanation=msg) if 'location' in image_data: image_data['locations'] = [image_data.pop('location')] try: image_data = _normalize_image_location_for_db(image_data) #调用glance db 模块进行数据库操作， #glance db 模块是对sqlalchemy 的封装 image_data = self.db_api.image_create(req.context, image_data) image_data = dict(image=make_image_dict(image_data)) LOG.info(_LI("Successfully created image %(id)s"), &#123;'id': image_data['image']['id']&#125;) return image_data except exception.Duplicate: msg = _("Image with identifier %s already exists!") % image_id LOG.warn(msg) return exc.HTTPConflict(msg) except exception.Invalid as e: msg = (_("Failed to add image metadata. " "Got error: %s") % encodeutils.exception_to_unicode(e)) LOG.error(msg) return exc.HTTPBadRequest(msg) except Exception: LOG.exception(_LE("Unable to create image %s"), image_id) raise OK,对metedata的分析到这，再回去看下对image chunk data的处理,先是方法_handle_source： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def _handle_source(self, req, image_id, image_meta, image_data): copy_from = self._copy_from(req) location = image_meta.get('location') sources = [obj for obj in (copy_from, location, image_data) if obj] if len(sources) &gt;= 2: msg = _("It's invalid to provide multiple image sources.") LOG.warn(msg) raise HTTPBadRequest(explanation=msg, request=req, content_type="text/plain") if len(sources) == 0: return image_meta if image_data: #第一种情况 image_meta = self._validate_image_for_activation(req, image_id, image_meta) image_meta = self._upload_and_activate(req, image_meta) elif copy_from: #第二种情况，异步上传 msg = _LI('Triggering asynchronous copy from external source') LOG.info(msg) pool = common.get_thread_pool("copy_from_eventlet_pool") pool.spawn_n(self._upload_and_activate, req, image_meta) else: #第三种情况，不上传，修改metadata location,以及status if location: self._validate_image_for_activation(req, image_id, image_meta) image_size_meta = image_meta.get('size') if image_size_meta: try: image_size_store = store.get_size_from_backend( location, req.context) except (store.BadStoreUri, store.UnknownScheme) as e: LOG.debug(encodeutils.exception_to_unicode(e)) raise HTTPBadRequest(explanation=e.msg, request=req, content_type="text/plain") # NOTE(zhiyan): A returned size of zero usually means # the driver encountered an error. In this case the # size provided by the client will be used as-is. if (image_size_store and image_size_store != image_size_meta): msg = (_("Provided image size must match the stored" " image size. (provided size: %(ps)d, " "stored size: %(ss)d)") % &#123;"ps": image_size_meta, "ss": image_size_store&#125;) LOG.warn(msg) raise HTTPConflict(explanation=msg, request=req, content_type="text/plain") location_data = &#123;'url': location, 'metadata': &#123;&#125;, 'status': 'active'&#125; image_meta = self._activate(req, image_id, location_data) return image_meta 看上面这段代码其实就是之前描述的四种情况： 1. 如果是本地文件，直接上传，image status 为queued,上传成功，queued 更改为active。 2. 如果是外部文件（如在swift或ceph等），如果使用这个header:x-glance-api-copy-from,那么会异步上传，status同1. 3. 如果是外部文件，使用的header是x-image-meta-location，那么就不用上传了，但是image metedata 中的location 要更改为对应的外部地址。 4. image文件是not available的话，不上传，但image metedata 会有，image status 为queued。 参考文章openstack 设计与实现 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--glanceApi 源代码解读(一)]]></title>
    <url>%2F2016%2F12%2F15%2Fglance-api-sourcecode-explain-1%2F</url>
    <content type="text"><![CDATA[目录结构：glanceApi 介绍 glanceApi 启动过程代码解读 glanceApi 介绍 glanceApi主要作用就是启动一个WSGI Server,接受restful请求，路由给对应V1/V2版本的不同执行函数。 因为版本不同，代码也可能不同，该版本为Openstack Liberty glanceApi 启动过程代码解读 首先看下glanceAPI 启动代码入口： 1234567891011121314151617181920212223242526272829def main(): try: #找到配置文件并读取配置文件中的值到config.CONF中 config.parse_args() #设置默认值 config.set_config_defaults() #设置协程调度使用epoll,若不支持，使用selects wsgi.set_eventlet_hub() logging.setup(CONF, 'glance') notifier.set_defaults() #osprofiler的使用与Ceilometer性能分析有关，此处略过 if cfg.CONF.profiler.enabled: _notifier = osprofiler.notifier.create("Messaging", oslo_messaging, &#123;&#125;, notifier.get_transport(), "glance", "api", cfg.CONF.bind_host) osprofiler.notifier.set(_notifier) osprofiler.web.enable(cfg.CONF.profiler.hmac_keys) else: osprofiler.web.disable() #初始化WSGI Server server = wsgi.Server(initialize_glance_store=True) #先加载paste文件，生成app,然后启动WSGI Serve server.start(config.load_paste_app('glance-api'), default_port=9292) #循环等待请求 server.wait() except KNOWN_EXCEPTIONS as e: fail(e) 看下WSGI Server初始化的过程： 123456789101112131415161718192021222324class Server(object): """Server class to manage multiple WSGI sockets and applications. This class requires initialize_glance_store set to True if glance store needs to be initialized. """ def __init__(self, threads=1000, initialize_glance_store=False): os.umask(0o27) # ensure files are created with the correct privileges self._logger = logging.getLogger("eventlet.wsgi.server") self.threads = threads self.children = set() self.stale_children = set() self.running = True # NOTE(abhishek): Allows us to only re-initialize glance_store when # the API's configuration reloads. #注意此处的参数会在start时用于初始化glance的后端存储 self.initialize_glance_store = initialize_glance_store self.pgid = os.getpid() try: # NOTE(flaper87): Make sure this process # runs in its own process group. os.setpgid(self.pgid, self.pgid) except OSError: self.pgid = 0 启动Server之前会加载paste文件，看下代码： 123456789101112131415161718192021222324252627282930def load_paste_app(app_name, flavor=None, conf_file=None): """ Builds and returns a WSGI app from a paste config file. """ # append the deployment flavor to the application name, # in order to identify the appropriate paste pipeline app_name += _get_deployment_flavor(flavor) if not conf_file: conf_file = _get_deployment_config_file() try: logger = logging.getLogger(__name__) logger.debug("Loading %(app_name)s from %(conf_file)s", &#123;'conf_file': conf_file, 'app_name': app_name&#125;) app = deploy.loadapp("config:%s" % conf_file, name=app_name) # Log the options used when starting if we're in debug mode... if CONF.debug: CONF.log_opt_values(logger, logging.DEBUG) return app except (LookupError, ImportError) as e: msg = (_("Unable to load %(app_name)s from " "configuration file %(conf_file)s." "\nGot: %(e)r") % &#123;'app_name': app_name, 'conf_file': conf_file, 'e': e&#125;) logger.error(msg) raise RuntimeError(msg) 不解释了，code explain everything. Server start 的过程： 123456789101112131415161718192021222324252627282930313233343536373839def start(self, application, default_port): """ Run a WSGI server with the given application. :param application: The application to be run in the WSGI server :param default_port: Port to bind to if none is specified in conf """ self.application = application self.default_port = default_port self.configure() self.start_wsgi()def configure(self, old_conf=None, has_changed=None): """ Apply configuration settings :param old_conf: Cached old configuration settings (if any) :param has changed: callable to determine if a parameter has changed """ eventlet.wsgi.MAX_HEADER_LINE = CONF.max_header_line self.client_socket_timeout = CONF.client_socket_timeout or None self.configure_socket(old_conf, has_changed) #初始化后端存储 if self.initialize_glance_store: initialize_glance_store()def start_wsgi(self): #根据配置文件指定worker数创建Server 进程 workers = get_num_workers() if workers == 0: # Useful for profiling, test, debug etc. self.pool = self.create_pool() self.pool.spawn_n(self._single_run, self.application, self.sock) return else: LOG.info(_LI("Starting %d workers"), workers) signal.signal(signal.SIGTERM, self.kill_children) signal.signal(signal.SIGINT, self.kill_children) signal.signal(signal.SIGHUP, self.hup) while len(self.children) &lt; workers: self.run_child() 再看下初始化后端存储部分，initialize_glance_store方法：12345def initialize_glance_store(): """Initialize glance store.""" glance_store.register_opts(CONF) glance_store.create_stores(CONF) glance_store.verify_default_store() 执行了三个glance_store导入函数，实现根据配置文件的指定，加载相应的后端存储。这里提一下glance-store项目，这个项目之前跟glance是在一块的，J版本之后就独立出来。看了一下github上的README,说该库提供的接口不是特别的稳定，有一些缺陷，可能最终会重写这部分，所以，就不详细展开了。 参考文章openstack 设计与实现 glance-api 服务启动流程(官方Kilo版本) glance-store 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--glance 源代码整体解读]]></title>
    <url>%2F2016%2F12%2F14%2Fglance-sourcecode-explain%2F</url>
    <content type="text"><![CDATA[目录结构：glance 介绍 glance API 详解 glance 整体代码解读 glance 介绍 openstack glance项目提供restful接口实现了虚拟机镜像的发现，注册，检索等功能。它本身不负责镜像的存储，提供driver接口，依赖于swift等其他项目完成镜像的存储。 因为版本不同，代码也可能不同，该版本为Openstack Liberty glance API 详解 glance api 的版本目前如下： v1版本逐渐弃用，目前v2版本用的比较多，v3版本正在开发中。v1与v2版的区别还是比较大的，不只是添加了很多API。v1版本中glance-api与glance-registry都是一个WSGI server,glance-api负责接收用户的restful请求，如果该请求是与metedata相关的，则将其转发给glance-registry，glance-registry会解析请求并与数据库交互，如果该请求是与image自身存取相关的，则直接转发给store backend。V2版本中，glance-registry服务的内容被整合进了glance-api中，采用责任链的模式实现API的处理流程。在下面结合实例分析的时候会更清晰。以下示例图来自网上，正确性不保证： 关于API 的详细介绍在这个页面check this glance 整体代码解读 看下setup.cfg中的启动脚本内容： 1234567891011121314[entry_points]console_scripts = glance-api = glance.cmd.api:main glance-cache-prefetcher = glance.cmd.cache_prefetcher:main glance-cache-pruner = glance.cmd.cache_pruner:main glance-cache-manage = glance.cmd.cache_manage:main glance-cache-cleaner = glance.cmd.cache_cleaner:main glance-control = glance.cmd.control:main glance-manage = glance.cmd.manage:main glance-registry = glance.cmd.registry:main glance-replicator = glance.cmd.replicator:main glance-scrubber = glance.cmd.scrubber:main glance-glare = glance.cmd.glare:main .................. 1，glance-cache-* :四个对image cache进行管理的工具，pruner用于执行一些周期性任务，cleaner用于清理cache文件释放空间。 2，glance-manage :执行对glance数据库的操作。 3，glance-replicator:实现镜像的复制功能 4，glance-scrubber:清理已经删除的image 5, glance-control:glance 提供了glance-api,glance-registry 两个WSGI Server,以及一个glance-scrubber后台服务进程，这里的glance-control 就是用于控制这三个服务进程（start,stop,restart）。 其实这里glance-api在初始时，还会导入一个叫做glance-store的项目，以便初始化后台的存储系统，glance通过glance-store这个框架所提供的接口，实现了对各种不同存储系统的支持。 glance的主要操作对象是image，image的metedata存储在数据库，真实的image数据存储在后端存储系统（如swift,ceph）。 image的主要metedata有：id,name,owner,size,location,disk_format,container_format(一般设置为bare),status。 image的访问权限： public 公共的：可以被所有的 tenant 使用。 private 私有的/项目的：只能被 image owner 所在的 tenant 使用。 shared 共享的：一个非共有的image 可以 共享给另外的 tenant，可通过member-* 操作来实现。 protected 受保护的：protected 的 image 不能被删除。 image的status变更： glance 中的task概念：在V2版本的API中提出了task的概念，关于为什么提出task的概念以可看这篇博客Getting started with Tasks API in Glance。 总结一下就是： 要给用户一个上传/下载镜像等操作的接口，但是目前的接口只是单纯地实现了功能，过程是不可控的，要是用户上传的不是镜像文件怎么办，甚至是恶意文件怎么办，执行过程中出现错误怎么办（没有反馈机制，用户是不可见的），本身镜像文件是非常大的，上传/下载占用太多时间怎么办。 因此针对这种耗时较长且不可控的操作，抽象出了task的概念。task是针对image的异步操作，本身具有一些属性如id,owner，status等。 task的status大致有：pending(task被创建，但未执行)，processing(task执行中),success(task成功结束),failure(执行失败)。 目前task支持的操作包括：import（用户上传镜像）,export（用户下载镜像），clone（region之间clone,注：紧紧跟随AWS啊）。具体的task api 可以看之前的参考页面。 参考文章openstack 设计与实现 openstack glance github Image Service API Getting started with Tasks API in Glance wiki–glance-task-api 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--novaCompute 源代码解读]]></title>
    <url>%2F2016%2F12%2F09%2FnovaComputer-sourcecode-explain%2F</url>
    <content type="text"><![CDATA[目录结构：novacompute 介绍 以nova boot 为例解读代码 novacompute 整体讲解 novacompute 介绍 novacomputer 完成的主要任务是虚拟机的生命周期管理。 目前支持的虚拟化包括libvirt (KVM, Xen, LXC and more), Hyper-V, VMware, XenServer 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova boot 为例解读代码 首先我们先把之前解读novaconductor时发送rpc消息给novacompute的代码拿过来nova/conductor/manager.py.ComputeTaskManager.build_instances： 1234567891011#发送异步消息给nova-compute，完成instance的bootself.compute_rpcapi.build_and_run_instance(context, instance=instance, host=host['host'], image=image, request_spec=request_spec, filter_properties=local_filter_props, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=bdms, node=host['nodename'], limits=host['limits']) 可以看到novaconductor 引入novacomputer中的rpcapi.py文件，调用ComputeAPI.build_and_run_instance方法，看下该方法： 12345678910111213141516171819def build_and_run_instance(self, ctxt, instance, host, image, request_spec, filter_properties, admin_password=None, injected_files=None, requested_networks=None, security_groups=None, block_device_mapping=None, node=None, limits=None): version = '4.0' #创建一个指定host的RPCClient cctxt = self.router.by_host(ctxt, host).prepare( server=host, version=version) #发送异步rpc请求 cctxt.cast(ctxt, 'build_and_run_instance', instance=instance, image=image, request_spec=request_spec, filter_properties=filter_properties, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=block_device_mapping, node=node, limits=limits) 由之前分析可知，rpc的请求被服务端接收后，真正执行函数的地方在manager.py这个文件里，找到执行函数： 12345678910111213141516171819202122232425262728@wrap_exception() #捕获所有exception的装饰器@reverts_task_state #一旦build instance failure 会还原 task-state@wrap_instance_fault #捕获所有instance相关的异常def build_and_run_instance(self, context, instance, image, request_spec, filter_properties, admin_password=None, injected_files=None, requested_networks=None, security_groups=None, block_device_mapping=None, node=None, limits=None): #保证某一instance的任务是同步执行的 @utils.synchronized(instance.uuid) def _locked_do_build_and_run_instance(*args, **kwargs): # NOTE(danms): We grab the semaphore with the instance uuid # locked because we could wait in line to build this instance # for a while and we want to make sure that nothing else tries # to do anything with this instance while we wait. with self._build_semaphore: #最终调用该函数 self._do_build_and_run_instance(*args, **kwargs) # NOTE(danms): We spawn here to return the RPC worker thread back to # the pool. Since what follows could take a really long time, we don't # want to tie up RPC workers. #利用eventlet新建一个线程执行_locked_do_build_and_run_instance函数 utils.spawn_n(_locked_do_build_and_run_instance, context, instance, image, request_spec, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping, node, limits) 看下最后执行的_do_build_and_run_instance函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@hooks.add_hook('build_instance')@wrap_exception()@reverts_task_state@wrap_instance_event(prefix='compute')@wrap_instance_faultdef _do_build_and_run_instance(self, context, instance, image, request_spec, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping, node=None, limits=None): try: LOG.debug('Starting instance...', instance=instance) instance.vm_state = vm_states.BUILDING instance.task_state = None #更新task_stat instance.save(expected_task_state= (task_states.SCHEDULING, None)) except exception.InstanceNotFound: msg = 'Instance disappeared before build.' LOG.debug(msg, instance=instance) return build_results.FAILED except exception.UnexpectedTaskStateError as e: LOG.debug(e.format_message(), instance=instance) return build_results.FAILED # b64 decode the files to inject: decoded_files = self._decode_files(injected_files) if limits is None: limits = &#123;&#125; if node is None: #由具体实现的driver（如libvirtDriver）获取nodename,因为hypervisor不一样，有的hypervisor其实就一个node(如libvirt),而有的则是多个node. node = self.driver.get_available_nodes(refresh=True)[0] LOG.debug('No node specified, defaulting to %s', node, instance=instance) try: #设定一个时间检测，调用_build_and_run_instance with timeutils.StopWatch() as timer: self._build_and_run_instance(context, instance, image, decoded_files, admin_password, requested_networks, security_groups, block_device_mapping, node, limits, filter_properties) LOG.info(_LI('Took %0.2f seconds to build instance.'), timer.elapsed(), instance=instance) return build_results.ACTIVE except exception.RescheduledException as e: retry = filter_properties.get('retry') if not retry: # no retry information, do not reschedule. LOG.debug("Retry info not present, will not reschedule", instance=instance) self._cleanup_allocated_networks(context, instance, requested_networks) compute_utils.add_instance_fault_from_exc(context, instance, e, sys.exc_info(), fault_message=e.kwargs['reason']) self._nil_ou t_instance_obj_host_and_node(instance) self._set_instance_obj_error_state(context, instance, clean_task_state=True) return build_results.FAILED .................................... 接着往下看self._build_and_run_instance函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def _build_and_run_instance(self, context, instance, image, injected_files, admin_password, requested_networks, security_groups, block_device_mapping, node, limits, filter_properties): #image是一个包含镜像信息的字典，‘name’是镜像的名字 image_name = image.get('name') self._notify_about_instance_usage(context, instance, 'create.start', extra_usage_info=&#123;'image_name': image_name&#125;) self._check_device_tagging(requested_networks, block_device_mapping) try: # #获取/创建ResourceTracker实例，为后续的资源申请做准备（此处涉及的resource Tracker Claim机制后面后详细讲） rt = self._get_resource_tracker(node) with rt.instance_claim(context, instance, limits): # NOTE(russellb) It's important that this validation be done # *after* the resource tracker instance claim, as that is where # the host is set on the instance. #group_policy包含两种affinity 和 anti-affinity，在创建server_group的时候指定。 #创建虚拟机时指定server_group,会根据group_policy倾向于创建在同一台host或不同host self._validate_instance_group_policy(context, instance, filter_properties) image_meta = objects.ImageMeta.from_dict(image) #为云主机申请网络资源，完成块设备验证及映射 with self._build_resources(context, instance, requested_networks, security_groups, image_meta, block_device_mapping) as resources: instance.vm_state = vm_states.BUILDING instance.task_state = task_states.SPAWNING # NOTE(JoshNang) This also saves the changes to the # instance from _allocate_network_async, as they aren't # saved in that function to prevent races. #更新实例状态 instance.save(expected_task_state= task_states.BLOCK_DEVICE_MAPPING) block_device_info = resources['block_device_info'] network_info = resources['network_info'] LOG.debug('Start spawning the instance on the hypervisor.', instance=instance) with timeutils.StopWatch() as timer: #调用hypervisor的spawn方法启动instance，如果使用libvirt的话，会调用nova/virt/libvirt/driver.py/libvirtDriver.spawn self.driver.spawn(context, instance, image_meta, injected_files, admin_password, network_info=network_info, block_device_info=block_device_info) LOG.info(_LI('Took %0.2f seconds to spawn the instance on ' 'the hypervisor.'), timer.elapsed(), instance=instance) except (exception.InstanceNotFound, exception.UnexpectedDeletingTaskStateError) as e: 看下nova/virt/libvirt/driver.py/libvirtDriver.spawn 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def spawn(self, context, instance, image_meta, injected_files, admin_password, network_info=None, block_device_info=None): #根据模拟器类型，获取块设备及光驱的总线类型 #默认使用kvm，所以：块设备默认使用virtio；光驱默认使用ide；并且根据block_device_info设置设备映射 #最后返回包含&#123;disk_bus,cdrom_bus,mapping&#125;的字典 disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type, instance, image_meta, block_device_info) gen_confdrive = functools.partial(self._create_configdrive, context, instance, admin_pass=admin_password, files=injected_files, network_info=network_info) #从glance下载镜像（如果本地_base目录没有的话），然后上传到后端存储 self._create_image(context, instance, disk_info['mapping'], network_info=network_info, block_device_info=block_device_info, files=injected_files, admin_pass=admin_password) # Required by Quobyte CI self._ensure_console_log_for_instance(instance) #生成libvirt xml文件（libvirt创建虚拟机使用） xml = self._get_guest_xml(context, instance, network_info, disk_info, image_meta, block_device_info=block_device_info) #调用libvirt启动实例 self._create_domain_and_network( context, xml, instance, network_info, disk_info, block_device_info=block_device_info, post_xml_callback=gen_confdrive) LOG.debug("Instance is running", instance=instance) def _wait_for_boot(): """Called at an interval until the VM is running.""" state = self.get_info(instance).state if state == power_state.RUNNING: LOG.info(_LI("Instance spawned successfully."), instance=instance) raise loopingcall.LoopingCallDone() #等待实例创建结果（通过libvirt获取云主机状态判断） timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot) timer.start(interval=0.5).wait() 关于具体创建系统磁盘的代码_create_image，生成libvirt xml，调用libvirt启动实例的代码不再分析，可以参考这篇文章check this。 novacompute 整体讲解 novacomputer 代码目录如下： Resource Tracker:nova-computer 会维护数据库中一个computer_nodes的表用来存储主机的资源使用情况，以便nova-scheduler获取作为选择主机的依据，这就要求每次创建，迁移，删除虚拟机时都要对数据库中的的computer_node进行更新。nova-computer会为每一个主机创建一个ResourceTracker对象，更新computernode对象。目前是有两种更新机制：一种是ResourceTracker的claim机制，一种是使用周期性任务（Periodic Task） Claim机制：当一台主机被多个nova-scheduler同时选中并发出创建虚拟机的请求时，该主机并不一定有足够的资源满足创建要求。所以Claim机制就是在创建虚拟机之前先测试下是否满足新建需求，满足，则更新数据库，并将虚拟机申请的资源从主机可用的资源减掉，如果后来创建失败，或者虚拟机删除时，会通过claim加上之前减掉的部分。 @utils.synchronized(COMPUTE_RESOURCE_SEMAPHORE) def instance_claim(self, context, instance, limits=None): if self.disabled: # compute_driver doesn't support resource tracking, just # set the 'host' and node fields and continue the build: self._set_instance_host_and_node(instance) return claims.NopClaim() # sanity checks: if instance.host: LOG.warning(_LW("Host field should not be set on the instance " "until resources have been claimed."), instance=instance) if instance.node: LOG.warning(_LW("Node field should not be set on the instance " "until resources have been claimed."), instance=instance) #允许超配创建虚拟机 # get the overhead required to build this instance: overhead = self.driver.estimate_instance_overhead(instance) LOG.debug("Memory overhead for %(flavor)d MB instance; %(overhead)d " "MB", {'flavor': instance.flavor.memory_mb, 'overhead': overhead['memory_mb']}) LOG.debug("Disk overhead for %(flavor)d GB instance; %(overhead)d " "GB", {'flavor': instance.flavor.root_gb, 'overhead': overhead.get('disk_gb', 0)}) pci_requests = objects.InstancePCIRequests.get_by_instance_uuid( context, instance.uuid) #如果Claim返回none表示主机的资源满足不了新建虚拟机的需求， #会调用__exit__()方法将占用的资源返还到主机可用资源中。 claim = claims.Claim(context, instance, self, self.compute_node, pci_requests, overhead=overhead, limits=limits) # self._set_instance_host_and_node() will save instance to the DB # so set instance.numa_topology first. We need to make sure # that numa_topology is saved while under COMPUTE_RESOURCE_SEMAPHORE # so that the resource audit knows about any cpus we've pinned. instance_numa_topology = claim.claimed_numa_topology instance.numa_topology = instance_numa_topology #通过Object model，若不在同一节点则调用ConductorAPI 更新instance的host,node等属性。 self._set_instance_host_and_node(instance) if self.pci_tracker: # NOTE(jaypipes): ComputeNode.pci_device_pools is set below # in _update_usage_from_instance(). self.pci_tracker.claim_instance(context, pci_requests, instance_numa_topology) # Mark resources in-use and update stats #根据新建虚拟机的需求计算主机可用资源。 self._update_usage_from_instance(context, instance) elevated = context.elevated() # persist changes to the compute node: #根据上面的计算结果更新数据库。 self._update(elevated) return claim 使用Periodic Task:在类nova.compute.manager.ComputeManager中有个周期性的任务update_available_resource()用于更新主机可用资源。顺便提一点，nova-compute在启动的时候会启动两个周期任务，一个用于更新主机可用资源update_available_resource，一个用于汇报本机nova-compute的服务状态给数据库（report state）。 #根据配置文件周期性的调用该函数。 #该周期性的任务用于同步数据库内可用资源与hypervisor保持一致 @periodic_task.periodic_task(spacing=CONF.update_resources_interval) def update_available_resource(self, context): """See driver.get_available_resource() Periodic process that keeps that the compute host's understanding of resource availability and usage in sync with the underlying hypervisor. :param context: security context """ compute_nodes_in_db = self._get_compute_nodes_in_db(context, use_slave=True) nodenames = set(self.driver.get_available_nodes()) #更新所有主机数据库中的资源数据。 for nodename in nodenames: self.update_available_resource_for_node(context, nodename) self._resource_tracker_dict = { k: v for k, v in self._resource_tracker_dict.items() if k in nodenames} # Delete orphan compute node not reported by driver but still in db #清除inactive但仍在数据库中存有数据的计算节点信息 for cn in compute_nodes_in_db: if cn.hypervisor_hostname not in nodenames: LOG.info(_LI("Deleting orphan compute node %s"), cn.id) cn.destroy() 这两种机制并不冲突，一种是在新建或迁移虚拟机等涉及计算节点资源操作时，在当前数据库中数据的基础上更新，，保证数据库里的资源及时更新，以便为nova-scheduler 提供最新数据。周期性任务是保证hypervisor获取的资源数据与数据库中的数据保持一致。 关于resource tracker 如何更新数据库的代码分析 nova-compute Periodic tasks 机制 参考文章openstack 设计与实现 Openstack liberty源码分析 之 云主机的启动过程3 nova-compute Periodic tasks 机制 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--novaScheduler 源代码解读]]></title>
    <url>%2F2016%2F12%2F07%2FnovaScheduler-sourcecode-explain%2F</url>
    <content type="text"><![CDATA[目录结构：novaScheduler 介绍 以nova boot 为例解读代码 novascheduler 整体讲解 Filtering 讲解 Weighting 讲解 novaScheduler 介绍 关于nova不再赘述，之前文章由介绍。 nova-scheduler的主要作用就是就是根据各种规则为虚拟机选择一个合适的主机。 同nova-volume被单独剥离为cinder项目一样，社区也致力于将nova-scheduler剥离为gantt,作为一个通用的调度服务。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova boot 为例解读代码 由之前解读novaConductor时，我们把牵扯到调度novascheduler的部分拿过来： 12345678910def _schedule_instances(self, context, request_spec, filter_properties): scheduler_utils.setup_instance_group(context, request_spec, filter_properties) # TODO(sbauza): Hydrate here the object until we modify the # scheduler.utils methods to directly use the RequestSpec object spec_obj = objects.RequestSpec.from_primitives( context, request_spec, filter_properties) #调用scheduler_client,获取可用host列表 hosts = self.scheduler_client.select_destinations(context, spec_obj) return hosts 找到scheduler_client.select_destinations函数 1234#有一个装饰器，当发送失败的时候retry@utils.retry_select_destinationsdef select_destinations(self, context, spec_obj): return self.queryclient.select_destinations(context, spec_obj) 最终调用rpcapi中的select_destinations方法： 12345678910111213def select_destinations(self, ctxt, spec_obj): version = '4.3' msg_args = &#123;'spec_obj': spec_obj&#125; if not self.client.can_send_version(version): del msg_args['spec_obj'] msg_args['request_spec'] = spec_obj.to_legacy_request_spec_dict() msg_args['filter_properties' ] = spec_obj.to_legacy_filter_properties_dict() version = '4.0' #生成 RPCClient cctxt = self.client.prepare(version=version) #发送rpc call 同步请求 return cctxt.call(ctxt, 'select_destinations', **msg_args) 接下来看下novascheduler 接收到’select_destinations’ 的rpc请求之后的动作，之前说Novaconductor的时候说到接受请求执行的函数都在manager文件中。 12345678910111213141516@messaging.expected_exceptions(exception.NoValidHost)def select_destinations(self, ctxt, request_spec=None, filter_properties=None, spec_obj=_sentinel): """Returns destinations(s) best suited for this RequestSpec. The result should be a list of dicts with 'host', 'nodename' and 'limits' as keys. """ if spec_obj is self._sentinel: spec_obj = objects.RequestSpec.from_primitives(ctxt, request_spec, filter_properties) #调用driver中的方法，该driver即典型的stevedore中的driver插件导入 dests = self.driver.select_destinations(ctxt, spec_obj) return jsonutils.to_primitive(dests) 看下driver的相关代码： 12345678910def __init__(self, scheduler_driver=None, *args, **kwargs): if not scheduler_driver: scheduler_driver = CONF.scheduler.driver #根据nova配置文件选取scheduler_driver，利用stevedore的driver载入方式载入setup.cfg中对应scheduler_driver。 self.driver = driver.DriverManager( "nova.scheduler.driver", scheduler_driver, invoke_on_load=True).driver super(SchedulerManager, self).__init__(service_name='scheduler', *args, **kwargs) 一般setup.cfg 中对应的scheduler.driver有： 12345nova.scheduler.driver = filter_scheduler = nova.scheduler.filter_scheduler:FilterScheduler caching_scheduler = nova.scheduler.caching_scheduler:CachingScheduler chance_scheduler = nova.scheduler.chance:ChanceScheduler fake_scheduler = nova.tests.unit.scheduler.fakes:FakeScheduler 默认获取到的scheduler.driver是filter_scheduler，关于stevedore的用法，check this 找到filter_scheduler的select_destinations方法，该方法做一些参数的处理，然后调用_schedule方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def _schedule(self, context, spec_obj): """Returns a list of hosts that meet the required specs, ordered by their fitness. """ elevated = context.elevated() config_options = self._get_configuration_options() #获取所有的host列表 hosts = self._get_all_host_states(elevated) selected_hosts = [] num_instances = spec_obj.num_instances # NOTE(sbauza): Adding one field for any out-of-tree need spec_obj.config_options = config_options #此处num_instances 猜测是新建虚拟机的个数，因为 boot 命令中有一个参数新建多个虚拟机 for num in range(num_instances): # Filter local hosts based on requirements ... #根据过滤条件筛选符合filter 的host，具体filter代码位于filters/目录下 hosts = self.host_manager.get_filtered_hosts(hosts, spec_obj, index=num) if not hosts: # Can't get any more locally. break LOG.debug("Filtered %(hosts)s", &#123;'hosts': hosts&#125;) #根据weight条件将host列表按权重排序，具体weight代码位于weights/目录下 weighed_hosts = self.host_manager.get_weighed_hosts(hosts, spec_obj) LOG.debug("Weighed %(hosts)s", &#123;'hosts': weighed_hosts&#125;) host_subset_size = max(1, CONF.filter_scheduler.host_subset_size) if host_subset_size &lt; len(weighed_hosts): weighed_hosts = weighed_hosts[0:host_subset_size] chosen_host = random.choice(weighed_hosts) LOG.debug("Selected host: %(host)s", &#123;'host': chosen_host&#125;) selected_hosts.append(chosen_host) # Now consume the resources so the filter/weights # will change for the next instance. chosen_host.obj.consume_from_request(spec_obj) if spec_obj.instance_group is not None: spec_obj.instance_group.hosts.append(chosen_host.obj.host) # hosts has to be not part of the updates when saving spec_obj.instance_group.obj_reset_changes(['hosts']) return selected_hosts 先看下获取所有的host列表的代码_get_all_host_states，直接调用host_manager.get_all_host_states， 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def get_all_host_states(self, context): """Returns a list of HostStates that represents all the hosts the HostManager knows about. Also, each of the consumable resources in HostState are pre-populated and adjusted based on data in the db. """ #从数据库获取nova-computer服务的service列表 service_refs = &#123;service.host: service for service in objects.ServiceList.get_by_binary( context, 'nova-compute', include_disabled=True)&#125; # Get resource usage across the available compute nodes: #从数据库获取所有计算节点的资源使用情况 compute_nodes = objects.ComputeNodeList.get_all(context) seen_nodes = set() #更新计算节点的资源使用情况 for compute in compute_nodes: service = service_refs.get(compute.host) if not service: LOG.warning(_LW( "No compute service record found for host %(host)s"), &#123;'host': compute.host&#125;) continue host = compute.host node = compute.hypervisor_hostname state_key = (host, node) host_state = self.host_state_map.get(state_key) if not host_state: #获取host_state,如果本身维护的数据更新时间比数据库的早晚，就不用从数据库获取了。 host_state = self.host_state_cls(host, node, compute=compute) self.host_state_map[state_key] = host_state # We force to update the aggregates info each time a new request # comes in, because some changes on the aggregates could have been # happening after setting this field for the first time host_state.update(compute, dict(service), self._get_aggregates_info(host), self._get_instance_info(context, compute)) seen_nodes.add(state_key) # remove compute nodes from host_state_map if they are not active #去掉状态为inactive的node dead_nodes = set(self.host_state_map.keys()) - seen_nodes for state_key in dead_nodes: host, node = state_key LOG.info(_LI("Removing dead compute node %(host)s:%(node)s " "from scheduler"), &#123;'host': host, 'node': node&#125;) del self.host_state_map[state_key] #返回一个node的迭代器 return six.itervalues(self.host_state_map) 再看下根据filter过滤host的函数host_manager.get_filtered_hosts 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def get_filtered_hosts(self, hosts, spec_obj, index=0): """Filter hosts and return only ones passing all filters.""" #若filter_properties中指定了ignore_hosts,则排除相应host def _strip_ignore_hosts(host_map, hosts_to_ignore): ................................. ##若filter_properties中指定了forced_hosts,则排除不在forced_hosts中的host def _match_forced_hosts(host_map, hosts_to_force): ................................. #若filter_properties中指定了forced_nodes,则排除不在forced_nodes中的host def _match_forced_nodes(host_map, nodes_to_force): ................................. #若指定了requested_destination，则在指定requested_destination中filter host def _get_hosts_matching_request(hosts, requested_destination): .................................. ignore_hosts = spec_obj.ignore_hosts or [] force_hosts = spec_obj.force_hosts or [] force_nodes = spec_obj.force_nodes or [] requested_node = spec_obj.requested_destination if requested_node is not None: # NOTE(sbauza): Reduce a potentially long set of hosts as much as # possible to any requested destination nodes before passing the # list to the filters hosts = _get_hosts_matching_request(hosts, requested_node) if ignore_hosts or force_hosts or force_nodes: # NOTE(deva): we can't assume "host" is unique because # one host may have many nodes. name_to_cls_map = &#123;(x.host, x.nodename): x for x in hosts&#125; if ignore_hosts: _strip_ignore_hosts(name_to_cls_map, ignore_hosts) if not name_to_cls_map: return [] # NOTE(deva): allow force_hosts and force_nodes independently if force_hosts: _match_forced_hosts(name_to_cls_map, force_hosts) if force_nodes: _match_forced_nodes(name_to_cls_map, force_nodes) if force_hosts or force_nodes: # NOTE(deva): Skip filters when forcing host or node if name_to_cls_map: return name_to_cls_map.values() else: return [] #获得经过上述条件后满足条件的host列表 hosts = six.itervalues(name_to_cls_map) #调用配置文件中规定的filter对host列表进行过滤，筛选host return self.filter_handler.get_filtered_objects(self.enabled_filters, hosts, spec_obj, index) 再看下根据weight条件将host列表按权重排序host_manager.get_weighed_hosts方法，直接调用host_manager.get_weighed_hosts： 12345678910111213141516171819202122def get_weighed_objects(self, weighers, obj_list, weighing_properties): """Return a sorted (descending), normalized list of WeighedObjects.""" #获取WeighedObject对象 ，就是对host做一层抽象 weighed_objs = [self.object_class(obj, 0.0) for obj in obj_list] #如果只有一个WeighedObject，直接返回 if len(weighed_objs) &lt;= 1: return weighed_objs #逐一调用各权重过滤器。 for weigher in weighers: weights = weigher.weigh_objects(weighed_objs, weighing_properties) # Normalize the weights weights = normalize(weights, minval=weigher.minval, maxval=weigher.maxval) #累加权重值 for i, weight in enumerate(weights): obj = weighed_objs[i] obj.weight += weigher.weight_multiplier() * weight #根据权重值降序排列并返回 return sorted(weighed_objs, key=lambda x: x.weight, reverse=True) 到这里，Novascheduler的分析就基本结束了。 novascheduler 整体讲解 novascheduler 源码目录架构： nova scheduler实现了三种调度器：ChanceScheduler(随机调度器)，FilterScheduler(过滤调度器)，CachingScheduler(缓存调度器，在FilterScheduler的基础上将主机资源缓存在本地，通过后台定时任务从数据库更新缓存信息)。如果我们想自己实现定制的调度器，需要实现的接口在nova.scheduler.driver.Scheduler,继承类SchedulerDriver. 一般来说，scheduler的过程大致是这样的：从nova.scheduler.rpcapi.SchedulerAPI发送RPC请求到nova.scheduler.manager.SchedulerManager;从SchedulerManager到调度器（类SchedulerDriver）;从SchedulerDriver到Filters;从Filters到权重计算排序Weights。 Filtering 讲解 所有的filter都在/nova/scheduler/filters目录，继承自nova.scheduler.filters.BaseHostFilter。实现一个host_passes()函数。 1234567891011121314151617181920212223242526272829303132class DiskFilter(filters.BaseHostFilter): """Disk Filter with over subscription flag.""" def _get_disk_allocation_ratio(self, host_state, spec_obj): return host_state.disk_allocation_ratio #重点在实现该函数 def host_passes(self, host_state, spec_obj): """Filter based on disk usage.""" requested_disk = (1024 * (spec_obj.root_gb + spec_obj.ephemeral_gb) + spec_obj.swap) free_disk_mb = host_state.free_disk_mb total_usable_disk_mb = host_state.total_usable_disk_gb * 1024 # Do not allow an instance to overcommit against itself, only against # other instances. In other words, if there isn't room for even just # this one instance in total_usable_disk space, consider the host full. if total_usable_disk_mb &lt; requested_disk: LOG.debug("%(host_state)s does not have %(requested_disk)s " "MB usable disk space before overcommit, it only " "has %(physical_disk_size)s MB.", &#123;'host_state': host_state, 'requested_disk': requested_disk, 'physical_disk_size': total_usable_disk_mb&#125;) return False ............................... disk_gb_limit = disk_mb_limit / 1024 host_state.limits['disk_gb'] = disk_gb_limit return True Weighting 讲解 所有weighter位于nova/scheduler/weights目录，继承自weights.BaseWeighter; 1234567891011class RAMWeigher(weights.BaseHostWeigher): #可以设置maxval ,minval指明权重的最大最小值。 minval = 0 #权重的系数，最终排序时需要将各种weighter得到的权重乘上对应的系数，有多个weighter时才有意义，可以通过配置选项ram_weight_multiplier配置，默认为1.0 def weight_multiplier(self): """Override the weight multiplier.""" return CONF.filter_scheduler.ram_weight_multiplier #计算权重值，对于RAMWeighter直接返回可用内存大小即可 def _weigh_object(self, host_state, weight_properties): """Higher weights win. We want spreading to be the default.""" return host_state.free_ram_mb 参考文章openstack 设计与实现 Openstack liberty源码分析 之 云主机的启动过程2 学习Python动态扩展包stevedore 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--novaConductor 源代码解读]]></title>
    <url>%2F2016%2F12%2F06%2FnovaConductor-sourcecode-explain%2F</url>
    <content type="text"><![CDATA[目录结构：novaConductor 介绍 以nova boot 为例解读代码 novaConductor 介绍 关于nova不再赘述，之前文章由介绍。 根据wiki介绍，NovaConductor更像是一个编排工具，它会接受novaapi的请求，将创建虚拟机等任务交给novacompute处理，将寻找host的任务交给novascheduler，同时，会负责跟数据库的交互。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova boot 为例解读代码 先简略叙述下nova boot 在novaapi的过程，novaapi接收restful请求（post /servers）,根据apipaste配置文件,APIrouter,该请求最终路由到nova/api/openstack/servers.py.ServersController.create ,该方法完成参数的转换，解析，以及policy认证等工作后，调用nova/compute/api.py.API.create，再转给_create_instance 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687def _create_instance(self, context, instance_type, image_href, kernel_id, ramdisk_id, min_count, max_count, display_name, display_description, key_name, key_data, security_groups, availability_zone, user_data, metadata, injected_files, admin_password, access_ip_v4, access_ip_v6, requested_networks, config_drive, block_device_mapping, auto_disk_config, filter_properties, reservation_id=None, legacy_bdm=True, shutdown_terminate=False, check_server_group_quota=False): """Verify all the input parameters regardless of the provisioning strategy being performed and schedule the instance(s) for creation. """ # Normalize and setup some parameters if reservation_id is None: reservation_id = utils.generate_uid('r') security_groups = security_groups or ['default'] min_count = min_count or 1 max_count = max_count or min_count block_device_mapping = block_device_mapping or [] #获取image相关信息（metadata） if image_href: image_id, boot_meta = self._get_image(context, image_href) else: image_id = None boot_meta = self._get_bdm_image_metadata( context, block_device_mapping, legacy_bdm) self._check_auto_disk_config(image=boot_meta, auto_disk_config=auto_disk_config) #生成instance配置 base_options, max_net_count, key_pair = \ self._validate_and_build_base_options( context, instance_type, boot_meta, image_href, image_id, kernel_id, ramdisk_id, display_name, display_description, key_name, key_data, security_groups, availability_zone, user_data, metadata, access_ip_v4, access_ip_v6, requested_networks, config_drive, auto_disk_config, reservation_id, max_count) # max_net_count is the maximum number of instances requested by the # user adjusted for any network quota constraints, including # consideration of connections to each requested network if max_net_count &lt; min_count: raise exception.PortLimitExceeded() elif max_net_count &lt; max_count: LOG.info(_LI("max count reduced from %(max_count)d to " "%(max_net_count)d due to network port quota"), &#123;'max_count': max_count, 'max_net_count': max_net_count&#125;) max_count = max_net_count #确定块设备映射 block_device_mapping = self._check_and_transform_bdm(context, base_options, instance_type, boot_meta, min_count, max_count, block_device_mapping, legacy_bdm) # We can't do this check earlier because we need bdms from all sources # to have been merged in order to get the root bdm. self._checks_for_create_and_rebuild(context, image_id, boot_meta, instance_type, metadata, injected_files, block_device_mapping.root_bdm()) instance_group = self._get_requested_instance_group(context, filter_properties) #创建instance对象，并写入数据库 instances = self._provision_instances(context, instance_type, min_count, max_count, base_options, boot_meta, security_groups, block_device_mapping, shutdown_terminate, instance_group, check_server_group_quota, filter_properties, key_pair) #更新instance状态为create for instance in instances: self._record_action_start(context, instance, instance_actions.CREATE) #调用conductor api，之后会通过conductor rpc将请求转发给conductor manager self.compute_task_api.build_instances(context, instances=instances, image=boot_meta, filter_properties=filter_properties, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=block_device_mapping, legacy_bdm=False) return (instances, reservation_id) -nova/conductor/api.py.ComputeTaskAPI.build_instance 直接将请求转发给nova/conductor/rpcapi.py.ComputeTaskAPI.build_instance。 123456789101112131415161718192021222324252627282930def build_instances(self, context, instances, image, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping, legacy_bdm=True): image_p = jsonutils.to_primitive(image) version = '1.10' if not self.client.can_send_version(version): version = '1.9' if 'instance_type' in filter_properties: flavor = filter_properties['instance_type'] flavor_p = objects_base.obj_to_primitive(flavor) filter_properties = dict(filter_properties, instance_type=flavor_p) kw = &#123;'instances': instances, 'image': image_p, 'filter_properties': filter_properties, 'admin_password': admin_password, 'injected_files': injected_files, 'requested_networks': requested_networks, 'security_groups': security_groups&#125; if not self.client.can_send_version(version): version = '1.8' kw['requested_networks'] = kw['requested_networks'].as_tuples() if not self.client.can_send_version('1.7'): version = '1.5' bdm_p = objects_base.obj_to_primitive(block_device_mapping) kw.update(&#123;'block_device_mapping': bdm_p, 'legacy_bdm': legacy_bdm&#125;) #生成RPCClient对象，准备发送 cctxt = self.client.prepare(version=version) #发送异步rpc消息到消息队列，conductor-manager会收到该消息 cctxt.cast(context, 'build_instances', **kw) 顺便说下这个生成的cctxt其实是一个RPCClient对象，在oslo.messaging项目中，调用的call,cast方法就是RPCClient对象对应的方法。 在讲conductor-manage之前，先说一下rabbitmq服务端的创建过程，该过程在服务启动的时候创建。其实server创建的主要目的就是监听端口，而openstack的服务主要通过两种方式调用：AMQP和restful。所以server的创建也主要有两种：一种是监听AMQP请求（Service），一种是监听restful请求（WSGIService）。大致看下nova-conductor的启动过程。 12345678910111213141516def main(): config.parse_args(sys.argv) #解析配置文件 logging.setup(CONF, "nova") # 日志模块 utils.monkey_patch() #eventlet模块的补丁 objects.register_all() # object模块注册（与数据库访问有关） objects.Service.enable_min_version_cache() gmr.TextGuruMeditation.setup_autorun(version) #生成一个server对象 server = service.Service.create(binary='nova-conductor', topic=CONF.conductor.topic, manager=CONF.conductor.manager) workers = CONF.conductor.workers or processutils.get_worker_count() #启动service service.serve(server, workers=workers) service.wait() 看下server对象的创建过程service.Service.create： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def create(cls, host=None, binary=None, topic=None, manager=None, report_interval=None, periodic_enable=None, periodic_fuzzy_delay=None, periodic_interval_max=None, db_allowed=True): """Instantiates class and passes back application object. :param host: defaults to CONF.host :param binary: defaults to basename of executable :param topic: defaults to bin_name - 'nova-' part :param manager: defaults to CONF.&lt;topic&gt;_manager :param report_interval: defaults to CONF.report_interval :param periodic_enable: defaults to CONF.periodic_enable :param periodic_fuzzy_delay: defaults to CONF.periodic_fuzzy_delay :param periodic_interval_max: if set, the max time to wait between runs """ # 下面的参数解释见上面的英文注释 if not host: host = CONF.host if not binary: binary = os.path.basename(sys.argv[0]) #topic就是rabbitmq中连接queue 的依据 if not topic: topic = binary.rpartition('nova-')[2] #真正执行代码的manager类 if not manager: manager_cls = ('%s_manager' % binary.rpartition('nova-')[2]) manager = CONF.get(manager_cls, None) if report_interval is None: report_interval = CONF.report_interval if periodic_enable is None: periodic_enable = CONF.periodic_enable if periodic_fuzzy_delay is None: periodic_fuzzy_delay = CONF.periodic_fuzzy_delay debugger.init() #传参，完成初始化 service_obj = cls(host, binary, topic, manager, report_interval=report_interval, periodic_enable=periodic_enable, periodic_fuzzy_delay=periodic_fuzzy_delay, periodic_interval_max=periodic_interval_max, db_allowed=db_allowed) return service_obj 再看下service.serve()完成的工作,该函数直接调用oslo-service的launch()方法: 12345678910111213def launch(conf, service, workers=1, restart_method='reload'): if workers is not None and workers &lt;= 0: raise ValueError(_("Number of workers should be positive!")) #只起一个进程来运行服务 if workers is None or workers == 1: launcher = ServiceLauncher(conf, restart_method=restart_method) else: #起woker个进程运行服务 launcher = ProcessLauncher(conf, restart_method=restart_method) launcher.launch_service(service, workers=workers) return launcher 进入launcher.launch_service,调用一个add方法，看下代码（此处为起一个进程的launcher为例）： 12345678def add(self, service): """Add a service to a list and create a thread to run it. :param service: service to run """ #将一个service加入service list,并起一个thread去执行 self.services.append(service) self.tg.add_thread(self.run_service, service, self.done) add_thread会起一个green thread，执行callback这个回调，在这里就是server.start(): 123456789101112131415161718def start(self): ....................................... target = messaging.Target(topic=self.topic, server=self.host) endpoints = [ self.manager, baserpc.BaseRPCAPI(self.manager.service_name, self.backdoor_port) ] endpoints.extend(self.manager.additional_endpoints) serializer = objects_base.NovaObjectSerializer() #获取rpcserver并监听对应端口 self.rpcserver = rpc.get_server(target, endpoints, serializer) self.rpcserver.start() self.manager.post_start_hook() ................................... 接下来看conductor-manage的处理过程。nova-conductor收到rpc请求，根据路由映射，该请求会交给对应manager的函数处理，nova/conductor/manager.py.ComputeTaskManager.build_instances 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def build_instances(self, context, instances, image, filter_properties, admin_password, injected_files, requested_networks, security_groups, block_device_mapping=None, legacy_bdm=True): if (requested_networks and not isinstance(requested_networks, objects.NetworkRequestList)): requested_networks = objects.NetworkRequestList.from_tuples( requested_networks) # flavor = filter_properties.get('instance_type') if flavor and not isinstance(flavor, objects.Flavor): # Code downstream may expect extra_specs to be populated since it # is receiving an object, so lookup the flavor to ensure this. flavor = objects.Flavor.get_by_id(context, flavor['id']) filter_properties = dict(filter_properties, instance_type=flavor) request_spec = &#123;&#125; try: # check retry policy. Rather ugly use of instances[0]... # but if we've exceeded max retries... then we really only # have a single instance. #生成发送给scheduler的信息 request_spec = scheduler_utils.build_request_spec( context, image, instances) scheduler_utils.populate_retry( filter_properties, instances[0].uuid) #发送同步消息给nova-scheduler，选取用于创建云主机的主机(留给下篇文章) hosts = self._schedule_instances( context, request_spec, filter_properties) except Exception as exc: updates = &#123;'vm_state': vm_states.ERROR, 'task_state': None&#125; #设置虚拟机state,taskstate for instance in instances: self._set_vm_state_and_notify( context, instance.uuid, 'build_instances', updates, exc, request_spec) try: # If the BuildRequest stays around then instance show/lists # will pull from it rather than the errored instance. self._destroy_build_request(context, instance) except exception.BuildRequestNotFound: pass self._cleanup_allocated_networks( context, instance, requested_networks) return ............................. #发送异步消息给nova-compute，完成instance的boot(下篇文章) self.compute_rpcapi.build_and_run_instance(context, instance=instance, host=host['host'], image=image, request_spec=request_spec, filter_properties=local_filter_props, admin_password=admin_password, injected_files=injected_files, requested_networks=requested_networks, security_groups=security_groups, block_device_mapping=bdms, node=host['nodename'], limits=host['limits']) 从上面的源码可以看出，build_instances方法主要实现过滤参数的组装，然后通过客户端scheduler_client发送rpc请求到scheduler完成host的选取，最后发送rpc请求到选取的host上，由nova-compute完成云主机的启动。 对nova boot在 Novaconductor的分析就完成了。简单从整体源码层面分析一下novaconductor,源码目录结构如下： 一般来说，rpcapi.py都是与rpc相关的，别的服务只需导入该模块便可以使用它提供的远程调用Novaconductor的服务，Novaconductor注册的RPCServer接收到rpc请求，然后由manager.py中的ConductorManager真正完成数据库的访问。由于数据库访问的特殊性，api.py又对rpc.api做了一层封装，所以其他模块需要导入的是api.py，api.py的封装主要是区分访问数据库是否需要通过RPC,如果数据库就在本地host,那么无需通过RPC调用。所以api.py中包含了四个类：LocalAPI,API,LocalComputerTaskAPI,ComputerTaskAPI。前两个类是Novaconductor访问数据库的接口，后两个是TaskaAPI接口，TaskaAPI接口主要包含耗时较长的任务，比如新建虚拟机，迁移虚拟机等。如果Novacompute与Novaconductor模块在同一台机器上，那么不用通过RPC,直接通过LocalAPI访问数据库，同理TaskAPI也不需要RPC,使用LocalTaskAPI。看下nova/conductor/init.py 1234567891011121314151617def API(*args, **kwargs): #根据配置选项use_local判断Novacompute与novaconductor是否在同一节点，决定使用哪一类API use_local = kwargs.pop('use_local', False) if CONF.conductor.use_local or use_local: api = conductor_api.LocalAPI else: api = conductor_api.API return api(*args, **kwargs)def ComputeTaskAPI(*args, **kwargs): #分析同上 use_local = kwargs.pop('use_local', False) if CONF.conductor.use_local or use_local: api = conductor_api.LocalComputeTaskAPI else: api = conductor_api.ComputeTaskAPI return api(*args, **kwargs) 接下来看下novaconductor与数据库交互的部分，在manager.py文件中有两个类，一个ConductorManager,一个ComputerTaskManager,对应两种API。与数据库交互的主要是ConductorManager。但当我打开这个类时瞬间懵圈了： 类里面不是我们期待的数据库操作方法，而是关于object的一些方法。这里要讲下object Model。 Object Model算是nova访问数据库的分水岭。之前，对某一个表的操作都是放在一个同名文件中,例如flavor.py,使用时直接调用文件中的函数操作数据库。Object Model引入后，新建flavor对象与flavor表对应，将对flavor 的操作封装在flavor对象中。这么做的原因大致是：1，nova-computer 与数据库升级时的版本问题。2，减少写入数据库的数据量。3，数据库的传值类型问题。看下图Object Model 工作流程： novaComputer 与novaConductor不在同一个节点时，虚线为引入ObjectModel之前NovaComputer访问数据库的流程。实现表示引入objectModel后的流程。可以看到，Novacomputer需要做数据库的操作时，将通过ObjectModel调用nova.conductor.rpcapi.ConductorAPI提供的RPC接口，novaConductor 接受到RPC请求后，通过本地ObjectModel完成数据库的更新。ObjectModel的代码位于nova/objects目录，里面的每一个类对应数据库中的一个表，如下instance.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 @obj_base.NovaObjectRegistry.registerclass Network(obj_base.NovaPersistentObject, obj_base.NovaObject, obj_base.NovaObjectDictCompat): # Version 1.0: Initial version # Version 1.1: Added in_use_on_host() # Version 1.2: Added mtu, dhcp_server, enable_dhcp, share_address VERSION = '1.2' #基类base.NovaObject 会记录变化的字段，更新数据库时只更新这些变化的字段。 #字典field是ComputerNode对象维护的信息，该字典的值不一定包含ComputerNode表内所有信息，每一个值的类型都是nova.object.fields模块定义的类型，若数据类型不匹配，就会抛出异常。 fields = &#123; 'id': fields.IntegerField(), 'label': fields.StringField(), 'injected': fields.BooleanField(), 'cidr': fields.IPV4NetworkField(nullable=True), 'cidr_v6': fields.IPV6NetworkField(nullable=True), 'multi_host': fields.BooleanField(), 'netmask': fields.IPV4AddressField(nullable=True), 'gateway': fields.IPV4AddressField(nullable=True), 'broadcast': fields.IPV4AddressField(nullable=True), 'netmask_v6': fields.IPV6AddressField(nullable=True), 'gateway_v6': fields.IPV6AddressField(nullable=True), 'bridge': fields.StringField(nullable=True), 'bridge_interface': fields.StringField(nullable=True), 'dns1': fields.IPAddressField(nullable=True), 'dns2': fields.IPAddressField(nullable=True), 'vlan': fields.IntegerField(nullable=True), 'vpn_public_address': fields.IPAddressField(nullable=True), 'vpn_public_port': fields.IntegerField(nullable=True), 'vpn_private_address': fields.IPAddressField(nullable=True), 'dhcp_start': fields.IPV4AddressField(nullable=True), 'rxtx_base': fields.IntegerField(nullable=True), 'project_id': fields.UUIDField(nullable=True), 'priority': fields.IntegerField(nullable=True), 'host': fields.StringField(nullable=True), 'uuid': fields.UUIDField(), 'mtu': fields.IntegerField(nullable=True), 'dhcp_server': fields.IPAddressField(nullable=True), 'enable_dhcp': fields.BooleanField(), 'share_address': fields.BooleanField(), &#125; ...................................... @obj_base.remotable_classmethod def get_by_id(cls, context, network_id, project_only='allow_none'): db_network = db.network_get(context, network_id, project_only=project_only) return cls._from_db_object(context, cls(), db_network) @obj_base.remotable_classmethod def get_by_uuid(cls, context, network_uuid): db_network = db.network_get_by_uuid(context, network_uuid) return cls._from_db_object(context, cls(), db_network) nova.object.base 中有定义两个重要的修饰函数：remotable_classmethod 和remotable,前者用于修饰类的方法，后者用于修饰实例的方法。如果novacomputer 与novaconductor不在同一节点，novacomputer 初始化时会将novaObject.indirection_api初始化为nova.conductor.rpcapi.ConductorAPI,此时调用remotable_classmethod修饰的函数时，会通过rpc交给Novaconductor处理，rpc请求中包含了该函数名，novaconductor接收到该rpc请求会调用相应的object model 函数完成数据库操作。 参考文章nova wiki Openstack liberty源码分析 之 云主机的启动过程1 openstack 设计与实现 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--novaApi 源代码解读]]></title>
    <url>%2F2016%2F12%2F01%2FnovaApi-sourcecode-explain%2F</url>
    <content type="text"><![CDATA[目录结构：novaApi 介绍 以nova list 为例解读代码 novaApi 介绍 关于nova不再赘述，之前文章由介绍。 之前说的novaclient的主要作用就是将命令行最终转化为restful请求去novaApi查找。 novaApi的执行过程主要分为两个阶段：接收到restful请求后，根据PasteDeploy 将请求的路由到具体的WSGI Application ,之后Routers将请求路由到具体函数并执行。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova list 为例解读代码 首先看下novaapi的启动过程，进入nova项目的setup.cfg文件 123456789101112131415161718192021222324252627282930313233343536console_scripts = nova-all = nova.cmd.all:main # nova 所有service的启动脚本，已弃用 nova-api = nova.cmd.api:main #此处为Nova-api的启动脚本 nova-api-metadata = nova.cmd.api_metadata:main nova-api-os-compute = nova.cmd.api_os_compute:main nova-cells = nova.cmd.cells:main nova-cert = nova.cmd.cert:main nova-compute = nova.cmd.compute:main nova-conductor = nova.cmd.conductor:main nova-console = nova.cmd.console:main nova-consoleauth = nova.cmd.consoleauth:main nova-dhcpbridge = nova.cmd.dhcpbridge:main nova-idmapshift = nova.cmd.idmapshift:main nova-manage = nova.cmd.manage:main nova-network = nova.cmd.network:main nova-novncproxy = nova.cmd.novncproxy:main nova-policy = nova.cmd.policy_check:main nova-rootwrap = oslo_rootwrap.cmd:main nova-rootwrap-daemon = oslo_rootwrap.cmd:daemon nova-scheduler = nova.cmd.scheduler:main nova-serialproxy = nova.cmd.serialproxy:main nova-spicehtml5proxy = nova.cmd.spicehtml5proxy:main nova-xvpvncproxy = nova.cmd.xvpvncproxy:mainwsgi_scripts = nova-placement-api = nova.api.openstack.placement.wsgi:init_applicationnova.api.v21.extensions = admin_actions = nova.api.openstack.compute.admin_actions:AdminActions admin_password = nova.api.openstack.compute.admin_password:AdminPassword agents = nova.api.openstack.compute.agents:Agents aggregates = nova.api.openstack.compute.aggregates:Aggregates assisted_volume_snapshots = nova.api.openstack.compute.assisted_volume_snapshots:AssistedVolumeSnapshots attach_interfaces = nova.api.openstack.compute.attach_interfaces:AttachInterfaces availability_zone = nova.api.openstack.compute.availability_zone:AvailabilityZone baremetal_nodes = nova.api.openstack.compute.baremetal_nodes:BareMetalNodes block_device_mapping = nova.api.openstack.compute.block_device_mapping:BlockDeviceMapping 简单解释下console_scripts中值得注意的几个服务： nova-api:包括两种API服务：nova-api-metadata ,nova-api-os-compute。根据配置文件中的enable_apis确定启动哪种服务。 nova-api-metadata:接受虚拟机的metadata相关的请求。只有采用nova-network部署时才使用。该工作目前已由neutron项目完成。 nova-api-os-compute:主要的Openstack Compute API服务。 nova-cells:主要适用于openstack增强横向扩展能力，check this nova-rootwrap:在openstack运行时以root身份运行某些shell命令 setup.cfg文件中nova.api.v21.extensions的字段中的每一项对应一个API，novaapi 启动时会用stevedore据此动态加载。 首先进入main函数 1234567891011121314151617181920212223242526def main(): config.parse_args(sys.argv) logging.setup(CONF, "nova") utils.monkey_patch() objects.register_all() if 'osapi_compute' in CONF.enabled_apis: # NOTE(mriedem): This is needed for caching the nova-compute service # version which is looked up when a server create request is made with # network id of 'auto' or 'none'. objects.Service.enable_min_version_cache() log = logging.getLogger(__name__) gmr.TextGuruMeditation.setup_autorun(version) # 生成Guru report的相关代码 launcher = service.process_launcher() started = 0 for api in CONF.enabled_apis: #根据配置文件中的配置创建server should_use_ssl = api in CONF.enabled_ssl_apis try: server = service.WSGIService(api, use_ssl=should_use_ssl) #创建WSGI server ,创建过程中paste deploy参与进来，加载相应application launcher.launch_service(server, workers=server.workers or 1) #根据配置文件中worker个数启动server started += 1 except exception.PasteAppNotFound as ex: log.warning( _LW("%s. ``enabled_apis`` includes bad values. " "Fix to remove this warning."), ex) ............................ 看下service.WSGIService()创建时发生了什么： 12345678910111213141516171819202122232425262728293031323334353637383940class WSGIService(service.Service): """Provides ability to launch API from a 'paste' configuration.""" def __init__(self, name, loader=None, use_ssl=False, max_url_len=None): """Initialize, but do not start the WSGI server. :param name: The name of the WSGI server given to the loader. :param loader: Loads the WSGI application using the given name. :returns: None """ self.name = name # NOTE(danms): Name can be metadata, os_compute, or ec2, per # nova.service's enabled_apis self.binary = 'nova-%s' % name self.topic = None self.manager = self._get_manager() self.loader = loader or wsgi.Loader() #从paste配置文件加载api对应的wsgi application self.app = self.loader.load_app(name) # inherit all compute_api worker counts from osapi_compute if name.startswith('openstack_compute_api'): wname = 'osapi_compute' else: wname = name self.host = getattr(CONF, '%s_listen' % name, "0.0.0.0") self.port = getattr(CONF, '%s_listen_port' % name, 0) self.workers = (getattr(CONF, '%s_workers' % wname, None) or processutils.get_worker_count()) ............................ # 指定IP，port监听socket，server的实现再用了eventlet进行封装，监听到http请求时，不会新建一个线程，而是用协程。 self.server = wsgi.Server(name, self.app, host=self.host, port=self.port, use_ssl=self.use_ssl, max_url_len=max_url_len) # Pull back actual port used self.port = self.server.port self.backdoor_port = None ................................ 当路径为/v21/servers/detail请求过来时，WSGI server监听到请求，根据paste配置文件路由到特定的WSGI application。 12345678[composite:openstack_compute_api_v21] use = call:nova.api.auth:pipeline_factory_v21 #调用pipeline_factory_v21函数，根据nova配置文件中的auth_strategy决定使用noauth2或keystonenoauth2 = compute_req_id faultwrap sizelimit noauth2 osapi_compute_app_v21keystone = compute_req_id faultwrap sizelimit authtoken keystonecontext osapi_compute_app_v21 ...................... [app:osapi_compute_app_v21]paste.app_factory = nova.api.openstack.compute:APIRouterV21.factory .................... 据此，我们便找到最终调用的函数APIRouterV21.factory，第一阶段完成，进入第二阶段 第二阶段主要是找到最终调用的函数，nova封装了route模块实现该路由功能。进入上次找到的路由函数APIRouterV21.factory。 1234567891011121314151617181920212223242526272829303132333435363738394041class APIRouterV21(base_wsgi.Router): """Routes requests on the OpenStack v2.1 API to the appropriate controller and method. """ @classmethod def factory(cls, global_config, **local_config): """Simple paste factory, :class:`nova.wsgi.Router` doesn't have one.""" return cls() @staticmethod def api_extension_namespace(): return 'nova.api.v21.extensions' #该类主要实现利用stevedore加载位于setup.cfg中nova.api.v21.extensions下所有资源，并利用check_func()函数进行检查 def __init__(self, init_only=None): def _check_load_extension(ext): return self._register_extension(ext) self.api_extension_manager = stevedore.enabled.EnabledExtensionManager( namespace=self.api_extension_namespace(), check_func=_check_load_extension, invoke_on_load=True, invoke_kwds=&#123;"extension_info": self.loaded_extension_info&#125;) mapper = ProjectMapper() self.resources = &#123;&#125; # NOTE(cyeoh) Core API support is rewritten as extensions # but conceptually still have core #这部分做的工作就是对所有封装的资源进行注册（对资源的属性进行一些扩展），并使用mapper对象建立路由规则（与***controllers类建立）。 if list(self.api_extension_manager): # NOTE(cyeoh): Stevedore raises an exception if there are # no plugins detected. I wonder if this is a bug. self._register_resources_check_inherits(mapper) self.api_extension_manager.map(self._register_controllers) LOG.info(_LI("Loaded extensions: %s"), sorted(self.loaded_extension_info.get_extensions().keys())) super(APIRouterV21, self).__init__(mapper) #打印出该mapper对象便可找到路由关系 ..................... nova中每种资源都被封装成一个nova.api.openstack.wsgi.Resource资源，并封装为一个WSGI application。 最后APIRouterV21将mapper传给父类Router做最后的工作. 123456789101112131415161718192021222324252627282930313233343536373839class Router(object): """WSGI middleware that maps incoming requests to WSGI apps.""" #使用routes模块将mapper与_dispatch()关联起来 def __init__(self, mapper): """Create a router for the given routes.Mapper. Each route in `mapper` must specify a 'controller', which is a WSGI app to call. You'll probably want to specify an 'action' as well and have your controller be an object that can route the request to the action-specific method. """ self.map = mapper self._router = routes.middleware.RoutesMiddleware(self._dispatch, self.map) #根据mapper将请求路由到适当的WSGI应用，即资源上。 @webob.dec.wsgify(RequestClass=Request) def __call__(self, req): """Route the incoming request to a controller based on self.map. If no match, return a 404. """ return self._router @staticmethod @webob.dec.wsgify(RequestClass=Request) def _dispatch(req): """Dispatch the request to the appropriate controller. Called by self._router after matching the incoming request to a route and putting the information into req.environ. Either returns 404 or the routed WSGI app's response. """ match = req.environ['wsgiorg.routing_args'][1] if not match: return webob.exc.HTTPNotFound() app = match['controller'] return app 至此，所有阶段便完成了，GET /v21/servers/detail最终会调用的函数是setup.cfg中的server对应的controller的detail函数。 123456789101112131415161718192021222324252627282930313233343536373839404142@extensions.expected_errors((400, 403)) def detail(self, req): """Returns a list of server details for a given user.""" context = req.environ['nova.context'] context.can(server_policies.SERVERS % 'detail') try: servers = self._get_servers(req, is_detail=True) #调用下面的_get_servers函数 except exception.Invalid as err: raise exc.HTTPBadRequest(explanation=err.format_message()) return servers ..............def _get_servers(self, req, is_detail): """Returns a list of servers, based on any search options specified.""" search_opts = &#123;&#125; search_opts.update(req.GET) context = req.environ['nova.context'] remove_invalid_options(context, search_opts, self._get_server_search_options(req)) # Verify search by 'status' contains a valid status. # Convert it to filter by vm_state or task_state for compute_api. # For non-admin user, vm_state and task_state are filtered through # remove_invalid_options function, based on value of status field. # Set value to vm_state and task_state to make search simple. search_opts.pop('status', None) ............ #最终调用函数在这里compute_api.get_all() try: instance_list = self.compute_api.get_all(elevated or context, search_opts=search_opts, limit=limit, marker=marker, expected_attrs=expected_attrs, sort_keys=sort_keys, sort_dirs=sort_dirs) except exception.MarkerNotFound: msg = _('marker [%s] not found') % marker raise exc.HTTPBadRequest(explanation=msg) except exception.FlavorNotFound: LOG.debug("Flavor '%s' could not be found ", search_opts['flavor']) instance_list = objects.InstanceList() 参考文章openstack 设计与实现 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--novaClient 源代码解读]]></title>
    <url>%2F2016%2F11%2F30%2Fnovaclient-sourcecode-explain%2F</url>
    <content type="text"><![CDATA[目录结构：novaclient 介绍 以nova list 为例解读代码 novaclient 介绍 novaclient是Python写的nova API服务的客户端. 提供了两种使用形式：命令行形式和Python API形式（命令行形式其实最终也是调用的Python API）。 novaclient 本身不是类似于nova-api这种的service，没有一个守护进程在运行，而是在我们输入命令的之后根据入口main函数去相应的执行。 因为版本不同，代码也可能不同，该版本为Openstack Liberty 以nova list 为例解读代码 进入novaclient的根目录，找到setup.cfg文件，找到入口函数 其实我们也可以在安装了novaclient的机器中执行 which nova 找到对应入口。 进入novaclient.shell 的main函数，截取部分代码如下： 123456789101112131415161718192021def main(): try: argv = [encodeutils.safe_decode(a) for a in sys.argv[1:]] OpenStackComputeShell().main(argv) #重点在这个函数 except Exception as exc: logger.debug(exc, exc_info=1) if six.PY2: message = encodeutils.safe_encode(six.text_type(exc)) else: message = encodeutils.exception_to_unicode(exc) print("ERROR (%(type)s): %(msg)s" % &#123; 'type': exc.__class__.__name__, 'msg': message&#125;, file=sys.stderr) sys.exit(1) except KeyboardInterrupt: print(_("... terminating nova client"), file=sys.stderr) sys.exit(130)if __name__ == "__main__": main() 找到OpenStackComputeShell().main函数, 123456789101112131415161718192021def main(self, argv): # Parse args once to find version and debug settings parser = self.get_base_parser(argv) (args, args_list) = parser.parse_known_args(argv) self.setup_debugging(args.debug) self.extensions = [] do_help = ('help' in argv) or ( '--help' in argv) or ('-h' in argv) or not argv # bash-completion should not require authentication skip_auth = do_help or ( 'bash-completion' in argv) if not args.os_compute_api_version: api_version = api_versions.get_api_version( DEFAULT_MAJOR_OS_COMPUTE_API_VERSION) else: api_version = api_versions.get_api_version( args.os_compute_api_version) ......................... 这个函数很长，大体做的工作如下： 1,解析命令行，判定是否需要做认证操作（如help命令不需要） 2,需要做认证操作的话，判定认证方法（一般为keystone） 1234567891011if use_session: # Not using Nova auth plugin, so use keystone with utils.record_time(self.times, args.timings, 'auth_url', args.os_auth_url): keystone_session = ( loading.load_session_from_argparse_arguments(args)) keystone_auth = ( loading.load_auth_from_argparse_arguments(args)) else: # set password for auth plugins os_password = args.os_password 3,创建一个Client用于确定API版本 1234567891011121314151617181920212223242526272829# This client is just used to discover api version. Version API needn't# microversion, so we just pass version 2 at here.self.cs = client.Client( api_versions.APIVersion("2.0"), os_username, os_password, os_project_name, tenant_id=os_project_id, user_id=os_user_id, auth_url=os_auth_url, insecure=insecure, region_name=os_region_name, endpoint_type=endpoint_type, extensions=self.extensions, service_type=service_type, service_name=service_name, auth_token=auth_token, volume_service_name=volume_service_name, timings=args.timings, bypass_url=bypass_url, os_cache=os_cache, http_log_debug=args.debug, cacert=cacert, timeout=timeout, session=keystone_session, auth=keystone_auth, logger=self.client_logger)if not skip_auth: if not api_version.is_latest(): if api_version &gt; api_versions.APIVersion("2.0"): if not api_version.matches(novaclient.API_MIN_VERSION, novaclient.API_MAX_VERSION): raise exc.CommandError( _("The specified version isn't supported by " "client. The valid version range is '%(min)s' " "to '%(max)s'") % &#123; "min": novaclient.API_MIN_VERSION.get_string(), "max": novaclient.API_MAX_VERSION.get_string()&#125; ) api_version = api_versions.discover_version(self.cs, api_version) 4,根据发现的API版本创建一个client 1234567891011121314# Recreate client object with discovered version.self.cs = client.Client( api_version, os_username, os_password, os_project_name, tenant_id=os_project_id, user_id=os_user_id, auth_url=os_auth_url, insecure=insecure, region_name=os_region_name, endpoint_type=endpoint_type, extensions=self.extensions, service_type=service_type, service_name=service_name, auth_token=auth_token, volume_service_name=volume_service_name, timings=args.timings, bypass_url=bypass_url, os_cache=os_cache, http_log_debug=args.debug, cacert=cacert, timeout=timeout, session=keystone_session, auth=keystone_auth) 5,根据args调用指定版本API 1args.func(self.cs, args) 6,看下最终调用的V2版本下的shell.py(此处为do_list()函数) 12345678910111213141516171819202122 @utils.arg( '--not-tags-any', dest='not-tags-any', metavar='&lt;not-tags-any&gt;', default=None, help=_("Only the servers that do not have at least one of the given tags" "will be included in the list result. Boolean expression in this " "case is 'NOT(t1 OR t2)'. Tags must be separated by commas: " "--not-tags-any &lt;tag1,tag2&gt;"), start_version="2.26")def do_list(cs, args): """List active servers.""" imageid = None flavorid = None if args.image: imageid = _find_image(cs, args.image).id if args.flavor: flavorid = _find_flavor(cs, args.flavor).id # search by tenant or user only works with all_tenants if args.tenant or args.user: args.all_tenants = 1 ....................... 比较重要的语句如下： 1234567servers = cs.servers.list(detailed=detailed, search_opts=search_opts, sort_keys=sort_keys, sort_dirs=sort_dirs, marker=args.marker, limit=args.limit) ....................... 7,调用v2 下client中的servers 中的list方法， 123456789101112password = kwargs.pop('password', api_key)self.projectid = project_idself.tenant_id = tenant_idself.user_id = user_idself.flavors = flavors.FlavorManager(self)self.flavor_access = flavor_access.FlavorAccessManager(self)self.images = images.ImageManager(self)self.glance = images.GlanceManager(self)self.limits = limits.LimitsManager(self)self.servers = servers.ServerManager(self) # 调用此处的serversself.versions = versions.VersionManager(self)....................... 8,终于到最后一步 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def list(self, detailed=True, search_opts=None, marker=None, limit=None, sort_keys=None, sort_dirs=None): if search_opts is None: search_opts = &#123;&#125; qparams = &#123;&#125; for opt, val in six.iteritems(search_opts): if val: if isinstance(val, six.text_type): val = val.encode('utf-8') qparams[opt] = val detail = "" if detailed: detail = "/detail" result = base.ListWithMeta([], None) while True: if marker: qparams['marker'] = marker if limit and limit != -1: qparams['limit'] = limit # Transform the dict to a sequence of two-element tuples in fixed # order, then the encoded string will be consistent in Python 2&amp;3. if qparams or sort_keys or sort_dirs: # sort keys and directions are unique since the same parameter # key is repeated for each associated value # (ie, &amp;sort_key=key1&amp;sort_key=key2&amp;sort_key=key3) items = list(qparams.items()) if sort_keys: items.extend(('sort_key', sort_key) for sort_key in sort_keys) if sort_dirs: items.extend(('sort_dir', sort_dir) for sort_dir in sort_dirs) new_qparams = sorted(items, key=lambda x: x[0]) query_string = "?%s" % parse.urlencode(new_qparams) else: query_string = "" servers = self._list("/servers%s%s" % (detail, query_string), "servers") #重点在这 result.extend(servers) result.append_request_ids(servers.request_ids) if not servers or limit != -1: break marker = result[-1].id return result 找到最终调用的_list方法如下： 1234567def _list(self, url, response_key, obj_class=None, body=None): if body: resp, body = self.api.client.post(url, body=body) else: resp, body = self.api.client.get(url) ....................... 即最终通过restful api 去查找对应的数据,此处路径为（/servers/detail ）self.api.client是novaclient.client.SessionClient类的对象，但是在SessionClient类中没有找到get函数，在其父类adapter.LegacyJsonAdapter中发现该函数，而adapter文件是在keystoneclient中，所以self.api.client.get(url)调用的是keystoneclient/adapter.py中的LegacyJsonAdapter类的get()函数,最终找到Adapter类中的get()函数。 12def get(self, url, **kwargs): return self.request(url, 'GET', **kwargs) 之后会调用sessionClient的request()函数。 1234567891011121314151617181920212223def request(self, url, method, **kwargs): kwargs.setdefault('headers', kwargs.get('headers', &#123;&#125;)) api_versions.update_headers(kwargs["headers"], self.api_version) # NOTE(jamielennox): The standard call raises errors from # keystoneauth1, where we need to raise the novaclient errors. raise_exc = kwargs.pop('raise_exc', True) with utils.record_time(self.times, self.timings, method, url): resp, body = super(SessionClient, self).request(url, method, raise_exc=False, **kwargs) # 该函数 # if service name is None then use service_type for logging service = self.service_name or self.service_type _log_request_id(self.logger, resp, service) # TODO(andreykurilin): uncomment this line, when we will be able to # check only nova-related calls # api_versions.check_headers(resp, self.api_version) if raise_exc and resp.status_code &gt;= 400: raise exceptions.from_response(resp, body, url, method) return resp, body 调用adapter.LegacyJsonAdapter类中的request方法 123456789101112131415161718192021222324252627282930class LegacyJsonAdapter(Adapter): """Make something that looks like an old HTTPClient. A common case when using an adapter is that we want an interface similar to the HTTPClients of old which returned the body as JSON as well. You probably don't want this if you are starting from scratch. """ def request(self, *args, **kwargs): headers = kwargs.setdefault('headers', &#123;&#125;) headers.setdefault('Accept', 'application/json') try: kwargs['json'] = kwargs.pop('body') except KeyError: # nosec(cjschaef): kwargs doesn't contain a 'body' # key, while 'json' is an optional argument for Session.request pass resp = super(LegacyJsonAdapter, self).request(*args, **kwargs) body = None if resp.text: try: body = jsonutils.loads(resp.text) except ValueError: # nosec(cjschaef): return None for body as # expected pass return resp, body 不往下看了，再往下代码比较复杂。 参考文章openstack-L版源码解析之novaclientpythonnovaclient 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux系列--关于 linux 日志]]></title>
    <url>%2F2016%2F11%2F20%2Flinux-log%2F</url>
    <content type="text"><![CDATA[目录结构：Linux日志概述 常见日志文件 常用命令 Linux日志概述 Linux 系统的日志主要分为两种类型：一种是进程所属日志，另一种是syslog的信息，即系统syslog记录的日志。 Linux 日志系统可以划分为三个子系统：1，连接时间用户登录的日志，一般写入到/var/log/wtmp,/var/log/utmp两个文件中。2，进程统计日志。3错误日志，通过syslogd写入文件/var/log/message。 关于syslogd:Linux 内核由很多子系统组成，包括网络、文件访问、内存管理等。子系统需要给用户传送一些消息，这些消息内容包括消息的来源及其重要性等。所有的子系统都要把消息送到一个可以维护的公用消息区，于是就有了一个叫 syslog 的程序。它的配置文件通常位于/etc/log/syslog.conf。 关于logrotate:logrotate程序用来帮助用户管理日志文件，它以自己的守护进程工作。logrotate 周期性地旋转日志文件，可以周期性地把每个日志文件重命名成一个备份名字，然后让它的守护进程开始使用一个日志文件的新的拷贝。配置文件通常位于/etc/logrotate.conf。 常见日志文件直接引自云服务器 ECS Linux 系统中常见的日志文件介绍 /var/log/cron可以在 cron 文件中检查 crontab 任务调度是否实际执行，执行过程是否发生错误，以及 /etc/crontab 文件是否有语法错误或编写错误。 /var/log/dmesg可以在 dmesg 文件中检查系统在开机时候内核检测过程所产生的各项信息，包括系统的设备信息，以及在启动和操作过程中系统记录的任何错误和问题的信息。通过 dmesg 文件可以判断某些硬件设备（比如磁盘）在系统启动过程中是否被正确识别。 /var/log/lastlog可以在 lastlog 文件中检查系统上面所有账号的最后一次登录系统时的相关信息。 /var/log/messages可以在 messages 文件中检查到绝大多数系统发生的错误信息，如果系统发生一些未知的错误，建议客户首先检查一下 messages 文件。可以通过 tail -f /var/log/messages 实时查看系统内的变化情况。iptables 的默认日志就是 /var/log/messages。 /var/log/secure所有涉及需要输入账号密码的软件或程序，在登陆时（无论登录成功或失败）的信息都会被记录到 secure 文件，比如系统的 login 程序；su、sudo 命令；ssh、telnet、pop3、ftp 等程序的登录信息。 /var/log/wtmp可以在 wtmp 文件中检查到正确登陆系统的账户信息。由于 wtmp 文件已经被编码过的，所以需要使用 last 指令来取出文件的内容，用 cat 等命令无法直接查看此文件。 常用命令 w查看当前系统登录的用户以及用户在干什么 history当前登录用户的执行命令历史 last查看当前用户登录历史 who /var/log/wtmp查看用户登录系统历史 参考文章云服务器 ECS Linux 系统中常见的日志文件介绍 Linux系统日志简介 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--设置nfs作为cinder backend]]></title>
    <url>%2F2016%2F11%2F10%2Fmake-nfs-as-cinder-backend%2F</url>
    <content type="text"><![CDATA[目录结构：nfs 设置 修改cinder 配置文件 nfs 设置首先对nfs进行设置，大体以下几个步骤： 磁盘分区，格式化磁盘在格式化磁盘的时候出现如下错误：搜了很长时间没找到答案，看提示应该是系统还有什么进程在使用这块磁盘，后请教别人后发现该磁盘之前做过ceph的osd盘，虽然已经删除了对应的关联，但是并没有删除完全，必须将/var/lib/ceph/osd/下对应的ceph包删除。然后，格式化成功。 新建共享目录，mount 磁盘 nfs安装 先查看有没有安装 配置nfs共享文件路径并启动nfsserver. 修改cinder 配置文件 首先修改cinder配置文件cinder.conf，指定nfsshare文件路径(图中注释部分为之前的ceph配置) 添加nfsshare 文件，只需要指出nfs暴露的接口即可。 重启cinder-volume 发现cinder volume报错，发现是cinder对挂载的nfs没有权限，修改nfs挂载文件的user,group 为cinder，或者直接chmod 777 (不安全) 重启cinder-volume，成功。 查看mount验证下。 参考文章磁盘分区，格式化与检验 CentOS 6.5下NFS安装配置Configure an NFS storage back end 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[devops系列-- 运维利器SaltStalk介绍]]></title>
    <url>%2F2016%2F11%2F03%2F2016-11-3-intro-to-salt%2F</url>
    <content type="text"><![CDATA[SaltStalk是什么salt官网给出的介绍简洁明了： 一个配置管理系统，能够维护预定义状态的远程节点(比如，确保指定的报被安装，指定的服务在运行) 一个分布式远程执行系统，用来在远程节点（可以是单个节点，也可以是任意规则挑选出来的节点）上执行命令和查询数据。开发其的目的是为远程执行提供最好的解决方案，并使远程执行变得更好，更快，更简单。 其实，除了saltstack,还有其他的同类型软件供我们选择，比如puppet、chef、ansible、fabric等,关于其中的区别以及优劣势可以参考这里，Ansible vs. Chef vs. Fabric vs. Puppet vs. SaltStack稍微总结下：chef,puppet 是出现时间比较早的系统，对于比较重视成熟性和稳定性的公司来说比较合适。ansible与saltstack比较适合快速灵活的生产部署环境，且适合没有太多额外要求，生产环境操作系统一致的情况下。fabric比较适合规模比较小的环境，属于入门级别的配置管理系统。 这里有一个视频更细粒度的（可用性，互操作性，扩展性等）比较了一下，Chef vs Puppet vs Ansible vs SaltStack | Configuration Management Tools Comparison SaltStalk 安装安装比较简单，check this,不再赘述。 SaltStalk 配置saltstalk是典型的CS架构，主要分两种角色，master和minions,master负责minions的配置管理以及远程执行。一般来说，master的配置文件位于/etc/salt/master，minion的配置文件在相应机器的/etc/salt/minion,只需在minion配置文件中指定master指向即可运行。 master的配置项非常多，大致包括以下几项： 主要配置：包括网络接口，提供服务的端口，master id,最大打开文件数，工作线程个数，返回端口，缓存目录，各种缓存配置（minions数据缓存，作业缓存等） master 安全相关配置：主要是针对PKI验证的一些配置。 master模块管理相关配置 master state system 相关配置 pillar 相关配置 日志相关配置 windows软件源相关配置 minion的配置项跟master大致类似，不再赘述。 其他： 新版saltstack 中minion有一个minion_blackout的配置，该选项设为true后，该minion不会执行除saltutil.refresh_pillar之外的其他所有命令。 saltstack有一个访问控制系统对于非admin用户进行细粒度的访问控制。check this job管理器：可以实现对minion job的发送信号，定时调度等 job 返回数据管理：系统默认返回数据存储在Job cache（默认是在/var/cache/salt/master/jobs目录）中，还有其他两种存储模式：一种是External Job Cache，如下图 另一种是Master Job Cache，如下图： returner: 我们可以在master端执行远程指令时指定returner,这样minion返回的数据不仅会返回到master，也会返回到我们指定的returner。而且我们也可以按照要求实现一个returner，来替换我们的Default Job Cache。 SaltStalk 一些概念解释sls 文件salt state 文件的简称，saltstalk实现配置管理的核心部分，描述了系统的目标状态，一般遵循yaml格式，参考YAML 语言教程。 首先是配置管理的入口文件top.sls，默认在/srv/salt/目录下。 1234567891011base: # 默认环境变量 '*': # 通过正则进行匹配minion - apache # 需要自己写的state.sls模块名 my_group: #通过分组名去进行匹配 须定义match:nodegroup - match: nodegroup - nginx 'os:CentOs': #通过grains模块去匹配，须定义match:grain - match: grain - nginx 再看下需要自己写的webserver.sls文件，默认位于/srv/salt/apache.sls。当然，还有其他组织形式，常用的还有/srv/salt/apache/init.sls 形式。 1234567891011apache: # 标签定义 pkg: # 状态定义，这里使用（pkg state module） - installed # 安装nginx（yum安装） service.running: # 保持服务是启动状态 - enable: True - reload: True - require: - file: /etc/init.d/httpd - watch: #检测下面配置文件，有变动，立马执行上述/etc/init.d/httpd 命令reload操作 - file: /etc/apache/httpd.conf - pkg: nginx state 之间的逻辑关系一般有三种： require :依赖某个state，在运行此state前，先运行依赖的state，依赖可以有多个 watch :在某个state变化时运行此模块，watch除具备require功能外，还增了关注状态的功能. order：优先级比require和watch低，有order指定的state比没有order指定的优先级高 一般对某个minion执行具体state的时候，我们可以执行1$ salt minion-1 state.sls apache 当然，如果是执行所有的sts文件，则是如下命令： 1$ salt minion-1 state.highstate # test=True 参数可以测试安装，并不真正安装 grain vs pillar vs mine如果说sls文件是配置管理的骨架或框架的话，那么grain与pillar就是填充骨架的血与肉。其实他们就是saltstack自己定义的一些数据，通过这些数据定制我们的系统配置。 grains 存储在minion一端，包括minion自己生成的一些信息，比如操作系统，内存，磁盘信息，cpu架构等等。当然我们也可以自己定制minion 的grains信息。通常做法是在默认/etc/salt/grains中定义。 pillar 是存储 在master端，完全是用户自定义的一些动态数据。一般存储在master端/srv/pillar 目录下。组织形式类似salt state。 两者区别：一般来说，变化较少或不变的数据存储在grains中，一些敏感数据（如各种密码），易变数据，以及涉及到minion 配置的数据一般存储在pillar中。 mine 更像是以上两种形式的结合，它是定期从minions 收集的数据，传给master并存储在master端，这样，所有的minions都能获取到。mine数据是有时效性的，每隔一段时间便会更新。mine可以存储在mine配置文件中，不过更多的是将它放在pillar中，只需定义mine_functions 关键字即可。官网示例参考EXAMPLE 两者的常用操作如下： 12345678910111213$ salt '*' grains.items # 查看所有 grains键值对$ salt '*' grains.get os # 查看 grains中的 os对应的值$ salt '*' saltutil.sync_grains # 同步所有 grains数据$ salt '*' pillar.items # 查看所有 pillar键值对$ salt '*' pillar.get data # 查看 pillar中的 data对应的值$ salt '*' saltutil.refresh_pillar # 刷新所有 pilllar数据$ salt '*' state.apply my_sls_file pillar='&#123;"hello": "world"&#125;' # 命令行更改或增加pillar execution modules也就是运程执行时需要 minions执行的函数，通常salt已经封装好了大量的modules,可以通过 salt ‘*’ sys.doc 查看所有modules的doc。当然也可以自己写modules，下文会给出示例。 runnerrunner 本质是可定制化的Python 脚本，与execution modules 类似，不过是运行在master节点，可以使用salt-run 命令使用，非常方便（最新版的salt-run也可以运行modules 了，命令：salt-run salt.cmd test.ping ,）。直接举个栗子：首先指定runner脚本的位置，在master配置文件中指定：runner_dirs: [/srv/runner/]然后就可以在指定目录下写Python 脚本了。 minions.py ：1234567891011# Import salt modulesimport salt.clientdef up(): ''' Print a list of all of the minions that are up ''' client = salt.client.LocalClient(__opts__['conf_file']) minions = client.cmd('*', 'test.ping', timeout=1) for minion in sorted(minions): print minion 使用命令行： salt-run minions.up 即可调用。注：有两种模式，默认为同步模式，即有数据返回后才显示，还有一种异步模式，即立即返回不显示数据，可以指定returner将数据返回到指定容器中。 salt enginessalt engines 是一个利用salt 并独立长时间运行的进程。有以下特点： 可以获取到salt的各种配置项，modules以及runner 以独立进程运行，由salt监视，一旦挂掉后由salt负责重启。 可以运行在master和minion上。 SaltStalk 使用远程执行saltstack 最基本的功能之一，在master节点实现对minions节点的远程控制与执行，命令格式如下： 1$ salt '&lt;target&gt;' &lt;function&gt; [arguments] target 指定 minions,可以有多种方式匹配minions，正则匹配，group匹配，grains匹配，pillar匹配等。 function 就是前面讲的execution modules ，后面可以加参数。 下面介绍下execution modules 的定制开发。首先确定一下modules的位置，默认位于/srv/salt/_modules目录下，可以通过以下命令实现modules的同步。12345$ salt '*' saltutil.sync_modules$ salt '*' saltutil.sync_all$ salt '*' state.apply 通常modules的脚本会使用zip压缩文件，官网示例 脚本中可以调用salt本身的module以及调用grains数据。同时注意str 类型 与unicode的转换。 配置管理配置管理功能是在远程执行的基础上建立起来的，Salt 状态系统的核心是就是上面提到过的SLS文件。SLS表示系统将会是什么样的一种状态（比如安装什么服务，服务是否启动，服务配置等等），而且是以一种很简单的格式来包含这些数据。下面是一个写SLS文件的示例： 首先是在master配置文件中定义三个环境： 12345678910file_roots: base: - /srv/salt/prod qa: - /srv/salt/qa - /srv/salt/prod dev: - /srv/salt/dev - /srv/salt/qa - /srv/salt/prod 简单解释下：这里定义了三个环境，base环境就是生产环境，它只有一个salt根目录，qa环境可以获取到两个salt根目录下的配置文件，按照上下顺序优先采用 /srv/salt/qa 目录下的配置文件，dev环境同理。通常我们开发时，会首先在开发环境部署，新的SLS文件会存储在 /srv/salt/dev 环境中，然后将改变push到对应的开发机器。开发完成后，将新的SLS文件 复制到/srv/salt/qa 目录，push到对应的测试机器以供测试。最后测试完成后，才会把SLS文件复制到/srv/salt/prod 目录，push到生产环境的机器。 接下来开始编写SLS文件了，首先编写各环境根目录下的top文件,因为 /srv/salt/prod目录下的文件是所有环境都能获取到的,所以只需在该目录下编写top文件即可。 123456789101112base: 'roles:prod': - match: grain - apacheqa: 'roles:qa': - match: grain - apachedev: 'roles:dev': - match: grain - apache 这里我们是通过在 grain 中添加roles 键值对，然后通过grain进行匹配，当然也可以通过其他方法匹配（pillar,group，正则等）。apache是之后我们需要编写SLS文件，不过这在这之前，先在各minions 添加grain roles键值对。 在对应minions 中添加grain roles键值对，默认编辑/etc/salt/grains ,添加 roles: prod 即可。添加完后，master 执行 salt ‘*’ saltutil.sync_grains 同步命令。 接下来开始编写apache的sls 文件。 /srv/salt/prod/apache/init.sls12345678910111213apache: pkg.installed: - name: httpd service.running: - name: httpd - require: - pkg: apache/var/www/html/index.html: file.managed: - source: salt://apache/index.html - require: - pkg: apache index.html 是我们的一个测试网页，放在/srv/salt/prod/apache/目录下。 接下来在 master端执行 salt ‘*’ state.apply 命令 即可完成对应minions 端apache的安装，启动等工作。浏览器输入minion ip,可以查看到测试网页。 如果该过程中出现错误，多半是SLS文件写错了，可以查看 minion日志，如果日志不详细，可以先关掉salt-minion，然后运行 salt-minion -l debug ，再复现一次错误，便可查看到更详细日志。 event/reactorevent/reactor 是saltstack 中的两个系统。两者结合我们可以定制一些自动化的功能，比如，minion端的salt-minion服务重启后，master端立马同步grains，pillars信息。event系统是一个本地的ZeroMQ PUB接口, 用于产生salt events.这个event总线是一个开放的系统,用于发送给Salt和其他系统发送关于操作的通告信息.event系统产生event有一个严格的标准. 每一个event有一个 tag . event tags用于快速过滤events. 每一个event有一个数据结构附加在tag后. 这个数据结构是个字典, 包含关于本event的信息。reactor 系统会结合sls文件在master端去匹配event tags，匹配成功后执行相应的sls文件。 暂时还没有碰到此类需求，回头再看吧。 SaltStalk 架构关于架构部分，内容比较繁杂，通常会涉及到multimaster,multimaster with failover,salt-syndic等，后续有需求了再研究吧。 参考SALTSTACK ARCHITECTURE 参考文章saltstalk doc Saltstack SLS文件解读 Saltstack自动化（五）sls文件使用 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>saltstack</tag>
        <tag>devops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux shell系列-- 如何在系统中保证只有一个实例]]></title>
    <url>%2F2016%2F11%2F01%2F2016-11-1-keep-one-instance-in-a-machine%2F</url>
    <content type="text"><![CDATA[目录结构：如何在系统中保证只有一个实例 写 pid 方式 文件锁的方式 独占端口的方式 dbus API 写成service并交由第三方托管 如何在系统中保证只有一个实例从这发现的这个问题，如何确保系统中某个程序只存在一个实例 从评论区中汇总几种可行的方案并实验，记录如下，。 写 pid 方式这种方式应该是最直观，最容易想到的，类似于算法中的暴力破解法。原理就是在程序执行之前将PID写入某个约定好的文件，每次程序启动前通过检测该PID对应的程序是否存在来判断。它的伪代码如下： Invariant: File xxxxx will exist if and only if the program is running, and the contents of the file will contain the PID of that program. On startup: If file xxxxx exists: If there is a process with the PID contained in the file: Assume there is some instance of the program, and exit Else: Assume that the program terminated abnormally, and overwrite file xxxx with the PID of this program Else: Create file xxxx, and save the current PID to that file. On termination (typically registered via atexit): Delete file xxxxx 这种方式还要注意假如两个程序同时读取文件的时候这种情况，可以再加一个文件锁（其实就是下面的方式了）。 文件锁的方式对文件锁的概念其实一直比较陌生，但其实与我们还是挺接近的。我们数据库中经常出现锁表或者锁记录的情况，就是利用文件锁实现的。关于文件锁的理论知识可以参考如下一篇文章：Linux 2.6 中的文件锁 一个简单的demo如下： 123456789101112131415161718#!/usr/bin/env pythonimport fcntlimport os, timeFILE = "flag-file.txt"if not os.path.exists(FILE): file = open(FILE,"w") file.write(""+os.getpid()) file.close()file = open(FILE,"r+")try: fcntl.flock(file.fileno(),fcntl.LOCK_EX | fcntl.LOCK_NB) print "acquire lock, i am running" time.sleep(10) except IOError: print "another programme is running" 效果如下： 使用这种方式有个地方要注意：你不知道什么时候别人可能把你作为flag的文件给删除了。当然可以通过逻辑判断的方式来解决，但当程序比较复杂的时候可能还会出现意料外的问题。所以我们可以用端口独占的方式来避免这种情况。 独占端口的方式同上，通过独占端口的方式来达到互斥的效果，demo如下： 1234567891011121314151617#!/usr/bin/env pythonimport socketimport os, timePORT = 888def check_port(ip,port): s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) try: s.bind((ip,int(port))) print "port is open,i'm running" time.sleep(10) except: print "another programme is running"if __name__ == '__main__': check_port("127.0.0.1",PORT) dbus API首先简单介绍下什么是 dbus,直接引用wiki： D-Bus是一个进程间通信及远程过程调用机制，可以让多个不同的计算机程序（即进程）在同一台电脑上同时进行通信[4]。D-Bus作为freedesktop.org项目的一部分，其设计目的是使Linux桌面环境（如GNOME与KDE等）提供的服务标准化。 dbus 的Python库主要有几下几个 1,GDbus and QtDbus are wrappers over the C/C++ APIs of GLib and Qt 2,pydbus is a modern, pythonic API with the goal of being as invisible a layer between the exported API and its user as possible 3,dbus-python is a legacy API, built with a deprecated dbus-glib library, and involving a lot of type-guessing (despite “explicit is better than implicit” and “resist the temptation to guess”). 4,txdbus is a native Python implementation of the D-Bus protocol for the Twisted networking framework. demo代码如下，因为对dbus这一块不熟，好像有个地方参数不对，后续再调吧 12345678910111213141516171819202122#!/usr/bin/env pythonimport dbusimport dbus.serviceimport dbus.mainloop.glibimport os, timedef check_dbus(bus_name): dbus.mainloop.glib.DBusGMainLoop(set_as_default = True) #bus = dbus.SessionBus() # s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) try: my_bus_name = dbus.service.BusName(bus_name, bus, allow_replacement = False, replace_existing = True, do_not_queue = True) #s.bind((ip,int(port))) print "i'm running" time.sleep(10) except: print "another programme is running"if __name__ == '__main__': check_dbus("name") 写成service并交由第三方托管这个就没什么好说的了，将Python文件打包并执行，由supevisor等第三方工具来管理。 参考文章v2ex stackoverflow-Preventing multiple process instances on Linux Linux 编程中的文件锁之 flock Linux 2.6 中的文件锁 Python wiki 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列-- benchmarking工具rally]]></title>
    <url>%2F2016%2F10%2F31%2Fopenstack-rally%2F</url>
    <content type="text"><![CDATA[目录结构：ralley是什么 rally 安装以及快速引导 rally 任务启动配置文件 rally 插件 ralley是什么一句话概括，rally是一个测试openstack 性能的工具。它的存在主要回答这样一个问题：How does OpenStack work at scale? 意译一下就是：openstack 在负载比较大的规模下运转情况怎么样？rally以一种插件式的形式工作，对openstack是无侵入的。通常会作为CI的一种功能集成到CI中去。 tips:随着rally项目的不断发展，rally又衍生出了其他的一些功能，比如与tempest结合进行openstack的功能测试等。如下图 deploy(部署)功能并非是另一种部署方式，只是与devstack等部署方式以插件形式结合来简化工作而已。 rally 安装以及快速引导rally 安装比较简单，还可以跟Devstack 部署时一块安装，以及docker 安装check this! 安装完成之后我们就可以进行性能测试了，大致顺序如下： 创建一个openstack 的rally测试环境：如果是有现成的openrc文件，那么直接source 一下，然后执行： rally deployment create –fromenv –name=existing 就创建完成了。也可以将openrc文件中的内容写入一个json文件，假设取名为existing.json,执行如下命令： rally deployment create –file=existing.json –name=existing 。 执行rally deployment list 即可查看我们创建的deployment 以及哪一个正在被激活（正在使用）。通过 rally deployment check 可查看检测到的openstack服务： 创建完一个deployment之后我们就可以进行测试了，我们需要指定一个json或yaml文件 来说明测试的内容， 在rally安装目录samples/tasks/scenarios 中有很多示例文件，比如samples/tasks/scenarios/nova 目录下就有nova对应的情景测试文件，例如boot-and-delete.json，就是启动虚拟机再删除操作，内容如下： 1234567891011121314151617181920212223242526&#123; "NovaServers.boot_and_delete_server": [ &#123; "args": &#123; "flavor": &#123; "name": "m1.tiny" &#125;, "image": &#123; "name": "^cirros.*uec$" &#125;, "force_delete": false &#125;, "runner": &#123; "type": "constant", "times": 10, "concurrency": 2 &#125;, "context": &#123; "users": &#123; "tenants": 3, "users_per_tenant": 2 &#125; &#125; &#125; ]&#125; 利用此文件启动任务： rally task start samples/tasks/scenarios/nova/boot-and-delete.json还可以利用rally查看images ，flavors列表等。 rally提供了多种方式进行结果的查看，最直观的就是以html形式在浏览器中展现（需翻墙）：rally task report –out=report1.html –open tips:执行任务时指定的json 文件格式如下： 1234&#123; "&lt;ScenarioName1&gt;": [&lt;benchmark_config&gt;, &lt;benchmark_config2&gt;, ...] "&lt;ScenarioName2&gt;": [&lt;benchmark_config&gt;, ...]&#125; 其中， &lt;benchmark_config&gt; 格式如下： 1234567&#123; "args": &#123; &lt;scenario-specific arguments&gt; &#125;, "runner": &#123; &lt;type of the runner and its specific parameters&gt; &#125;, "context": &#123; &lt;contexts needed for this scenario&gt; &#125;, "sla": &#123; &lt;different SLA configs&gt; &#125;&#125; rally 任务启动配置文件rally任务启动配置文件就是上文中提到过的json/yaml文件，其实可以通过此配置文件实现很多功能，比如：多任务配置，设置SLA(Service-Level Agreement,就是一个成功与否的基准，例如：max_seconds_per_iteration”: 10)，利用jinja模板传参数进来，或利用jinja模板进行简单的逻辑控制等，check this,不再赘述。 rally 插件rally插件就是我们在json文件中配置的ScenarioName，通过这些插件的组合可以完成我们自己定义的要求。几个命令如下： 关于rally与tempest的结合使用还不是很成熟，感兴趣的可以去试下，check this! 参考文章openstack rally wiki SDN使用 Rally 来实现 Openstack Tempest 测试 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列 --关于keystone配置详解]]></title>
    <url>%2F2016%2F10%2F24%2Fopenstack-keystone-config%2F</url>
    <content type="text"><![CDATA[目录结构keystone 配置文件综述 keystone 配置文件详解 keystone 配置文件综述关于keystone的概述以及整体架构我们已经在之前的一篇文章中做过介绍，不再赘述。这里具体介绍下keystone的配置方面的知识。 首先，keystone的配置文件主要有两个，一个是通常位于/etc/keystone/或/etc/目录下的keystone.conf，另一个是位于keystone安装根目录下的keystone-paste.ini。其中，后者其实就是paste deploy 文件，我们这里不需要关注，主要讨论keystone.conf文件。 tips:keystone默认端口是35357，因此我们最好将35357端口从临时端口范围中删除，以免该端口被用作临时端口。命令如下： sysctl -w ‘net.ipv4.ip_local_reserved_ports=35357’ 如果是想重启后仍然有效，在/etc/sysctl.conf 或/etc/sysctl.d/keystone.conf文件后追加： net.ipv4.ip_local_reserved_ports = 35357 keystone 配置文件详解keystone.conf文件中所有的配置项如下： 接下来我们从其中挑选几个值得注意的配置项详细说明下： 关于TOKEN 对于token的持久化有两种形式：一种是以key/value对的形式存储，另一种是存储在SQL数据库中。需要在[token] 配置项中的driver选项进行具体配置。 token provider 大致有以下四种形式： UUID:长度固定为 32 Byte 的随机字符串,UUID token 简单美观,验证流程如下： 由于每当 OpenStack API 收到用户请求，都需要向 Keystone 验证该 token 是否有效。随着集群规模的扩大，Keystone 需处理大量验证 token 的请求，在高并发下容易出现性能问题。为了杜绝keystone成为瓶颈，引出了下面的几种。 PKI:验证流程如下：可以看出，PKI token携带更多用户信息的同时还附上了数字签名，以支持本地认证，从而避免了步骤 4。因为 PKI token 携带了更多的信息，这些信息就包括 service catalog，随着 OpenStack 的 Region 数增多，service catalog 携带的 endpoint 数量越多，PKI token 也相应增大，很容易超出 HTTP Server 允许的最大 HTTP Header(默认为 8 KB)，导致 HTTP 请求失败。 PKIZ:顾名思义，就是对PKI token进行压缩，但压缩效果有限，无法良好的处理 token size 过大问题。原理同上，无须赘述。 FERNET:前三种 token 都会持久性存于数据库，与日俱增积累的大量 token 引起数据库性能下降，所以用户需经常清理数据库的 token（ 命令为：keystone-manage token_flush）。为了避免该问题，社区提出了 Fernet token，它携带了少量的用户信息，大小约为 255 Byte，采用了对称加密，无需存于数据库中。 几种token provider比较如下： Caching Layer 启动keystone的的cache功能，首先要在[cache]配置项，设置为enabled,指定backend等等。然后再在其他具体配置项进行设置。支持cache的地方有以下几个地方： token resource role具体配置根据backend的不同也不尽相同。 Service Catalogkeystone 提供了两种配置选项。 SQL-based Service Catalog：配置选项如下： 12[catalog]driver = sql 可以通过查阅以下命令进行相关操作： 123openstack --helpopenstack help service createopenstack help endpoint create File-based Service Catalog (templated.Catalog)：就是基于一个配置模板文件，配置选项如下： 123[catalog]driver = templatedtemplate_file = /opt/stack/keystone/etc/default_catalog.templates 模板文件示例： 其他 其实，除了sql数据库，我们也可以把数据存在文件中，以 LDAP形式，不过用的不多，不再赘述。 参考文章openstack doc 理解 Keystone 的四种 Token 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git系列--关于github 的碎碎念]]></title>
    <url>%2F2016%2F10%2F17%2Fgithub-tips%2F</url>
    <content type="text"><![CDATA[目录结构github 快捷键 github trending &amp;&amp; github subscribe 为开源项目贡献代码 git 常用命令 github 快捷键逛v2ex的时候逛到了这个帖子，录了几个有关 Github 的视频，我觉得你也应该知道这些,很有意思,把视频看了一遍，都不是很长，强烈推荐。顺手记录下这些信息。首先是github的一些快捷键： 项目中搜索含有某个关键字的文件：在github 项目的code目录按下 “t”,即可进入搜索页面，如下图： 列出github的快捷键：在项目目录按下 “？” 即可出现： 进入 issue页面：按下“g” 与 “i”键，就是goto issue 的缩写 进入code页面：按下“g”与“c”键，goto code 其他的按下“？”自己探索吧。 github trending &amp;&amp; github subscribe github trending:进入这个页面github trending就可以看到最近按照stars数排序靠前的比较优秀的项目，可以按照语言分类。 github subscribe:进入这个页面github subscribe,我们可以订阅我们关注的大牛，他们的动态会按周/月发送到你的邮箱。 为开源项目贡献代码关于这个，强烈推荐看下视频吧，直观易懂。这里简短记录下过程： fork and clone github remote -add upstream ssh@xxxxxxxx.git :标示我们fork的上级，master分支要与upstream分支保持一致 github checkout -b feature/bug-fixed : 创建并切换分支，在此分支上编码。 git add &amp;&amp; commit git checkout master &amp;&amp; git pull upstream master：在push之前我们先要保证master与upstream一致。 git checkout feature/bug-fixed &amp;&amp; git rebase master：将在bug-fixed上的commit log 打到master分支的最上面。 git push origin featue/bug-fixed git pull request: 现在可以向开源作者pull request了 git 常用命令 切换分支后，清理本地目录：git reset –hard HEAD &amp;&amp; git clean -i 不详细写了，以后用得着直接去这个页面找吧：git-tips 参考文章v2ex git-tips 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph系列--ceph存储介绍]]></title>
    <url>%2F2016%2F09%2F29%2Fceph-intro%2F</url>
    <content type="text"><![CDATA[目录结构：关于分布式存储 ceph 是什么 ceph 架构 what makes ceph unique 关于分布式存储在了解ceph之前，我们最好还是先了解一下分布式存储的一些概念。分布式存储是随着存储容量的需求变大而出现的概念。由于存储需求变大，出现了两种解决方案，一种是scale up,采用容量更大，价格更昂贵的存储机器。 另一种是scale out,利用众多的普通存储机器（PC机）横向扩展成一个大的存储集群。 第二种就是分布式存储的概念了。相对采用昂贵的专用商业存储设备来说，这种存储的性价比更高，但也因此引出了分布式存储几个比较棘手的问题，比如数据如何组织存储，如何保证高可用性，如何保证冗余数据的一致性，是否支持分布式事务，如何避免单点失败等等，这些概念可以参考这两篇文章，分布式存储系统 知识体系,An Introduction to Distributed Systems。 ceph 是什么以下篇目是基于这场presentation，Ceph Intro &amp; Architectural Overview. 我们首先看一下ceph 的设计哲学： 由此我们也可以看出ceph的一些特点：开源，社区驱动，可扩展，无单点故障，软件层面，自我管理。 那具体提供什么服务呢？看下图 对应过来就是对象存储，块存储，文件系统存储。 ceph 架构再从技术层面看下ceph架构 我们从底往上一层层的介绍，首先是RADOS,我们从上图的介绍中也可以看出，RADOS是一系列的节点，这些节点上跑着两种程序，跑着OSD程序的我们称为OSD节点（storage node），跑着MON程序的我们称为MON节点（monitor node）。 我们再详细看下OSD节点。 每个OSD节点内，有多个磁盘disk,这些磁盘上是对应的文件系统（官网推荐 xfs），在往上就是我们的逻辑存储单元OSD，每个磁盘对应一个OSD。我们的数据就是存储在OSD里。OSD与MON节点各自功能如下： 可以看出MON节点只负责集群状态的维护，具体存储交给OSD处理。 再看下LIBRADOS的作用。 简而言之，就是为Application的调用提供了多种语言版本的接口。通信机制为SOCKET。 再往上是RADOSGW. 简而言之，就是在LIBRADOS提供的API的基础上进行封装对外提供对象存储的REST服务。 与之并列的是RBD，也就是ceph提供的块存储服务。我们最常用的就是attach到某台虚拟机上（如下图），Ross Turk还提到了另外两种用法，感兴趣可以去看一下。 关于ceph提供的文件系统因为用的不多，不再详述。 what makes ceph unique第一个就是crush算法，一种计算寻址而非查找寻址的方法。 图中的placement group 有很重要的作用，有了它，我们底层添加节点或节点崩溃，ceph都会自动更新，而对用户是透明的。 其次是 rbdlayering ,参考这里RBD LAYERING 参考文章Ceph Intro &amp; Architectural Overview Ceph Intro and Architectural Overview by Ross Turk 理解 OpenStack + Ceph （2）：Ceph 的物理和逻辑结构 [Ceph Architecture] ceph doc本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python系列--Python中的生成器以及协程的相关知识]]></title>
    <url>%2F2016%2F09%2F26%2Fpython-yield-and%20coroutine%2F</url>
    <content type="text"><![CDATA[目录结构：迭代器与生成器 yield, send 关键字解析 协程的一些知识 迭代器与生成器在说生成器之前我们得先讨论一下迭代器的概念。 我们知道，对于一个可迭代的对象，我们可以通过 for in 的语句来进行迭代输出。这个可迭代的对象可以是一个list，string，文件等。其实我们深入这个可迭代的对象会发现，这些可迭代对象都实现了两个函数：iter() 和 next()。在我们调用list等这些可迭代对象的时候，需要把整个list数据全部读到内存里。这样就存在一个问题：list数据量小还可以，一旦变得特别大，内存就有可能被占满而导致运行缓慢甚至崩溃。这个时候我们想到，能不能只把我们需要的那个数据读入内存，调用next()的时候再把下一个数据读入内存呢。这就是生成器了。 一般来说，含有 yield 语句的函数就可叫做生成器。 1234567891011def g(): yield 0 yield 1 yield 2ge = g()type(ge) #ge 的类型为 'generator'dir(ge) #列出ge包含的函数我们发现有__iter__() 和 next()ge.next()ge.next()ge.next() 生成器也是一种迭代器，但我们只可以读取它一次，因为它并非把所有数据都存入内存中，而是实时地生成数据。上述例子当我们再次读取ge.next()后就会报错。 此处再略提一下生成器推导式：我们知道Python中有各种集合的推导式，如下： 列表推导式：my_list = [ f(x) for x in sequence if cond(x) ] 字典推导式：my_dict = { k(x): v(x) for x in sequence if cond(x) } 集合推导式：my_set = { f(x) for x in sequence if cond(x) } 相应的，我们也可以用推导式来生成生成器，跟列表推导式类似，只需要将[]改为()：my_generator = ( f(x) for x in sequence if cond(x) ) yield, send 关键字解析由上文可以看到，yield关键字是理解生成器的关键。那么yield什么意思呢？我们来详细解释一下。yield 关键字有点类似我们平常写程序用到的return。但是程序运行到return的时候就会返回，运行完毕。而运行到yield的时候也会返回，但会保存上下文之后再返回，等到下次再次唤醒该程序的时候恢复上下文，继续从此运行，直到碰到下一次yield。 123mygenerator = (x*x for x in range(3))for i in mygenerator : print(i) #输出0,1,4 首先我们创建了一个生成器mygenerator，注意，此时程序并不执行，只是创建了一个生成器。接着是一个for循环,fou循环其实就是执行了一个next()函数，此时进入生成器获取第一个i就是0乘以0，也就是0。返回并输出。再次循环，进入生成器，以此类推。这个例子可能不是特别直接，只是说明一下运行的逻辑顺序，下文中我们有一个更为直接的实例。 我们先看一下send()函数再将两个函数放在一个例子中说明一下。 在调用生成器时，除了next()方法，我们还可以用send()方法唤醒生成器，而且send(args),在唤醒生成器的同时会把参数 args传给指定的数据。如下例： 1234567891011121314def f(): #定义一个生成器函数 while True: val = yield # val接受send()传过来的的参数并赋值 yield val*10 g = f() #新建一个生成器g.next() #触发生成器，生成器执行到val=yield ，保存上下文退出，等待传值g.send(1) # 触发生成器，生成器继续执行，赋值val = 1，执行到yield val*10，保存上下文，,返回10，退出。g.next() #同上g.send(10)g.next()g.send(0.5) 注意，我们在用send()之前必须保证生成器已经执行到yield,也就是说生成器已经被触发过一次，我们可以用send(none)来实现。其实，next()跟send(none)效果是一样的。 接下来我们看一个比较复杂的例子： 1234567891011121314151617181920212223242526#!/usr/bin/env python# -*- coding: utf-8 -*-# @Date : 2016-09-22 16:40:36# @Author : Zhang Chen (pekingzcc@gmail.com)# @Link : zhangchenchen.github.ioimport osdef countdown(n): print "Into genaration :counting down from", n while n &gt;= 0: print "Genaration: ###loops from here###" newvalue = (yield n) print "Gerenation: newvalue is",newvalue # If a new value got sent in, reset n with it if newvalue is not None: n = newvalue else: n -= 1if __name__=='__main__': c = countdown(5) print "Main function begin from here " for x in c: print "Main function: x is", x if x == 5: c.send(3) 这个例子看明白了，yield与send的用法就理解的差不多了。我们首先看一下执行结果： 接下来分析一下：我们从main 函数开始，先创建了一个生成器（此时并未执行），接着输出main函数开始的语句。接着进入for循环，注意，遇到for循环就相当于执行了一次next(),所以进入生成器输出“Into genaration :counting down from 5”，继续运行，进入生成器的for循环，输出“Genaration: ###loops from here###”，继续往下，碰到yield n ，保存上下文，退出并返回n(此时是5)到main函数，主函数输出“Main function: x is 5”，进入条件语句，c.send(5)触发生成器，再次进入生成器，赋值newvalue为3，接着输出“Gerenation: newvalue is 3” ，赋值n = newvalue =3 ,继续循环输出“Genaration: ###loops from here###”，碰到yield n ,返回main函数，注意，此时main函数又进入到for循环，所以再次进入生成器，但是没有send()数据传过来，也可以理解为send(none),所以输出“Gerenation: newvalue is none”,接着执行n-1 ,n变为2。继续循环，输出“Genaration: ###loops from here###”。。。。。 协程的一些知识在了解协程之前，我们需要先从进程，线程说起。我们知道，进程的出现是为了并发，在一台机器上同时运行多个程序（当然，内部实现可能是多CPU并行，也有可能是单CPU时间分片，但在外部看来就是多个程序一起运行），进程的切换需要陷入内核，由OS来进行切换。一切换进程得反复进入内核，置换掉一大堆状态，这样，进程数一高，就会吃掉很多的系统资源。为了解决这个问题，就出现了线程的概念。 一个进程里可以有多个线程，这样就能处理多个逻辑，当某个线程阻塞的时候，可以切换线程到另一个线程。因为线程是共享附属进程的资源的，它们件的切换相对要比进程间的切换消耗的资源要少很多。但之后，问题又来了，操作系统为了程序运行的高效性每个线程都有自己缓存Cache等等数据，操作系统还需要做这些数据的恢复操作，所以线程多了之后，它们之间的切换也非常耗性能。 协程的概念就来了，既然线程切换费资源，那我们干脆自己做逻辑流的切换，不用交给OS处理。注意，这里经常提到的是切换，具体到应用场景就是IO密集型场景。在cpu密集型的场景中协程的意义就没有那么大了。在IO处理时，我们有同步，异步两种处理方式，关于IO模型，可以check this 。在异步处理IO时，我们需要写回调函数来实现异步，这种写法是比较反人类的，可读性比较差。协程可以很好解决这个问题。比如 把一个IO操作 写成一个协程。当触发IO操作的时候就自动让出CPU给其他协程。协程的切换很轻，消耗资源少。协程通过这种对异步IO的封装既保留了性能也保证了代码的 容易编写和可读性。 其实协程不是一个新生的事物，它在很早之前就出现了，只不过最近因为在一些动态语言的世界里大放异彩。对其历史感兴趣的check this。 再简单说一下Python中的协程，其实Python中的协程跟yield关键字是分不开的。只要含有yield的函数都可以认为是一个协程，利用协程实现的生产者消费者如下： 12345678910111213141516171819202122232425262728293031323334import random def get_data(): """返回0到9之间的3个随机数，模拟异步操作""" return random.sample(range(10), 3) def consume(): """显示每次传入的整数列表的动态平均值""" running_sum = 0 data_items_seen = 0 while True: print('Waiting to consume') data = yield data_items_seen += len(data) running_sum += sum(data) print('Consumed, the running average is &#123;&#125;'.format(running_sum / float(data_items_seen))) def produce(consumer): """产生序列集合，传递给消费函数（consumer）""" while True: data = get_data() print('Produced &#123;&#125;'.format(data)) consumer.send(data) yield if __name__ == '__main__': consumer = consume() consumer.send(None) producer = produce(consumer) for _ in range(10): print('Producing...') next(producer) 利用协程的Python库比较常见的是Greenlet库，它是以C扩展模块形式接入Python的轻量级协程，将一些原本同步运行的网络库以mockey_patch的方式进行了重写。Greenlets全部运行在主程序操作系统进程的内部，它们被协作式地调度。 参考文章A Curious Course on Coroutines and Concurrency 提高你的Python: 解释‘yield’和‘Generators（生成器） (译)Python关键字yield的解释(stackoverflow) Python 中的进程、线程、协程、同步、异步、回调 什么是Python中的生成器推导式？ 生成器 聊聊 Python 中生成器和协程那点事儿 GENERATOR.SEND() WITH YIELD 了解协程（Coroutine）/) 协程的好处是什么？ 为什么觉得协程是趋势？ 编程珠玑番外篇-Q 协程的历史，现在和未来 利用python yielding创建协程将异步编程同步化 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列 --keystone]]></title>
    <url>%2F2016%2F09%2F16%2Fopenstack-keystone%2F</url>
    <content type="text"><![CDATA[目录结构keystone 是什么 keystone 的架构 token 的生成方式 可信计算部分介绍 keystone 是什么keystone 是 openstack 的认证服务模块（identy service）。 nova,glance,swift,cinder等其他服务通过keystone注册其服务的endpoint，针对这些服务的任何调用都需要经过keystone的身份认证，并获得服务的endpoint进行访问。 keystone 提供的服务可以概括为以下四个方面： Identity:对用户身份进行验证。用户的身份凭证通常是用户名和密码。 Token:Identity确认完用户身份后，会给用户提供一个token以请求后续的资源。而keystone也会提供针对token的验证。token大致有两类：一类是与Tenant(也就是project)无关的token，通过这个token，可以向keystone获取Tenant列表，用户选择要访问的Tenant,然后可以获取与该Tenant绑定的token,只有通过与某个特定Tenant绑定的token才能访问此Tenant中的资源。token有自己的过期时间，如果删除某个用户的访问权限，只要删除对应token即可。 Catalog:Catalog服务对外提供一个服务的查询目录，即可访问的endpoint列表。 Policy:一个基于规则的身份验证引擎。通过配置文件来定义各种动作与用户角色的匹配关系。该部分已作为Oslo的一部分进行开发维护。 以创建虚拟机为例，keystone 的大致工作流程如下(注：以下内容摘自&lt;&lt;openstack设计与实现&gt;&gt;)： 用户Alice发送自己的凭证到keystone，keystone认证通过后，返回给Alice一个token以及服务目录。 Alice通过token请求keystone查询他所拥有的Tenant列表。（如果已经知道Tenant，略过以上两步） Alice选择一个Tenant，发送自己的凭证给keystone申请token，keystone验证后，返回token2。 Alice选择endpoint并发送token2请求创建虚拟机，keystone验证token2(包括该token是否有效，是否有权限创建虚拟机等)成功后，把请求转发给Nova，并创建虚拟机。 keystone 的架构 keystone还涉及另外一个子项目，keystonemiddleware,它提供了对token合法性进行验证的中间件。比如：客户端访问keystone提供的资源时用的是PKI类型的token，为了不必每次都需要keystone介入token的验证，我们通常会在本地节点上缓存相关证书和密钥，利用keystonemiddleware对token进行验证。 Keystone项目本身，除了后台的数据库，主要包括了一个处理restful请求的API服务进程。这些API涵盖了Identity,Token,Catalog,Policy等服务。这些不同的服务提供的功能则分别由后端Driver实现。 token 的生成方式在openstack F版本之前，token的生成方式只有UUID这一种方式（即由程序随机生成一段序列），但是在大规模的集群中，大量客户端并发请求的情况下，keystone的性能存在瓶颈（该版本还未引入keystonemiddleware）,所有的请求都要跟keystone交互。于是PKI系统在随后的版本中引入，就可以做到本地检验而不用与keystone频繁交互。 PKI详细介绍，check this!此处简单介绍几个概念： Certificate Auth：即CA,认证中心，数字签证的签发机构，是PKI应用中权威的，可信任的第三方机构。 CA私钥：CA签发的非对称加密算法中的私钥。 CA公钥证书：包含CA的公钥信息。 签名私钥 签名公钥证书 下图为UUID的token验证流程： 下图为PKI的token验证流程： 可信计算部分介绍在云计算环境中，可能会有成千上万个计算节点部署在不同的地方，可能有些云租户对安全的要求比较高，要求应用或虚拟机必须运行在验证为可信的节点上。openstack 在E版本时为此引入了可信计算池的概念。可信计算池的实现位于Nova项目。在FilterScheduler中加入了一个新的TrustedFilter,经过该filter的即认为是可信计算的节点。 那么可信节点是如何判定的呢？计算节点采用基于Intel TXT 的TBoot进行可信启动，对主机的BIOS,VMM和操作系统进行完整性度量，并在得到来自认证服务的请求时将度量数据发送给认证服务。认证服务器部署基于OpenAttestation的认证服务，通过将来自主机的度量值与白名单数据库进行比对确定主机的可信状态。 参考文章openstack 设计与实现 Understanding OpenStack Authentication: Keystone PKI 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安全系列 --简单加固 ubuntu 服务器]]></title>
    <url>%2F2016%2F09%2F16%2Fsecure-your-ubuntu-server%2F</url>
    <content type="text"><![CDATA[目录结构前言 花五分钟的时间加固 ubuntu server 其他 前言逛v2ex的时候看到了一篇讨论 在服务器上做什么加固策略的帖子，想起自己之前看过的一篇文章，My First 5 Minutes On A Server; Or, Essential Security for Linux Servers，顺手写下这篇文章，权当记录，本篇的绝大部分内容都是出自这篇文章，强烈推荐，如果觉得TLDR,就接着往下翻吧。 安全，在大部分程序员潜意识里总是处于一种low priority的状态（参看此知乎链接）。比如，我们搭建一个web server，会在第一时间去想用什么去部署，去实现，实现之后可能还会考虑高可用，可扩展等因素。而安全却总是会在最后才出现在我们的脑海里，甚至于直到出现安全事故才去关注。其实，我们拿到一台Linux服务器，只需要花些时间做一些安全方面的考量，做一些简单的措施就可以抵挡很大一部分攻击，提高攻击门槛。 那对于安全方面的措施，我比较推崇上文作者所说：简单有效。因为比较复杂的安全策略实施起来的增量收益可能会低于增加的大量人力物力资源。而且指不定还会引入其他的安全隐患。引用作者的话说：据我的经验来讲，大部分安全事故，要么是没有做足够的安全措施，要么是做了足够的安全措施，但没有更新维护导致出现漏洞。 Simplicity is the heart of good security. 下面就简单说下我们拿到一台服务器（或VPS）后需要做的一些安全加固措施。 花五分钟的时间加固 ubuntu server准备本文中用的Linux发行版为 14.04。其他实验环境作者没有实践。首先更改你的 root 密码为强类型密码。然后执行如下命令做一下升级： 12apt-get updateapt-get upgrade 尽量不要使用root用户新增一个用户，当需要root权限时再用sudo暂时提升权限。新建用户的密码也要是强类型密码。 12adduser example_user #新建 用户adduser example_user sudo #给新建用户添加sudo权限 使用SSH密钥认证登录利用ssh登录到Linux服务器有很多种方案，比较常用的就是用户名密码登录和SSH密钥认证的方式。相对用户名密码登陆方式，后者是一种更安全的方案，因为前者容易受到暴力破解的威胁，而后者用暴力破解的方法不太实际。首先需要在客户端产生公约密钥对。Linux下是如下命令： 1ssh-keygen -t rsa 如果客户端是windows，可以利用putty等客户端实现。 创建完之后，我们会获取到两个文件，id_rsa.pub，和 id_rsa。也就是公钥文件和密钥文件。 接下来将公钥内容复制到服务器.ssh 目录下authorized_keys文件中。 关闭SSH 密码服务器登陆添加如下内容到 /etc/ssh/sshd_config 文件中。 123PermitRootLogin noPasswordAuthentication noAllowUsers deploy@(your-ip) deploy@(another-ip-if-any) # 添加允许登录IP 重启ssh服务 1service ssh restart 添加防火墙添加防火墙有很多种方法，最简单的就是利用ubuntu 提供的ufw。1234ufw allow from &#123;your-ip&#125; to any port 22 #限定IP的端口ufw allow 80 #打开80端口ufw allow 443 #打开443端口ufw enable 当然要根据你的实际要求来打开对应的端口。除了ufw，还有iptables,ip6tables,以及最新的nftables等都可以实现防火墙。 安装Fail2ban1apt-get install fail2ban Fail2ban 会在服务端扫描日志并根据一些策略屏蔽一些有可疑行为的用户（IP）。官方介绍说的比较清楚： Fail2ban scans log files (e.g. /var/log/apache/error_log) and bans IPs that show the malicious signs – too many password failures, seeking for exploits, etc. Generally Fail2Ban is then used to update firewall rules to reject the IP addresses for a specified amount of time, although any arbitrary other action (e.g. sending an email) could also be configured. Out of the box Fail2Ban comes with filters for various services (apache, courier, ssh, etc). Fail2Ban is able to reduce the rate of incorrect authentications attempts however it cannot eliminate the risk that weak authentication presents. Configure services to use only two factor or public/private authentication mechanisms if you really want to protect services. 设置安全自动升级123apt-get install unattended-upgradesvim /etc/apt/apt.conf.d/10periodic 更新如下内容： 1234APT::Periodic::Update-Package-Lists "1";APT::Periodic::Download-Upgradeable-Packages "1";APT::Periodic::AutocleanInterval "7";APT::Periodic::Unattended-Upgrade "1"; 再更新下这个文件/etc/apt/apt.conf.d/50unattended-upgrades 1234 Unattended-Upgrade::Allowed-Origins &#123; "Ubuntu lucid-security";// "Ubuntu lucid-updates";&#125;; 安装LogwatchLogwatch 是一个强大的日志解析和分析软件。它被设计用来给出一台服务器上所有的活动报告，可以以命令行的形式输出，或邮件发送。 123apt-get install logwatchvim /etc/cron.daily/00logwatch 添加如下内容： 1/usr/sbin/logwatch --output mail --mailto test@gmail.com --detail high 其他其实原文在hacking news 上引发了一系列的讨论。比如有人就提出，我们拿到一台服务器的时候，第一件事不是加固，而是应该先安装配置管理工具（Puppet or Chef），利用配置管理工具才更有条理。其实lz认为还是看实际场景吧，配置管理工具确实方便，尤其是对于一些大型的项目，而对于只搭一个blog的vps，确实没有必要。 参考文章My First 5 Minutes On A Server; Or, Essential Security for Linux Servers Securing Your Server Hacking news v2ex 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack 系列 --cinder]]></title>
    <url>%2F2016%2F09%2F06%2Fopenstack-cinder%2F</url>
    <content type="text"><![CDATA[目录结构在学习一个新东西的时候，我们其实默认是带着问题去学习的。但当我们真正在卷帙繁多的互联网或者书籍里获取信息时。之前的问题往往就淡化了，反而更多的是跟着资料所给的思路去学习。这样有好处也有坏处，好处是我们不用费心去思考，而坏处也是如此，不用思考带来的后果就是这些信息虽然当时我们理解了，但很难在我们的脑海中建立知识图谱，比较容易遗忘。 所以以后我会在写文章的时候先把问题具象化，从自己问题的角度出发去寻找答案. 本篇我们的主题是openstack的块存储项目cinder,那么第一个问题自然是 cinder是干什么用的？ 由这个问题出发，我们逐渐引出 cinder如何实现？ ，cinder的具体功能 。 cinder是干什么用的 cinder如何实现 cinder的具体功能 cinder是干什么用的openstack中创建虚拟机的时候是附带一块硬盘的，但这块硬盘随着虚拟机的destroy也相应的删除了。所以，openstack推出了两种应付持久存储的解决方案，一个是之前我们提过的对象存储swift，一个就是现在我们讲的块存储cinder。块存储,对象存储的概念我们之前已介绍过，不再赘述。需要说明的是，cinder并不是新开发的一个块设备存储系统，它更像是一个资源管理系统，对不同的存储后端进行封装，以统一的API形式向虚拟机提供持久块存储资源。对于不同的存储后端，它采用插件的形式，结合不同的后端存储的驱动提供块存储服务。 cinder如何实现首先看一下cinder的主要组件：简单介绍下这几个组件： cinder-api: 负责接受和处理外界的API请求，并将请求放入RabbitMQ队列，交由其他程序执行。 cinder-scheduler: 在多个存储节点的情况下，新建或迁移volume的时候由该程序依据指定的算法选出合适的节点。算法有过滤算法和权重算法两种。 cinder-volume： 运行在存储节点上，管理存储空间，处理cinder数据库维护状态的读写请求，通过消息队列和直接在块存储设备或软件上与其他进程交互。每个存储节点都有一个Volume Service，若干个这样的存储节点联合起来可以构成一个存储资源池。cinder-volume为后端的volume provider 定义了统一的 driver 接口，volume provider 只需要实现这些接口，就可以 driver 的形式即插即用到 OpenStack 中。 接下来介绍下这种插件机制以及后端的具体存储：后端存储大致可以分为两类，一类是软件实现的存储系统，比如LVM,ceph,sheepdog等。另一类是商用的硬件支持的专业存储系统，比如IBM,HP,EMC等公司的专业存储系统。 以LVM 为例： 此处简单介绍下iSCSI协议,引自 wiki: iSCSI利用了TCP/IP的port 860 和 3260 作为沟通的渠道。透过两部计算机之间利用iSCSI的协议来交换SCSI命令，让计算机可以透过高速的局域网集线来把SAN模拟成为本地的储存装置。iSCSI使用 TCP/IP 协议（一般使用TCP端口860和3260）。 本质上，iSCSI 让两个主机通过 IP 网络相互协商然后交换 SCSI 命令。这样一来，iSCSI 就是用广域网仿真了一个常用的高性能本地存储总线，从而创建了一个存储局域网（SAN）。不像某些 SAN 协议，iSCSI 不需要专用的电缆；它可以在已有的交换和 IP 基础架构上运行。然而，如果不使用专用的网络或者子网（ LAN 或者 VLAN ），iSCSI SAN 的部署性能可能会严重下降。于是，iSCSI 常常被认为是光纤通道（Fiber Channel）的一个低成本替代方法，而光纤通道是需要专用的基础架构的。但是，基于以太网的光纤通道（FCoE）则不需要专用的基础架构。 其他商用存储系统： 两者对比，check this! cinder的具体功能 从代码层面了解具体实现的流程，check this! 参考文章探索 OpenStack 之（9）：深入块存储服务Cinder 每天五分钟玩转openstack openstack 设计与实现 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python系列--python web 运行与部署]]></title>
    <url>%2F2016%2F09%2F05%2Fpython-web-intro%2F</url>
    <content type="text"><![CDATA[目录结构：在学习一个新东西的时候，我们其实默认是带着问题去学习的。但当我们真正在卷帙繁多的互联网或者书籍里获取信息时。之前的问题往往就淡化了，反而更多的是跟着资料所给的思路去学习。这样有好处也有坏处，好处是我们不用费心去思考，而坏处也是如此，不用思考带来的后果就是这些信息虽然当时我们理解了，但很难在我们的脑海中建立知识图谱，比较容易遗忘。 所以以后我会在写文章的时候先把问题具象化，从自己问题的角度出发去寻找答案。 本篇我们的问题从 python web 是如何跑起来的？ 开始，一步步探究由此问题引起的其他内容。 python web 是如何跑起来的？ 探究 WSGI 探究 python web framework 探究 python web server 常见 python web生产部署环境 python web 是如何跑起来的？要知道python web 是如何跑起来的，我们首先要知道一个http请求的生命周期。一个http请求从浏览器发出，在服务端被http server接收，这个http server对请求进行解析，如果是一些图片，文本等静态资源，它就会根据路径去寻找对应的资源并返回。如果是请求的动态资源，比如jsp/php等，它就会把相应的http请求转发给一个web server,由web server对请求进行解析并处理，并最终返回一个 http response。这就是一个http请求的大致周期。接下来对应到python web中，当web server 接收到一个请求时，它当然是解析请求并把该请求映射到我们写的对应的处理逻辑中。我们的处理逻辑处理完成后，再将结果由web server 转发出去。那web werver 与 我们写的处理逻辑该怎样做才能实现上述效果呢，这就引出了 WSGI。 以下示例摘自廖雪峰的官方网站 WSGI:Web Server Gateway Interface.WSGI接口定义非常简单，它只要求Web开发者实现一个函数，就可以响应HTTP请求。 12345# hello.pydef application(environ, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) return '&lt;h1&gt;Hello, web!&lt;/h1&gt;' 以上代码中application 函数接收两个参数，environ是一个包含所有HTTP请求信息的dict对象，start_response是一个发送HTTP响应的函数，该函数就是符合WSGI标准的一个HTTP处理函数。调用start_response()就发送了一个http header, http body 就是下文return 的数据。利用python自带的 web server 加载上面这个application. 1234567891011# server.py# 从wsgiref模块导入:from wsgiref.simple_server import make_server# 导入我们自己编写的application函数:from hello import application# 创建一个服务器，IP地址为空，端口是8000，处理函数是application:httpd = make_server('', 8000, application)print "Serving HTTP on port 8000..."# 开始监听HTTP请求:httpd.serve_forever() 确保以上两个文件在同一个目录下，然后在命令行输入python server.py来启动WSGI服务器。 探究 WSGI以下内容摘自wsgi和tornado.md WSGI是为python语言定义的web服务器和web应用程序或框架之间的一种简单而实用的接口。wsgi是一个web组件的接口规范，它将web组件分为三类：server，middleware，application。接下来简单介绍下这三个组件： wsgi server :可以理解为一个符合wsgi规范的web server，接收request请求，封装一系列环境变量，按照wsgi规范调用注册的wsgi app，最后将response返回给客户端。 wsgi application :就是一个普通的callable对象，当有请求到来时，wsgi server会调用这个wsgi app。这个对象接收两个参数，通常为environ,start_response。environ可以理解为环境变量，跟一次请求相关的所有信息都保存在了这个环境变量中，包括服务器信息，客户端信息，请求信息。start_response是一个callback函数，wsgi application通过调用start_response，将response headers/status 返回给wsgi server。此外这个wsgi app会return 一个iterator对象 ，这个iterator就是response body。 wsgi middleware :可以简单地理解为对application的封装。通过封装实现一些公用的功能，如下示例用一个简单Dispatcher Middleware，用来实现URL 路由： 1234567891011121314151617181920212223242526272829303132333435363738from wsgiref.simple_server import make_serverURL_PATTERNS= ( ('hi/','say_hi'), ('hello/','say_hello'), )class Dispatcher(object): def _match(self,path): path = path.split('/')[1] for url,app in URL_PATTERNS: if path in url: return app def __call__(self,environ, start_response): path = environ.get('PATH_INFO','/') app = self._match(path) if app : app = globals()[app] return app(environ, start_response) else: start_response("404 NOT FOUND",[('Content-type', 'text/plain')]) return ["Page dose not exists!"]def say_hi(environ, start_response): start_response("200 OK",[('Content-type', 'text/html')]) return ["kenshin say hi to you!"]def say_hello(environ, start_response): start_response("200 OK",[('Content-type', 'text/html')]) return ["kenshin say hello to you!"]app = Dispatcher()httpd = make_server('', 8000, app)print "Serving on port 8000..."httpd.serve_forever() 以上代码就已经有点web framework 的意思啦。由这个实例我们可以看到写一个基于wsgi的python web framework其实就是实现wsgi application部分和wsgi middleware 部分。其实，写一个python web framework 也是一件非常简单的事，见廖雪峰的官方博客。下面我们就探究一下业界常用的python web framework。而wsgi server部分我们待会再谈。 探究 python web framework翻看python 的wiki，我们发现关于python的web framework 以井喷的速度发展，各式各样的框架。截两张图，分别是主流的全栈框架和非全栈框架： 主流的全栈python web框架: 这里要多说一句tornado框架，其实它不单是一个web框架，还是一个web服务器。作为Web框架，是一个轻量级的Web框架，类似于另一个Python web 框架Web.py，其拥有异步非阻塞IO的处理方式。作为Web服务器，Tornado有较为出色的抗负载能力，官方用nginx反向代理的方式部署Tornado和其它Python web应用框架进行对比，结果最大浏览量超过第二名近40%。见下表： 主流的非全栈python web框架： 探究 python web server Apache + modwsgi，配置安装比较繁琐，用的人越来越少。 Gunicorn，运行在UNIX平台上的python wsgi http server。 uWSGI,实现了uwsgi和WSGI两种协议的Web服务器，负责响应python 的web请求。此处提醒一下，uwsgi是一种uWSGI服务器自有的协议，它跟WSGI毛的关系都没有。关于他们的区别，看这 Tornado,上文已介绍。 附录一张图： 常见 python web生产部署环境 参考文章廖雪峰的官方网站 大家都是怎么部署python网站的 区分wsgi、uWSGI、uwsgi、php-fpm、CGI、FastCGI的概念 使用了Gunicorn或者uWSGI,为什么还需要Nginx？ fcgi vs. gunicorn vs. uWSGI What is the difference between uwsgi protocol and wsgi protocol? 理解Python WSGI Web Frameworks for Python 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--swift]]></title>
    <url>%2F2016%2F09%2F02%2Fopenstack-swift%2F</url>
    <content type="text"><![CDATA[目录结构：在学习一个新东西的时候，我们其实默认是带着问题去学习的。但当我们真正在卷帙繁多的互联网或者书籍里获取信息时。之前的问题往往就淡化了，反而更多的是跟着资料所给的思路去学习。这样有好处也有坏处，好处是我们不用费心去思考，而坏处也是如此，不用思考带来的后果就是这些信息虽然当时我们理解了，但很难在我们的脑海中建立知识图谱，比较容易遗忘。 所以以后我会在写文章的时候先把问题具象化，从自己问题的角度出发去寻找答案。比如本篇我们学习openstack的对象存储模块swift，出现在脑海中的第一个问题就是 swift 是做什么用的？ 在解决了这个问题以后，我们知道是做对象存储的，那么， 它是如何进行存储的？ ，这就是第二个问题。在了解了它的存储机制后，我们还想问的就是 既然是存储，那么它的安全性机制是怎么做的？数据丢失了怎么办？ 。最后我们就要了解下 swift 的具体使用 以及 深入到代码层面 源码的组织架构 等。 swift什么用？ swift中数据如何存储？ swift中的安全机制 swift 如何使用？ swift 源码目录结构 swift 什么用？我们已经知道swift是openstack中负责对象存储的组件。自然而然的引出另一个问题，什么是对象存储？除了对象存储还有其他什么类型的存储，区别是什么？OK，接下来慢慢解决这些问题。 对象存储，有点类似于hashmap这种数据结构，我们可以通过key来获取/删除对应的value值。对象存储就是将对象数据作为一个逻辑单元存储起来，更多的操作是获取，删除。更新操作频率比较低。相应的，对象存储比较适合于存放静态数据或者更新频率比较低的大容量数据。比如一些多媒体数据，数据备份，虚拟机镜像等。 其他的存储类型还包括块存储，文件存储。 块存储通常提供的原始块设备，就是裸磁盘空间，比如磁盘阵列里面有2块硬盘，然后可以通过划逻辑盘、做Raid、或者LVM（逻辑卷）等种种方式逻辑划分出N个逻辑的硬盘，再采用映射的方式将这几个逻辑盘映射给主机使用。一些对实时性要求比较高的存储我们就得用块存储了，比如数据库。openstack中对应的组件为cinder。 文件存储就比较好理解了，我们常用的Windows操作系统就搭载了一套文件系统，提供文件存储。在分布式环境中，常用NFS协议，通过TCP/IP实现网络化存储，比如我们常用的NFS或者FTP服务器。 swift中数据如何存储？首先看下swift的整体架构： 整体架构主要分为两部分，访问层（Access Tier）和存储层（Storage Node）。访问层最主要的有两部分，Proxy node 主要负责Http请求的的转发，还有一个负责用户身份的认证（Authentication）。可选的是一个负载均衡设备。 当一个Restful请求到来时，Proxy node 负责接受用户请求并转发给认证服务进行处理，认证通过后再转发给存储层进行数据的操作。在转发给存储层之前，如果启用缓存的话，首先会去缓存服务器检查是否命中，命中就不用去数据层了。 接下来我们看下存储层，存储层由一系列的存储节点组成。为了便于组织以及故障隔离，这些存储节点在物理上做了一些划分，比如根据地理位置的不同划分region,一个 region相当于一个数据中心，每个rigion内部有多个zone，zone可以理解为一组独立的存储节点。一个zone包含多个存储节点（storage node）,一个node里有多个Device(可以理解为磁盘)，一个Device包含多个Partition(可以理解为磁盘中文件系统上的一个目录)。 在每个Storage node 上存储的对象在逻辑上又分为三层： 相应的，Storage node 中运行着三种对应的服务： Account Server :Account server 负责Account相关的一些服务，比如包含的Countainer列表，以及Account的元数据等。这些数据存储在一个SQLLite数据库中。 Countainer Server : 负责Countainer相关服务，比如包含的object列表，Countainer元数据。也存储在SQLlite数据库中。 Object Server : 提供对象数据的存取及相应的元数据服务，以二进制的形式存储在存储节点中，元数据以扩展属性的形式存储其中。因为对应的存储系统必须支持文件的扩展属性。 由以上信息我们得知swift对象最终以二进制的形式存储在存储节点中，存储节点中并没有“路径”的概念，那么它最终是如何与物理位置映射的呢？swift提出了ring的概念，ring其实就是记录了存储对象与物理存储节点的映射关系，object，countainer，account都有与之对应的ring。proxy server接收到http请求时，先查找操作实体（object，countainer，account）对应的ring,根据ring确定他们在对应的服务器集群中的具体位置，并将对应的http请求转发给对应的节点上的server。 此处注意的是，对象寻址并非是渐进式的（即寻找某个object先寻找account，再寻找下面的countainer,再寻找对应的object），而是直接寻找（即寻找object ,直接ring就可以找到，同理寻找某个account，countainer也是如此）。 为什么叫ring呢？其实这跟那个映射算法有关，这个将数据映射在相应服务器集群上某个节点的算法叫一致性hash算法，关于该算法，可以花5分钟的时间了解下这篇文章 至于swift 中是如何具体实现该算法的就不详细叙述了。下图为大致架构图： swift中的安全机制swift 中为了保证数据在损坏的情况下依然保证高可用，采用了增加副本的策略，每一个对象都会有若干个备份的副本（默认是三个），且存储在不同的zone里，这样，当某一个zone的对象数据不可用时，还可以启用其他zone里的副本。这样为了解决数据一致性的问题，swift开启了以下三个服务： Replicator: 负责检查数据与相应副本是否一致。如果出现数据不一致的情况，会将过时的数据更新，并负责将标记为删除的数据从磁盘上删除。 Auditor: 持续扫描磁盘检查Acount,Container和object数据的完整性，如果发现数据损坏，就会对相应的数据隔离，然后通过Replicator从其他节点上获取副本以恢复。 Updator: 在创建一个object的时候，需要对应的更新该object所在的container列表，以及再上层的account 列表，同理，创建container的时候也是如此。有时候这些更新操作因为相应的server繁忙而更新失败，swift会使用Updator服务继续处理这些失败的更新操作。 swift的使用swift的使用跟其他openstack的组件使用一样都是通过restful API来提供服务。swift API主要提供了以下几种功能： 对象存储 单个对象默认最大值是5GB，可以用户自己配置。 超过最大值对象的数据可以通过中间件进行上传，存储 对象压缩 对象删除 swift API 的执行过程大致如下：swiftClient 将用户的命令转换为标准HTTP请求（如果是用户请求本身就是HTTP请求那么没有这一步）;paste deploy将请求路由到proxy-server WSGI Application;根据请求内容调用对应的Controller(AccountController,ContainerController或者ObjectController),该controller会将请求转发到特定的存储节点上的WSGI Server(Account Server,Container Server或Object Server);这些server接收到http请求并处理。 swift 源码目录结构 bin: 主要是一些启动脚本，工具脚本。比如proxy-server负责启动proxy server，swift-ring-builder 用来创建ring。 swift：swift的核心代码。其子目录account,container,obj,proxy分别对应相应的服务的具体实现。common子目录是被多个组件共用的公共代码。 etc:配置文件模板 参考文章openstack 设计与实现 五分钟了解一致性hash swift github 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python库系列--requests 与 刷票]]></title>
    <url>%2F2016%2F08%2F31%2Fpython-requests%2F</url>
    <content type="text"><![CDATA[目录结构：关于刷票的若干思考 requests 引出 requests 基本用法 requests 高级用法 关于刷票的若干思考前两天同学让我帮他去一个网站帮他刷票，就顺手写了一个小程序，整理一下关于刷票的若干思考：其实应付普通的投票机制非常简单，只要懂点http的相关知识就可以搞定，原理就是构造相应的HTTP请求。一般来说，为了防止刷票，都会有以下几种防刷票策略： 限制IP : 相应的对策就是伪造IP。要想伪造IP的话首先要知道后端服务器程序是怎么获取IP的。至少有三个HTTP HEAD可以获取用户端IP，REMOTE_ADDR、HTTP_VIA、HTTP_FORWARDED_FOR 。REMOTE_ADDR是WEB服务器获取的用户IP值，也就是最终的外网IP，但它的重复值太多，所以不能作为唯一判断标志。 HTTP_VIA是倒数第二个代理服务器/网关的IP。 HTTP_FORWARDED_FOR是所有网关和你自己的IP列表。大部分服务端程序都是用HTTP_FORWARDED_FOR来获取IP。这样我们就可以通过修改HTTP_FORWARDED_FOR来达到伪造IP的目的。 限制网页来源 : 修改 http header中的refer。 限制 user-agent : 修改 http header中的 user-agent。 限制cookie : 修改 http header中的 cookie 限制登录后才可以投票 :模拟登录 验证码验证 : 简单的验证码可以通过OCR识别技术，增加干扰策略的验证码就比较复杂了，不予讨论。 当然，其实防刷票机制不只是上面简单的几种，一般来说比较安全的网站都会制定一些策略，比如IP可疑就增加验证码难度，设定黑白名单等。 requests 引出因为写上面的那个小程序用的是requests库。就简单的学习了一下这个库。python有自己的标准网络库，urllib,urllib2。但这两个库用着都不是特别的顺畅，requests 库的出现让人们眼前一亮，官网的declare:HTTP for Humans 。强调人性化。 requests 基本用法requests包要完成的内容无非就是发送和接收http请求。接下来我们就从发送请求和接收请求两方面讨论下它的基本使用。 发送请求 ：首先最简单的就是常用的get，post等发送HTTP请求类型，requests是这样发送的： 123456789import requestsr = requests.get('https://github.com/timeline.json')r = requests.post("http://httpbin.org/post")r = requests.put("http://httpbin.org/put")r = requests.delete("http://httpbin.org/delete")r = requests.head("http://httpbin.org/get")r = requests.options("http://httpbin.org/get") 对于 get 请求，我们可以通过添加params添加关键字参数： 1234import requestsparam = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.get("http://httpbin.org/get", params=param) 对于post请求，我们可以通过data参数来模拟提交表单。 1234import requestsparam = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.get("http://httpbin.org/get", data=param) 还可以通过 headers参数定制我们发送的请求： 123456789 import requests my_headers = &#123; 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36', 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Accept-Encoding' : 'gzip', 'Accept-Language' : 'zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4'&#125; r = requests.get("http://httpbin.org/get", headers=my_headers) 除此之外，还有timeout，cookies参数来设置响应时间，cookie。 相应内容 ： 任何时候调用 requests.*() 你都在做两件主要的事情。其一，你在构建一个 Request 对象， 该对象将被发送到某个服务器请求或查询一些资源。其二，一旦requests得到一个从服务器返回的响应就会产生一个 Response 对象。该响应对象包含服务器返回的所有信息， 也包含你原来创建的 Request 对象。 1234567891011import requestsr = requests.get('https://github.com/timeline.json')print(r.text) # 字符形式，可以自己设置编码print(r.content) # 二进制形式print(r.json()) # json形式,requests 中有一个内置的 JSON 解码器,处理 JSON 数据。print(r.raw()) # 原始套接字形式 高级用法 会话对象：http是无状态的，为了能够获取用户登录状态，一般服务器会用一个sessionId放在cookie中。用户发送的请求也包含这个sessionId，这些工作由浏览器来做。而requests也实现了类似功能。 12345678import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')r = s.get("http://httpbin.org/cookies")print(r.text) #'&#123;"cookies": &#123;"sessioncookie": "123456789"&#125;&#125;' SSL 证书验证Requests 可以为 HTTPS 请求验证 SSL 证书，就像 web 浏览器一样。要想检查某个主机的 SSL 证书，可以使用 verify 参数: 123import requestsrequests.get('https://github.com', verify=True) 代理利用proxies 参数来配置请求 12345678import requestsproxies = &#123; "http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080",&#125;requests.get("http://example.org", proxies=proxies) 参考文章requests quickstart requests advance Python HTTP 库：requests 快速入门 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本系列--set 与 unset]]></title>
    <url>%2F2016%2F08%2F29%2Fshell-set%2F</url>
    <content type="text"><![CDATA[目录结构：命令用途 常用命令参数 使用示例 命令用途set 主要是显示系统中已经存在的shell变量，以及设置shell变量的新变量值。使用set更改shell特性时，符号”+”和”-“的作用分别是打开和关闭指定的模式。set命令不能够定义新的shell变量。如果要定义新的变量，可以使用declare命令以 变量名=值 的格式进行定义即可。 unset 用于删除已定义的shell变量（包括环境变量）和shell函数。unset命令不能够删除具有只读属性的shell变量和环境变量。 常用命令参数set常用参数 -a 标示已修改的变量，以供输出至环境变量。 -b 使被中止的后台程序立刻回报执行状态。 unset 常用参数 -f 仅删除函数 -v 仅删除变量 使用示例set 示例： 123456declare mylove='Visual C++' #定义新环境变量set -a mylove #设置为环境变量env | grep mylove #显示环境变量值 unset 示例： 12unset -v mylove #删除指定的环境变量 参考文章Linux unset linux set 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本系列--declare 与 typeset]]></title>
    <url>%2F2016%2F08%2F26%2Fshell-declare-typeset%2F</url>
    <content type="text"><![CDATA[目录结构：命令用途 常用命令参数 使用示例 命令用途declare 与 typeset 命令是bash的内建命令，两者是完全一样的，用来声明shell变量，设置变量的属性。 常用命令参数 -r 设置变量为只读 -i 设置变量为整数 -a 设置变量为数组array -f 如果后面没有参数的话会列出之前脚本定义的所有函数，如果有参数的话列出以参数命名的函数 -x 设置变量在脚本外也可以访问到 使用示例12345678910111213141516171819202122232425262728293031#!/bin/bashfunc1 ()&#123; echo This is a function.&#125;declare -f # Lists the function above.echodeclare -i var1 # var1 is an integer.var1=2367echo "var1 declared as $var1"var1=var1+1 # Integer declaration eliminates the need for 'let'.echo "var1 incremented by 1 is $var1."# Attempt to change variable declared as integer.echo "Attempting to change var1 to floating point value, 2367.1."var1=2367.1 # Results in error message, with no change to variable.echo "var1 is still $var1"echodeclare -r var2=13.36 # 'declare' permits setting a variable property #+ and simultaneously assigning it a value.echo "var2 declared as $var2" # Attempt to change readonly variable.var2=13.37 # Generates error message, and exit from script.echo "var2 is still $var2" # This line will not execute.exit 0 # Script will not exit here. 参考文章Advanced Bash-Scripting Guide linux bash shell之declare 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--Nova]]></title>
    <url>%2F2016%2F08%2F22%2Fopenstack-nova%2F</url>
    <content type="text"><![CDATA[目录结构：概述 体系架构 VM实例的生命周期管理 动态迁移实例 代码结构与部署 概述 OpenStack Nova provides a cloud computing fabric controller, supporting a wide variety of virtualization technologies, including KVM, Xen, LXC, VMware, and more. In addition to its native API, it includes compatibility with the commonly encountered Amazon EC2 and S3 APIs. 也就是说，Nova 是一个控制器，它支持利用各种虚拟化技术（KVM,XEN等）完成云计算平台的CPU,内存，硬盘的虚拟化。云中的实例instance生命周期的所有活动都是由Nova控制。Nova自身并没有提供任何虚拟化能力，它使用libvirt API来与被支持的Hypervisors交互。Nova 通过一个与Amazon Web Services（AWS）EC2 API，S3 API兼容的web services API来对外提供服务。 体系架构首先放一张 Openstack 的整体概念结构图： 第二张图是逻辑结构图(图是13年的，现在的Network Service已经变更为Neutron) 图中可以看出，Nova处于Openstack的一个核心位置，其他组件都为 Nova 提供支持，Glance 为 VM 提供 image ，Cinder 和 Swift 分别为 VM 提供块存储和对象存储，Neutron 为 VM 提供网络连接。 接下来一张就是Nova的架构图： 这些组件以子服务（后台 deamon 进程）的形式运行。总的来说，nova的各个组件是以数据库和队列为中心进行通信的，下面对其中的几个组件做一个简单的介绍： nova-api: 接收和响应客户的 API 调用。 除了提供 OpenStack 自己的API，nova-api 还支持 Amazon EC2 API。 nova-scheduler: 虚机调度服务，负责决定在哪个计算节点上运行虚机 。 nova-compute: 管理虚机的核心服务，通过调用 Hypervisor API 实现虚机生命周期管理 。 Hypervisor: 计算节点上跑的虚拟化管理程序，虚机管理最底层的程序。 不同虚拟化技术提供自己的 Hypervisor。 常用的 Hypervisor 有 KVM，Xen， VMWare 等 。 nova-conductor: nova-compute 经常需要更新数据库，比如更新虚机的状态。 出于安全性和伸缩性的考虑，nova-compute 并不会直接访问数据库，而是将这个任务委托给 nova-conductor。 Database: Nova 会有一些数据需要存放到数据库中，一般使用 MySQL。 数据库安装在控制节点上。 Nova 使用命名为 “nova” 的数据库。 Message Queue: 为解耦各个子服务，Nova 通过 Message Queue 作为子服务的信息中转站。OpenStack 默认是用 RabbitMQ 作为 Message Queue。 VM实例的生命周期管理 有的操作功能比较类似，也有各自的适用场景，简单介绍下上述几个重要的操作： 常规操作： 常规操作中，Launch、Start、Reboot、Shut Off 和 Terminate 都很好理解。 下面几个操作重点回顾一下： Resize: 通过应用不同的 flavor 调整分配给 instance 的资源。 Lock/Unlock: 可以防止对 instance 的误操作。 Pause/Suspend/Resume: 暂停当前 instance，并在以后恢复。 Pause 和 Suspend 的区别在于 Pause 将 instance 的运行状态保存在计算节点的内存中，而 Suspend 保存在磁盘上。 Pause 的优点是 Resume 的速度比 Suspend 快；缺点是如果计算节点重启，内存数据丢失，就无法 Resume 了，而 Suspend 则没有这个问题。 Snapshot: 备份 instance 到 Glance。产生的 image 可用于故障恢复，或者以此为模板部署新的 instance。 故障处理: 故障处理有两种场景：计划内和计划外。计划内是指提前安排时间窗口做的维护工作，比如服务器定期的微码升级，添加更换硬件等。 计划外是指发生了没有预料到的突发故障，比如强行关机造成 OS 系统文件损坏，服务器掉电，硬件故障等。 计划内故障处理:对于计划内的故障处理，可以在维护窗口中将 instance 迁移到其他计算节点。 涉及如下操作： Migrate: 将 instance 迁移到其他计算节点。 迁移之前，instance 会被 Shut Off，支持共享存储和非共享存储。 Live Migrate： 与 Migrate 不同，Live Migrate 能不停机在线地迁移 instance，保证了业务的连续性。也支持共享存储和非共享存储（Block Migration）。 Shelve/Unshelve： Shelve 将 instance 保存到 Glance 上，之后可通过 Unshelve 重新部署。 Shelve 操作成功后，instance 会从原来的计算节点上删除。 Unshelve 会重新选择节点部署，可能不是原节点。 计划外故障处理： 划外的故障按照影响的范围又分为两类：Instance 故障和计算节点故障 。 Instance 故障:Instance 故障只限于某一个 instance 的操作系统层面，系统无法正常启动。 可以使用如下操作修复 instance。 Rescue/Unrescue: 用指定的启动盘启动，进入 Rescue 模式，修复受损的系统盘。成功修复后，通过 Unrescue 正常启动 instance。 Rebuild: 如果 Rescue 无法修复，则只能通过 Rebuild 从已有的备份恢复。Instance 的备份是通过 snapshot 创建的，所以需要有备份策略定期备份。 计算节点故障: Instance 故障的影响范围局限在特定的instance，计算节点本身是正常工作的。如果计算节点发生故障，OpenStack 则无法与节点的 nova-compute 通信，其上运行的所有 instance 都会受到影响。这个时候，只能通过 Evacuate 操作在其他正常节点上重建 Instance。 Evacuate: 利用共享存储上 Instance 的镜像文件在其他计算节点上重建 Instance。 所以提前规划共享存储是关键。 动态迁移实例注意：以下内容大多摘自每天五分钟玩转openstack，强烈推荐，如果觉得TLDR,以下为部分节选。 接下来以一个instance 动态迁移的示例来看下Nova的具体执行流程。 Migrate 操作会先将 instance 停掉，也就是所谓的“冷迁移”。而 Live Migrate 是“热迁移”，也叫“在线迁移”，instance不会停机。 Live Migrate 分两种： 源和目标节点没有共享存储，instance 在迁移的时候需要将其镜像文件从源节点传到目标节点，这叫做 Block Migration（块迁移） 源和目标节点共享存储，instance 的镜像文件不需要迁移，只需要将 instance 的状态迁移到目标节点。 源和目标节点需要满足一些条件才能支持 Live Migration： 源和目标节点的 CPU 类型要一致。 源和目标节点的 Libvirt 版本要一致。 源和目标节点能相互识别对方的主机名称，比如可以在 /etc/hosts 中加入对方的条目。 在源和目标节点的 /etc/nova/nova.conf 中指明在线迁移时使用 TCP 协议。 Instance 使用 config driver 保存其 metadata。在 Block Migration 过程中，该 config driver 也需要迁移到目标节点。由于目前 libvirt 只支持迁移 vfat 类型的 config driver，所以必须在 /etc/nova/nova.conf 中明确指明 launch instance 时创建 vfat 类型的 config driver。 源和目标节点的 Libvirt TCP 远程监听服务得打开 接下来以非共享存储Block Migration 为例简述下Nova的工作流程： 向 nova-api 发送请求：通过dashboard 发送live migrate消息，指定源节点，目标节点。 nova-api 发送消息:nova-api 向 Messaging（RabbitMQ）发送了一条消息：“Live Migrate 这个 Instance” 源代码在 /opt/stack/nova/nova/compute/api.py，方法是 live_migrate。 nova-compute 执行操作： 目标节点执行迁移前的准备工作，首先将instance的数据迁移过来，主要包括镜像文件、虚拟网络等资源，日志在 devstack-controller:/opt/stack/logs/n-cpu.log。 源节点启动迁移操作，暂停 instance。 在目标节点上 Resume instance 在源节点上执行迁移的后处理工作，删除 instance。 在目标节点上执行迁移的后处理工作，创建 XML，在 Hypervisor 中定义 instance，使之下次能够正常启动。 代码结构与部署 未完待续 参考文章大家谈OpenStack-Nova组件理解 OpenStack Grizzly Architecture OpenStack Hacker养成指南 每天5分钟玩转 OpenStack openstack doc openstack 设计与实现 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--neutron]]></title>
    <url>%2F2016%2F08%2F22%2Fopenstack-neutron%2F</url>
    <content type="text"><![CDATA[概述In short，neutron就是openstack为实现云平台上networking as a service而开发的项目。可以简单地理解为网络部分的虚拟化由neutron实现。 核心概念解释 1, Tenant networks 首先解释下network,一个network就是一个二层独立广播域。 由租户在project中创建的network.默认创建的network是完全独立的，不与其他project共享。neutron提供的tenant networks包括如下几种类型： Flat : 所有的虚拟机实例都在同一个network中，没有分隔策略,都在一个广播域。基本不用这种类型。 Vlan : 提供对Vlan的支持。 GRE and VXLAN ：利用隧道技术封装二层协议帧在L3或L4传输，实现两个虚拟Vlan的二层联通。 2, Provider networks 由openstack administrator 创建的network，在数据中心中映射物理网络。通常的类型为flat或Vlan 3, Subnet subnet属于网络中的三层概念，指定一段IPV4或者IPV6地址并描述其相关的配置信息。它附加在一个二层network 上指明属于这个network的虚拟机可使用的IP范围。 4, Port port 是一个设备的连接点，例如一台虚拟机的虚拟网卡。同时描述了相关的网络配置，例如该端口的MAC地址与IP地址。 5, Vlan VS Vxlan neutron 提供的服务 1, 基础的网络虚拟化（core service） 上文提到的network，subnet等都是一些基础的网络虚拟化服务。 2, DHCP 服务， 路由服务 这两个服务 都需要network namespace 技术来实现。 3, 安全组服务 通过 Linux Iptables实现。 4, LBaas 它允许租户动态的在自己的网络创建一个负载均衡设备。 5, FWaas 防火墙一般放在网关上，用来隔离子网之间的访问。因此，防火墙即服务（FireWall as a Service）也是在网络节点上（具体说来是在路由器命名空间中）来实现。 目前，OpenStack 中实现防火墙还是基于 Linux 系统自带的 iptables，所以大家对于其性能和功能就不要抱太大的期望了。 一个可能混淆的概念是安全组（Security Group），安全组的对象是虚拟网卡，由L2 Agent来实现，比如neutron_openvswitch_agent 和 neutron_linuxbridge_agent，会在计算节点上通过配置 iptables 规则来限制虚拟网卡的进出访问。防火墙可以在安全组之前隔离外部过来的恶意流量，但是对于同个子网内部不同虚拟网卡间的通讯不能过滤（除非它要跨子网）。 可以同时部署防火墙和安全组实现双重防护。 6, DVR(分布式路由) 按照 Neutron 原先的设计，所有网络服务都在网络节点上进行，这意味着大量的流量和处理，给网络节点带来了很大的压力。 这些处理的核心是路由器服务。任何需要跨子网的访问都需要路由器进行路由。 很自然，能否让计算节点上也运行路由器服务？这个设计思路无疑是更为合理的，但具体实施起来需要诸多细节上的技术考量。 为了降低网络节点的负载，同时提高可扩展性，OpenStack 自 Juno 版本开始正式引入了分布式路由（Distributed Virtual Router，DVR）特性（用户可以选择使用与否），来让计算节点自己来处理原先的大量东西向流量和非 SNAT 南北流量（有 floating IP 的 vm 跟外面的通信）。 这样网络节点只需要处理占到一部分的 SNAT （无 floating IP 的 vm 跟外面的通信）流量，大大降低了负载和整个系统对网络节点的依赖。很自然的，FWaaS 也可以跟着放到计算节点上。 DHCP 服务、VPN 服务目前仍然需要集中在网络节点上进行。 7, VPNaas 架构与原理 Neutron，其实和其他的OpenStack组件差不多，他都是一个中间层，自己基本不干具体的活，通过插件的机制，调用第三方的组件来完成相关的功能。 参考文章Openstack Neutron: Introduction OpenStack Neutron网络的肤浅理解 Overview and components 深入理解Neutron – OpenStack 网络实现 openstack doc openstack 设计与实现 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--Openstack 技术栈总览]]></title>
    <url>%2F2016%2F08%2F22%2Fopenstack-technology-stack%2F</url>
    <content type="text"><![CDATA[本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--网络虚拟化基础知识]]></title>
    <url>%2F2016%2F08%2F15%2Fopenstack-network-virtualization%2F</url>
    <content type="text"><![CDATA[网络发展历程详见：SDN软件定义网络从入门到精通》导论课 总结一下目前还常用的硬件设备： 集线器(hub)：属于物理层产品，对信息进行中继和放大，从任意接口收到的数据会往其他所有接口广播。 交换机(switch):链路层产品，记录终端主机的mac地址并生成mac表，根据mac表转发主机之间的数据流。能够进行VLAN隔离。 路由器(router):网络层产品，基于IP寻址，采用路由表实现数据转发。 网络虚拟化与SDNSDN(Software Define Network)是一种集中控制的网络架构，可将网络划分为数据层面和控制层面,它是实现网络虚拟化的一种解决方案。通常来说，只要网络硬件可以集中式软件管理，可编程化，控制转发层面分开，则可以认为这个网络是一个SDN网络。openstack 的neutron组件就是一种SDN 架构(其实，SDN的创始人就是Openstack Quantum,即后来的neutron 的发起者之一)。SDN大体架构如下(图片来自SDN与网络虚拟化的起源与现状)： 网络虚拟化可以认为是一种网络技术，它可以在物理网络上虚拟多个相互隔离的虚拟网络，从而使得不同用户之间使用独立的网络资源切片，从而提高网络资源利用率，实现弹性的网络。网络虚拟化是云计算和SDN发展到一定阶段的产物，因此可以认为网络虚拟化是新一代的SDN。延伸阅读：SDN与网络虚拟化的起源与现状 openstack相关的网络虚拟化知识 Linux Bridge:Linux Bridge 是 Linux 上用来做 TCP/IP二层协议交换的设备，其功能大家可以简单的理解为是一个二层交换机或者Hub。多个网络设备可以连接到同一个 Linux Bridge，当某个设备收到数据包时，Linux Bridge 会将数据转发给其他设备。 如上图，两个虚拟机，各自分配一个虚拟网卡vnet0,vnet1.他们连接在一个叫做br0 的Linux bridge 上。宿主机的网卡eth0分配到br0下，这样VM1与VM2可以相互通信，与外网也可以通信。 LAN 与VLAN : LAN: 表示 Local Area Network，本地局域网，通常使用 Hub 和 Switch 来连接 LAN 中的计算机。一个 LAN 表示一个广播域。 其含义是：LAN 中的所有成员都会收到任意一个成员发出的广播包。 VLAN: Virtual LAN。一个带有 VLAN 功能的switch 能够将自己的端口划分出多个 LAN。计算机发出的广播包可以被同一个 LAN 中其他计算机收到，但位于其他 LAN 的计算机则无法收到。 简单地说，VLAN 将一个交换机分成了多个交换机，限制了广播的范围，在二层将计算机隔离到不同的 VLAN 中。 VLAN的实现：一种方法是使用两个交换机，不同VLAN组的机器分别接到一个交换机。 另一种方法是使用一个带 VLAN 功能的交换机，将不同VLAN组的机器分别放到不同的 VLAN 中。请注意，VLAN 的隔离是二层上的隔离，不同VLAN无法相互访问指的是二层广播包（比如 arp）无法跨越 VLAN 的边界。但在三层上（比如IP）是可以通过路由器让 不同VLAN互通的。 KVM环境下VLAN的实现： eth0 是宿主机上的物理网卡，有一个命名为 eth0.10 的子设备与之相连。 eth0.10 就是 VLAN 设备了，其 VLAN ID 就是 VLAN 10。 eth0.10 挂在命名为 brvlan10 的 Linux Bridge 上，虚机 VM1 的虚拟网卡 vent0 也挂在 brvlan10 上。这样，宿主机用软件实现了一个交换机（当然是虚拟的），上面定义了一个 VLAN10。 eth0.10，brvlan10 和 vnet0 都分别接到 VLAN10 的 Access口上。而 eth0 就是一个 Trunk 口。VM1 通过 vnet0 发出来的数据包会被打上 VLAN10 的标签。brvlan20 同理。 也可以利用 OpenVSwitch实现（官方推荐） OpenVSwitch: 简称OVS,是一个高质量的、多层虚拟交换机，使用开源Apache2.0许可协议，由Nicira Networks开发，主要实现代码为可移植的C代码。它的目的是让大规模网络自动化可以通过编程扩展,同时仍然支持标准的管理接口和协议（例如NetFlow, sFlow, SPAN, RSPAN, CLI, LACP, 802.1ag）。此外,它被设计位支持跨越多个物理服务器的分布式环境，类似于VMware的vNetwork分布式vswitch或Cisco Nexus 1000 V。Open vSwitch支持多种linux 虚拟化技术，包括Xen/XenServer， KVM和irtualBox。 Linux nameSpace: Linux 提供的一种内核级别环境隔离的方法。类似chroot系统调用（通过修改根目录把用户监禁到一个特定目录下，chroot内部的文件系统无法访问外部的内容），Linux Namespace在此基础上，提供了对UTS、IPC、mount、PID、network、User等的隔离机制。在openstack neutron 中就利用network namespace 进行网络的隔离。 参考文章SDN与网络虚拟化的起源与现状 SDN软件定义网络从入门到精通》导论课 Neutron 理解 (1): Neutron 所实现的虚拟化网络 动手实践 Linux VLAN - 每天5分钟玩转 OpenStack（13） openstack doc OVS初级教程：使用open vswitch构建虚拟网络 Introducing Linux Network Namespaces Docker基础技术：Linux Namespace（上） Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack系列--服务器虚拟化知识]]></title>
    <url>%2F2016%2F08%2F15%2Fopenstack-hypevisor%2F</url>
    <content type="text"><![CDATA[概念与认识直接引用wiki: 在计算机技术中，虚拟化（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的配置更好的方式来应用这些资源。这些资源的新虚拟部分是不受现有资源的架设方式，地域或物理配置所限制。一般所指的虚拟化资源包括计算能力和数据存储。 关于虚拟化的大致分类，参考虚拟化分类 目前比较常用的虚拟化技术包括KVM,XEN,vmware workstation等，本文重点介绍下KVM技术。 KVM 技术KVM 简介KVM（Kernel-based Virtual Machine） 技术是在x86硬件平台（包含虚拟化扩展如Intel VT或AMD-V）上的一种完全虚拟化解决方案。效率可达到物理机的80％以上。它包含一个为处理器提供底层虚拟化可加载的核心模块kvm.ko（kvm-intel.ko 或 kvm-AMD.ko)。kvm还需要一个经过修改的QEMU软件（qemu-kvm），作为虚拟机上层控制和界面。kvm能在不改变linux或windows镜像的情况下同时运行多个虚拟机，（ps：它的意思是多个虚拟机使用同一镜像）并为每一个虚拟机配置个性化硬件环境（网卡、磁盘、图形适配器……）。在主流的linux内核，如2.6.20以上的内核均包含了kvm核心。 图片来自wikipedia 名词解释：KVM QEMU qemu-kvm LibvirtKVM:一种虚拟化技术 QEMU:一种模拟器，它向Guest OS模拟CPU和其他硬件，Guest OS认为自己和硬件直接打交道，其实是同Qemu模拟出来的硬件打交道，Qemu将这些指令转译给真正的硬件。 qemu-kvm:kvm负责cpu虚拟化+内存虚拟化，实现了cpu和内存的虚拟化，但kvm不能模拟其他设备。qemu模拟IO设备（网卡，磁盘等），kvm加上qemu之后就能实现真正意义上服务器虚拟化。因为用到了上面两个东西，所以称之为qemu-kvm。 Libvirt:目前使用最为广泛的一种对虚拟机进行管理的工具和API。Libvirtd是一个daemon进程，可以被本地的virsh调用，也可以被远程的virsh调用，Libvirtd调用qemu-kvm操作虚拟机。 更多Libvirt介绍 KVM 与 Docker)docker优势： 更轻量，启动和关闭速度更快 性能比虚拟化来说具有优势 docker劣势： 安全性 相对虚拟化来说生态环境还未成熟 推荐文章 参考文章OpenStack设计与实现（一）虚拟化 kvm vs docker 虚拟机已死 “容器”才是未来？ 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opensatck 系列--RabbitMQ 快速入门]]></title>
    <url>%2F2016%2F08%2F08%2Fintroduce-to-RabbitMQ%2F</url>
    <content type="text"><![CDATA[简介RabbitMQ 是一个消息队列框架，一个message broker，它的主要工作就是接收并转发消息。类似生产者/消费者模型，它的三个主要角色也是生产者（产生消息的角色），消费者（处理消息的角色），消息队列（缓存消息），接下来介绍下RabbitMQ的几个经典的应用模式。 准备工作RabbitMQ 安装 封装了AMQP的python第三方库Pika 发布/订阅 模式所谓发布/订阅是指：producer 发布消息，订阅的consumer都会接收到此消息，类似广播。示例给出一个简单的日志系统，产生的日志信息会广播到所有的consumer。 producer 代码如下，解释在注释中： 1234567891011121314151617import pika # 引入pika包import sysconnection = pika.BlockingConnection(pika.ConnectionParameters( host='localhost')) #指定主机hostchannel = connection.channel() #创建channelchannel.exchange_declare(exchange='logs', type='fanout') #声明exchange，以及exchange的type，解释见下文#channel.queue_declare(queue='hello') #创建一个名为hello的queue，因为此处是用的rabbitmq自动创建模式，所以注释掉message = ' '.join(sys.argv[1:]) or "info: Hello World!" #要发送的消息channel.basic_publish(exchange='logs', routing_key='', body=message) #绑定exchange，消息等print(" [x] Sent %r" % message)connection.close() # 关闭连接，关闭前会确定queue中是否还有消息，没有则关闭 Exchanges 解释：你可以把exchange看作是producer与queue之间的一个分发器，它会按照指定的策略把消息分发到指定queue中。 exchange 的类型有四种， direct 与routing_key 结合使用，指定发送到某个queue topic 按照某种主题模式，指定发送到满足该模式的多个queue headers RPC模式 fanout 广播模式 consumer 代码如下： 1234567891011121314151617181920212223242526import pikaconnection = pika.BlockingConnection(pika.ConnectionParameters( host='localhost'))channel = connection.channel()channel.exchange_declare(exchange='logs', type='fanout')result = channel.queue_declare(exclusive=True) #没有指定名字所以创建一个临时queue,而且consume断开后该queue会被删除queue_name = result.method.queuechannel.queue_bind(exchange='logs', queue=queue_name) # 绑定exchange与queueprint(' [*] Waiting for logs. To exit press CTRL+C')def callback(ch, method, properties, body): # 被调用的处理逻辑函数 print(" [x] %r" % body)channel.basic_consume(callback, queue=queue_name, no_ack=True) #指定callback函数，queue,no_ack解释见下文channel.start_consuming() ACK(消息确认机制)在rabbitmq运行过程中，如果某个consumer挂掉，那么分配到该consumer的所有其他任务也就无法完成。为了防止这种情况，默认ACK机制开启，consumer完成某个任务后会返回queue一个ack,表示当前任务已完成，可以分配下一个任务。如果指定no_ack=True，那么就会关闭ack机制。 其余message分发机制routing 分发 topic 分发 RPC调用 参考文章RabbitMQ doc]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack 系列--Pecan 简介]]></title>
    <url>%2F2016%2F08%2F07%2Fintroduce-to-Pecan%2F</url>
    <content type="text"><![CDATA[简介Pecan是一个轻量级的python web框架，它专注于HTTP，提供对象路由的寻址方式，没有提供针对session或者数据库的封装功能。尽管是轻量级的，也提供了一些可扩展的特性： 对象路由 REST风格的controller支持 可扩展的安全框架 可扩展的模板语言支持 可扩展的json支持 基于python文件的配置 安装About installation 基于Pecan创建第一个应用Quick start 对象路由介绍所谓对象路由就是将Http请求路径map到相应的controller对象的方法。具体操作就是先将URL路径拆分，然后从root controller开始一层层的往下寻找对应的方法，示例如下： 12345678910111213141516171819202122232425262728from pecan import exposeclass BooksController(object): @expose() def index(self): return "Welcome to book section." @expose() def bestsellers(self): return "We have 5 books in the top 10."class CatalogController(object): @expose() def index(self): return "Welcome to the catalog." books = BooksController()class RootController(object): @expose() def index(self): return "Welcome to store.example.com!" @expose() def hours(self): return "Open 24/7 on the web." catalog = CatalogController() @expose()是一个注解的装饰函数，只有注解了该函数才能进行对象路由，其参数常用于指定返回值，模板。一个request：http://xxxx/catalog/books/bestsellers ,首先会找到RootController，接着找到对应的catalog,也就是CatalogController,再接着找BookController,以此类推。除此之外，pecan还提供了pecan.route()指定路由匹配，以及 _lookup(), _default(), _route()函数。详细说明见官方文档。 Restful 风格的controller为了兼容 TurboGears2，Pecan也提供了一套rest风格的controller，RestController. 12345678910111213from pecan import exposefrom pecan.rest import RestControllerfrom mymodel import Bookclass BooksController(RestController): @expose() def get(self, id): book = Book.get(id) if not book: abort(404) return book.title 默认的URL mapping 如下： 配置文件当我们新建一个Pecan项目时，默认配置文件如下： 1234567891011server = &#123; 'port' : '8080', 'host' : '0.0.0.0'&#125;app = &#123; 'root' : None, 'modules' : [], 'static_root' : 'public', 'template_path' : ''&#125; 首先是server的配置，就是端口与所在机器的IP.第二个是app配置，Pecan利用app配置选项将你的应用最终打包成一个wsgi的应用，一个典型的配置如下： 1234567app = &#123; 'root' : 'project.controllers.root.RootController', 'modules' : ['project'], 'static_root' : '%(confdir)s/public', 'template_path' : '%(confdir)s/project/templates', 'debug' : True&#125; root: 应用的rootcontroller。modules:就是pecan去找寻你的应用的入口目录，该目录下必须包含一个app.py,该文件必须包含setup_app() 函数。static_root:静态文件所在根目录。template_path：模板文件所在目录。debug：是否开启debug模式。除了这两个主要配置，还有其他配置，不再赘述。 Pecan Hooks利用wsgi中间件访问Pecan内部是一件比较难的事情，Pecan提供了hooks机制来实现类似wsgi中间件的功能。Hooks提供了如下函数让你在request周期内的一些节点上做操作。 on-route(): 在Pecan将一个请求路由给对应的controller前调用。 before():路由完成，代码执行前调用。 after(): 代码执行完后调用 on_error():一个请求出现异常时调用。 实现一个Pecan Hook 12345678910from pecan.hooks import PecanHookclass SimpleHook(PecanHook): def before(self, state): print "\nabout to enter the controller..." def after(self, state): print "\nmethod: \t %s" % state.request.method print "\nresponse: \t %s" % state.response.status 接下来就是将Pecan hook attached 到 controller，有两种，如果是attached到整个项目所有的controller，可以在配置文件中指定，如下： 12345app = &#123; 'root' : '...' # ... 'hooks': lambda: [SimpleHook()]&#125; 如果是指定到某一个controller，那么可以用hooks做如下操作： 123456789101112from pecan import exposefrom pecan.hooks import HookControllerfrom my_hooks import SimpleHookclass SimpleController(HookController): __hooks__ = [SimpleHook()] @expose('json') def index(self): print "DO SOMETHING!" return dict() 参考文章pecan doc 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本系列--Zsh简介与使用]]></title>
    <url>%2F2016%2F08%2F05%2Fintroduce-to-Zsh%2F</url>
    <content type="text"><![CDATA[Zsh同bash一样，是一款功能强大的终端（shell）软件，只不过bash是大部分Linux发行版默认的shell，而Zsh需要手动安装（mac默认已安装）。关于Zsh的更多介绍请看 wiki)。 Zsh相比于bash有很多优势，两者的对比请看 why zsh is cooler than your shell概括一下就是：高效，自动补全更优秀，可定制型高。借用官网上一句话：if you want you hand dirty, this is definitely your choice! 而且随着开源项目 oh my zsh的火爆，zsh的各种主题以及插件也很齐全，适应一段时间后用起来得心应手。 当然，如果生产环境是分布式的情况下，还是建议规矩的用bash，毕竟来回切换的成本也是不小的。 接下来开始简单说下zsh的安装以及试用。 准备工作 Unix-based 机器 已安装 curl或者wget ,git 安装Zsh 两种安装方法：借用包管理工具（如：apt-get install zsh），源码安装（略） 安装完成后输入 zsh –version 出现相应的版本号即安装成功 输入 chsh -s $(which zsh) 更改默认shell 为zsh 退出之后再次登录即进入Zsh 安装Oh My Zsh（两种方法） 第一种，通过curl: 1sh -c "$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)" 第二种，通过wget: 1sh -c "$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" Oh My Zsh使用简介 插件：zsh的配置文件为 ~/.zshrc 相应的插件配置选项为 plugins=(git bundler osx rake ruby) 主题：在配置文件中的配置选项为： ZSH_THEME=”robbyrussell” 插件与主题都是可以配置的，可供配置的插件以及列表在这Extern themes , External-plugins 参考文章Zsh wiki) oh my zsh wiki github readme 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思考系列--如何学习一门新技术]]></title>
    <url>%2F2016%2F08%2F01%2Fthe-way-to-learn-a-new-technology%2F</url>
    <content type="text"><![CDATA[目录结构：技术认知的三个层次 技术需求的定位 不同层次的学习实施方法 文章主要是以学习一门计算机编程相关技术为目的，其他的知识学习方法类似，具体实施细节可能会不同。 技术认知的三个层次为了之后描述方便，首先给出技术认知的三个大致层次：简洁明了。再简单介绍下我所理解的这三个层次： 了解：知道这门技术为什么会出现，所解决的问题，大体框架，以及相关的技术栈。In short, 吹牛逼的时候能轻易地拽出这个词，但还没有付诸实践。 熟悉：已经将该技术落实于至少一个项目之上，知道如何使用以及使用过程中会有哪些常见的坑,但内在原理还不甚了解。 精通：读过源码，落实过至少三个项目，原理通透，知晓相比业界其他类似解决方案的pros and cons。这三个层次也可以理解为渐进式的学习。 技术需求的定位技术永远是为业务而服务的，我们学习一门技术时最好先找准自己对技术需求的定位。找到自己的定位就容易理清主线，不至于产生“乱”的感觉。技术需求也可以根据技术认知的三个层次划分。– 接触到一项新技术，比较感兴趣或者只想知道该技术的用途以及它所解决的痛点，对技术的认知止于了解层面即可。比较适于技术偏产品的职业。– 会用即可，不必了解深层次实现。适用于大部分程序员。– 对技术有更高层次的追求，知其所以然，适用于部分程序员。 不同层次的学习实施方法 了解层次： 官网guide wiki 知乎/quora/stackoverflow/github 知名blog google 熟练层次： 官网doc,完成demo 找一个经典项目并完成 精通层次 相关高评分书籍（amazon,豆瓣） 项目重构 阅读源码 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>think</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 系列--Invalid default value for UPDATE_TIME]]></title>
    <url>%2F2013%2F06%2F05%2Ffrance-removes-internet-cut-off%2F</url>
    <content type="text"><![CDATA[导入SQL文件时出现“Invalid default value for UPDATE_TIME”,UPDATE_TIME 是timeStamp类型。设置的默认值为“00-00-00 0000 0000” 。 经过不断试验发现是因为sql_mode 的设置原因，按照网上说的更改my_default.ini 并重启mysql后发现还是不行。SELECT @@global.sql_mode;SELECT @@session.sql_mode; sql_mode 依然没有改变 后直接在数据库层面改： SET GLOBAL sql_mode=’’; SET SESSION sql_mode=’’; 将sql_mode 设置为空，问题解决。 关于sql_mode ，见此 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
</search>
